{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "config = {\n",
    "    \"API_KEY\": API_KEY,\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"store\": True,\n",
    "}\n",
    "openai.api_key = API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncCursorPage[FineTuningJob](data=[FineTuningJob(id='ftjob-2lIadOpy8BV2efd6b5sFDRfW', created_at=1741520769, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:andre-personal:q1-epoch-3-batch-8:B99UIgsW', finished_at=1741521120, hyperparameters=Hyperparameters(batch_size=8, learning_rate_multiplier=2.0, n_epochs=3), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-FwEmbhh2UoZiY6sQeo5uowVJ', result_files=['file-6h6YtFZHVA5QtXWio2Rvtk'], seed=1160947212, status='succeeded', trained_tokens=151824, training_file='file-Rimsx2EtxaWUiQaUM6265g', validation_file='file-LwCVEPCAi1scGNiNQxxNYo', estimated_finish=None, integrations=[], metadata=None, method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=8, learning_rate_multiplier=2.0, n_epochs=3)), type='supervised'), user_provided_suffix='Q1-epoch-3-batch-8'), FineTuningJob(id='ftjob-qbxPcbVQV7Ip4yLP66AhoSl4', created_at=1741520573, error=Error(code='invalid_training_file', message='The job failed due to an invalid training file. Invalid file format. Line 1, key \"id\": Extra inputs are not permitted', param='training_file'), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=8, learning_rate_multiplier=None, n_epochs=3), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-FwEmbhh2UoZiY6sQeo5uowVJ', result_files=[], seed=250012560, status='failed', trained_tokens=None, training_file='file-3PMLynb2LdFngPhu1uZgcQ', validation_file='file-5yEQd8xEMUBiE4rcHS5TQk', estimated_finish=None, integrations=[], metadata=None, method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=8, learning_rate_multiplier=None, n_epochs=3)), type='supervised'), user_provided_suffix='Q1-epoch-3-batch-8')], has_more=False, object='list')\n"
     ]
    }
   ],
   "source": [
    "models = openai.fine_tuning.jobs.list()\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in test file: 23\n",
      "Processing line 1\n",
      "Processing line 2\n",
      "Processing line 3\n",
      "Processing line 4\n",
      "Processing line 5\n",
      "Processing line 6\n",
      "Processing line 7\n",
      "Processing line 8\n",
      "Processing line 9\n",
      "Processing line 10\n",
      "Processing line 11\n",
      "Processing line 12\n",
      "Processing line 13\n",
      "Processing line 14\n",
      "Processing line 15\n",
      "Processing line 16\n",
      "Processing line 17\n",
      "Processing line 18\n",
      "Processing line 19\n",
      "Processing line 20\n",
      "Processing line 21\n",
      "Processing line 22\n",
      "Processing line 23\n",
      "✅ Predictions saved in ./output/predictions_Q3_11.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuned model ID\n",
    "# Q1\n",
    "# fine_tuned_model = \"ft:gpt-3.5-turbo-0125:andre-personal:q1-epoch-3-batch-8:B99UIgsW\"\n",
    "\n",
    "# Q2\n",
    "# fine_tuned_model = \"ft:gpt-3.5-turbo-0125:andre-personal:q2-epoch-3-batch-8:B9C2KwaN\"\n",
    "\n",
    "# Q3-full\n",
    "# fine_tuned_model = \"ft:gpt-3.5-turbo-0125:andre-personal:q3-epoch-3-batch-8:B9CDYR15\"\n",
    "\n",
    "# Q3-11\n",
    "fine_tuned_model = \"ft:gpt-3.5-turbo-0125:andre-personal:q3-epoch-3-batch-8:B9CDYIQC:ckpt-step-11\"\n",
    "\n",
    "# Q4\n",
    "# fine_tuned_model = \"ft:gpt-3.5-turbo-0125:andre-personal:q4-epoch-3-batch-8:B9CFAmn9\"\n",
    "\n",
    "# Load test dataset\n",
    "test_file = \"./dataset/processed/test_Q3.jsonl\"\n",
    "output_file = \"./output/predictions_Q3_11.jsonl\"\n",
    "\n",
    "predictions = []\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            predictions.append(data)\n",
    "\n",
    "\n",
    "cou = 0\n",
    "continuee = len(predictions)\n",
    "\n",
    "with open(test_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    print(\"Number of lines in test file:\", len(f.readlines()))\n",
    "    f.seek(0)\n",
    "    for line in f:\n",
    "        cou += 1\n",
    "        print(\"Processing line\", cou)\n",
    "        if cou <= continuee:\n",
    "            print(\"Skipping line\", cou)\n",
    "            continue\n",
    "        data = json.loads(line)\n",
    "\n",
    "        # Extract messages from test data\n",
    "        messages = data[\"messages\"]\n",
    "        idd = data[\"id\"]\n",
    "        # print(idd, messages)\n",
    "\n",
    "        # Perform inference\n",
    "        response = openai.chat.completions.create(\n",
    "            model=fine_tuned_model,\n",
    "            messages=messages,\n",
    "        )\n",
    "\n",
    "        # Extract predicted score\n",
    "        predicted_score = float(response.choices[0].message.content.split(\" \")[-1])\n",
    "\n",
    "        # Save result with ID\n",
    "        result = {\"id\": data[\"id\"], \"predicted_score\": predicted_score}\n",
    "        predictions.append(result)\n",
    "\n",
    "# Save predictions as JSONL\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in predictions:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Predictions saved in {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 372, 'predicted_score': 5.0}, {'id': 374, 'predicted_score': 2.0}, {'id': 375, 'predicted_score': 5.0}, {'id': 381, 'predicted_score': 1.0}, {'id': 390, 'predicted_score': 4.0}]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        predictions.append(data)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions saved in ./output/predictions_Q1_2.jsonl\n"
     ]
    }
   ],
   "source": [
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in predictions:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Predictions saved in {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 362, 'predicted_score': 5.0}, {'id': 363, 'predicted_score': 2.0}, {'id': 364, 'predicted_score': 1.5}, {'id': 365, 'predicted_score': 5.0}, {'id': 366, 'predicted_score': 5.0}, {'id': 367, 'predicted_score': 5.0}, {'id': 368, 'predicted_score': 5.0}, {'id': 369, 'predicted_score': 0.0}, {'id': 370, 'predicted_score': 5.0}, {'id': 371, 'predicted_score': 2.0}, {'id': 372, 'predicted_score': 5.0}, {'id': 373, 'predicted_score': 1.0}, {'id': 374, 'predicted_score': 2.0}, {'id': 375, 'predicted_score': 5.0}, {'id': 376, 'predicted_score': 4.0}, {'id': 377, 'predicted_score': 5.0}, {'id': 378, 'predicted_score': 0.0}, {'id': 379, 'predicted_score': 0.0}, {'id': 380, 'predicted_score': 5.0}, {'id': 381, 'predicted_score': 1.0}, {'id': 382, 'predicted_score': 3.5}, {'id': 383, 'predicted_score': 4.0}, {'id': 384, 'predicted_score': 3.5}, {'id': 385, 'predicted_score': 0.0}, {'id': 386, 'predicted_score': 5.0}, {'id': 387, 'predicted_score': 0.0}, {'id': 388, 'predicted_score': 5.0}, {'id': 389, 'predicted_score': 0.0}, {'id': 390, 'predicted_score': 4.0}, {'id': 391, 'predicted_score': 0.0}, {'id': 392, 'predicted_score': 0.0}, {'id': 393, 'predicted_score': 2.5}, {'id': 394, 'predicted_score': 0.0}, {'id': 395, 'predicted_score': 5.0}, {'id': 396, 'predicted_score': 5.0}, {'id': 397, 'predicted_score': 3.0}, {'id': 398, 'predicted_score': 0.0}, {'id': 399, 'predicted_score': 4.0}, {'id': 400, 'predicted_score': 3.5}, {'id': 401, 'predicted_score': 1.0}, {'id': 402, 'predicted_score': 5.0}, {'id': 403, 'predicted_score': 4.0}, {'id': 404, 'predicted_score': 3.0}, {'id': 405, 'predicted_score': 5.0}, {'id': 406, 'predicted_score': 5.0}, {'id': 407, 'predicted_score': 1.0}, {'id': 408, 'predicted_score': 2.0}, {'id': 409, 'predicted_score': 0.0}, {'id': 410, 'predicted_score': 5.0}, {'id': 411, 'predicted_score': 5.0}, {'id': 412, 'predicted_score': 1.0}, {'id': 413, 'predicted_score': 1.0}, {'id': 414, 'predicted_score': 5.0}, {'id': 415, 'predicted_score': 5.0}, {'id': 416, 'predicted_score': 5.0}, {'id': 417, 'predicted_score': 5.0}, {'id': 418, 'predicted_score': 5.0}, {'id': 419, 'predicted_score': 4.0}, {'id': 420, 'predicted_score': 0.0}, {'id': 421, 'predicted_score': 1.0}, {'id': 422, 'predicted_score': 5.0}, {'id': 423, 'predicted_score': 5.0}, {'id': 424, 'predicted_score': 5.0}, {'id': 425, 'predicted_score': 5.0}, {'id': 426, 'predicted_score': 0.5}, {'id': 427, 'predicted_score': 3.0}, {'id': 428, 'predicted_score': 5.0}, {'id': 429, 'predicted_score': 0.0}, {'id': 430, 'predicted_score': 5.0}, {'id': 431, 'predicted_score': 5.0}, {'id': 432, 'predicted_score': 5.0}, {'id': 433, 'predicted_score': 5.0}, {'id': 434, 'predicted_score': 0.0}, {'id': 435, 'predicted_score': 5.0}, {'id': 436, 'predicted_score': 4.5}, {'id': 437, 'predicted_score': 5.0}, {'id': 438, 'predicted_score': 0.0}, {'id': 439, 'predicted_score': 5.0}, {'id': 440, 'predicted_score': 5.0}, {'id': 441, 'predicted_score': 5.0}, {'id': 442, 'predicted_score': 5.0}, {'id': 443, 'predicted_score': 5.0}, {'id': 444, 'predicted_score': 5.0}, {'id': 445, 'predicted_score': 4.5}, {'id': 446, 'predicted_score': 1.0}, {'id': 447, 'predicted_score': 0.0}, {'id': 448, 'predicted_score': 1.0}, {'id': 449, 'predicted_score': 4.0}, {'id': 450, 'predicted_score': 1.0}, {'id': 451, 'predicted_score': 5.0}]\n"
     ]
    }
   ],
   "source": [
    "q1_ans_path = \"./output/predictions_Q1.jsonl\"\n",
    "q2_ans_path = \"./output/predictions_Q2.jsonl\"\n",
    "q3_ans_path = \"./output/predictions_Q3_11.jsonl\"\n",
    "q4_ans_path = \"./output/predictions_Q4.jsonl\"\n",
    "\n",
    "out_path = \"./output/predictions_all_2.csv\"\n",
    "\n",
    "path_all_ans = [q1_ans_path, q2_ans_path, q3_ans_path, q4_ans_path]\n",
    "\n",
    "col = [\"ID\", \"score\"]\n",
    "all_ans = []\n",
    "\n",
    "for path in path_all_ans:\n",
    "    with open (path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            all_ans.append(data)\n",
    "\n",
    "all_ans = sorted(all_ans, key=lambda x: x[\"id\"])\n",
    "print(all_ans)\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\",\".join(col) + \"\\n\")\n",
    "    for entry in all_ans:\n",
    "        f.write(f\"{entry['id']},{entry['predicted_score']}\\n\")\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
