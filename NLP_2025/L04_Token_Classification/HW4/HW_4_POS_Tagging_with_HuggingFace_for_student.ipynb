{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxOTS6n1ikVL"
      },
      "source": [
        "# HW 4 - POS Tagging with Hugging Face\n",
        "\n",
        "In this exercise, you will create a part-of-speech (POS) tagging system for Thai text using NECTEC’s ORCHID corpus. Instead of building your own deep learning architecture from scratch, you will leverage a pretrained tokenizer and a pretrained token classification model from Hugging Face.\n",
        "\n",
        "We have provided some starter code for data cleaning and preprocessing in this notebook, but feel free to modify those parts to suit your needs. You are welcome to use additional libraries (e.g., scikit-learn) as long as you incorporate the pretrained Hugging Face model. Specifically, you will need to:\n",
        "\n",
        "1. Load a pretrained tokenizer and token classification model.\n",
        "2. Fine-tune it on the ORCHID corpus for POS tagging.\n",
        "3. Evaluate and report the performance of your model on the test data.\n",
        "\n",
        "### Don't forget to change hardware accelrator to GPU in runtime on Google Colab ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ1Uqldlj81G"
      },
      "source": [
        "## 1. Setup and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kyb4FhsEEeH8"
      },
      "outputs": [],
      "source": [
        "# Install transformers and thai2transformers\n",
        "# !pip install wandb\n",
        "# !pip install -q transformers==4.30.1 datasets evaluate thaixtransformers\n",
        "# # !pip install datasets evaluate thaixtransformers\n",
        "# !pip install -q emoji pythainlp sefr_cut tinydb seqeval sentencepiece pydantic jsonlines\n",
        "# !pip install peft==0.10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTgw8WW3BxWZ"
      },
      "source": [
        "## Setup\n",
        "\n",
        "1. Register [Wandb account](https://wandb.ai/login?signup=true) (and confirm your email)\n",
        "\n",
        "2. `wandb login` and copy paste the API key when prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4FjMIqhTBfOX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mp50629-2013x\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4qSh7MXZB74z"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-BR7danGv6W"
      },
      "source": [
        "We encourage you to login to your `Hugging Face` account so you can upload and share your model with the community. When prompted, enter your token to login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "n7h8NENllZK2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0a56fac514a4db4b739734d77816c82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqkDkseilv19"
      },
      "source": [
        "Download the dataset from Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kRksERXFEngl"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "orchid = load_dataset(\"Thichow/orchid_corpus\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T_AWd4d5lCYd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n",
              "        num_rows: 18500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n",
              "        num_rows: 4625\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "orchid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QNmIqSo0FkAx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'label_tokens': ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1'],\n",
              " 'pos_tags': [21, 39, 26, 26, 37, 4, 18],\n",
              " 'sentence': 'การประชุมทางวิชาการ ครั้งที่ 1'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "orchid['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hUuz3dLGlI_S"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'การประชุมทางวิชาการ ครั้งที่ 1'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "orchid['train'][0][\"sentence\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bh7fX19zI85W"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'การประชุมทางวิชาการ ครั้งที่ 1'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "''.join(orchid['train'][0]['label_tokens'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "38jM9YcSFmjV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total type of pos_tags : 47\n",
            "['ADVI', 'ADVN', 'ADVP', 'ADVS', 'CFQC', 'CLTV', 'CMTR', 'CMTR@PUNC', 'CNIT', 'CVBL', 'DCNM', 'DDAC', 'DDAN', 'DDAQ', 'DDBQ', 'DIAC', 'DIAQ', 'DIBQ', 'DONM', 'EAFF', 'EITT', 'FIXN', 'FIXV', 'JCMP', 'JCRG', 'JSBR', 'NCMN', 'NCNM', 'NEG', 'NLBL', 'NONM', 'NPRP', 'NTTL', 'PDMN', 'PNTR', 'PPRS', 'PREL', 'PUNC', 'RPRE', 'VACT', 'VATT', 'VSTA', 'XVAE', 'XVAM', 'XVBB', 'XVBM', 'XVMM']\n"
          ]
        }
      ],
      "source": [
        "label_list = orchid[\"train\"].features[f\"pos_tags\"].feature.names\n",
        "print('total type of pos_tags :', len(label_list))\n",
        "print(label_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Uf_NDWg7F6z_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-02 22:40:59.475343: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-02-02 22:40:59.566753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738510859.607433    4642 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738510859.619330    4642 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-02 22:40:59.710427: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import numpy.random\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from functools import partial\n",
        "\n",
        "#transformers\n",
        "from transformers import (\n",
        "    CamembertTokenizer,\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForMaskedLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "#thaixtransformers\n",
        "from thaixtransformers import Tokenizer\n",
        "from thaixtransformers.preprocess import process_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1suScqmntBW"
      },
      "source": [
        "Next, we load a pretrained tokenizer from Hugging Face. In this work, we utilize WangchanBERTa, a Thai-specific pretrained model, as the tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt3ASYUVm54n"
      },
      "source": [
        "# Choose Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFBKLqbIm23-"
      },
      "source": [
        "In this notebook, you can choose from 5 versions of WangchanBERTa, XLMR and mBERT to perform downstream tasks on Thai datasets. The datasets are:\n",
        "\n",
        "* `wangchanberta-base-att-spm-uncased` (recommended) - Largest WangchanBERTa trained on 78.5GB of Assorted Thai Texts with subword tokenizer SentencePiece\n",
        "* `xlm-roberta-base` - Facebook's [XLMR](https://arxiv.org/abs/1911.02116) trained on 100 languages\n",
        "* `bert-base-multilingual-cased` - Google's [mBERT](https://arxiv.org/abs/1911.03310) trained on 104 languages\n",
        "* `wangchanberta-base-wiki-newmm` - WangchanBERTa trained on Thai Wikipedia Dump with PyThaiNLP's word-level tokenizer  `newmm`\n",
        "* `wangchanberta-base-wiki-syllable` - WangchanBERTa trained on Thai Wikipedia Dump with PyThaiNLP's syllabel-level tokenizer `syllable`\n",
        "* `wangchanberta-base-wiki-sefr` - WangchanBERTa trained on Thai Wikipedia Dump with word-level tokenizer  `SEFR`\n",
        "* `wangchanberta-base-wiki-spm` - WangchanBERTa trained on Thai Wikipedia Dump with subword-level tokenizer SentencePiece\n",
        "\n",
        "In the first part, we require you to select the wangchanberta-base-att-spm-uncased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HbZo_TZDn17"
      },
      "source": [
        "<b> Learn more about using wangchanberta at [wangchanberta_getting_started_ai_reseach](https://colab.research.google.com/github/PyThaiNLP/thaixtransformers/blob/main/notebooks/wangchanberta_getting_started_aireseach.ipynb?fbclid=IwY2xjawH61XZleHRuA2FlbQIxMAABHZUaAmHobzmCMHpX0EgdLdjDAEwSX0bjqpo5xPUSIx9b4O_dsIvvG8KVNA_aem_IyKkvzy-VPf9k2pYAFf6Nw#scrollTo=n5IaCot9b3cF) <b>\n",
        "\n",
        "\n",
        "\n",
        "*   You need to set the transformers version to transformers==4.30.1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl2NposVIh9-"
      },
      "source": [
        "`In the first part, we require you to select the wangchanberta-base-att-spm-uncased.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "n5IaCot9b3cF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
            "The class this function is called from is 'WangchanbertaTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
            "The class this function is called from is 'WangchanbertaTokenizer'.\n"
          ]
        }
      ],
      "source": [
        "model_names = [\n",
        "    'airesearch/wangchanberta-base-att-spm-uncased',\n",
        "    'airesearch/wangchanberta-base-wiki-newmm',\n",
        "    'airesearch/wangchanberta-base-wiki-ssg',\n",
        "    'airesearch/wangchanberta-base-wiki-sefr',\n",
        "    'airesearch/wangchanberta-base-wiki-spm',\n",
        "]\n",
        "\n",
        "#@title Choose Pretrained Model\n",
        "model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
        "\n",
        "#create tokenizer\n",
        "tokenizer = Tokenizer(model_name).from_pretrained(\n",
        "                f'{model_name}',\n",
        "                revision='main',\n",
        "                model_max_length=416,)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzdbERHLwd0X"
      },
      "source": [
        "Let's try using a pretrained tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qwrwXsHFwl-G"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text : ศิลปะไม่เป็นเจ้านายใคร และไม่เป็นขี้ข้าใคร\n",
            "tokens : ['<s>', '', 'ศิลปะ', 'ไม่เป็น', 'เจ้านาย', 'ใคร', '<_>', 'และ', 'ไม่เป็น', 'ขี้ข้า', 'ใคร', '</s>']\n"
          ]
        }
      ],
      "source": [
        "text = 'ศิลปะไม่เป็นเจ้านายใคร และไม่เป็นขี้ข้าใคร'\n",
        "print('text :', text)\n",
        "tokens = []\n",
        "for i in tokenizer([text], is_split_into_words=True)['input_ids']:\n",
        "  tokens.append(tokenizer.decode(i))\n",
        "print('tokens :', tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEQAVqO8pDhK"
      },
      "source": [
        "model : * `wangchanberta-base-att-spm-uncased`\n",
        "\n",
        "First, we print examples of label tokens from our dataset for inspection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Vw_GdRdlpAhu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id : 0\n",
            "label_tokens : ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']\n",
            "pos_tags : [21, 39, 26, 26, 37, 4, 18]\n",
            "sentence : การประชุมทางวิชาการ ครั้งที่ 1\n"
          ]
        }
      ],
      "source": [
        "example = orchid[\"train\"][0]\n",
        "for i in example :\n",
        "    print(i, ':', example[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTuiwEWkppdA"
      },
      "source": [
        "Then, we use the sentence 'การประชุมทางวิชาการ<space>ครั้งที่ 1' to be tokenized by the pretrained tokenizer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BRCxMtHToN16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [5, 10, 882, 8222, 8, 10, 1014, 8, 10, 59, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = 'การประชุมทางวิชาการ ครั้งที่ 1'\n",
        "tokenizer(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxi8WqZnGa5F"
      },
      "source": [
        "These are already mapped into discrete values. We can uncover the original token text from the tokens by."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "optGK_eco3K6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>\n",
            "▁\n",
            "การประชุม\n",
            "ทางวิชาการ\n",
            "<_>\n",
            "▁\n",
            "ครั้งที่\n",
            "<_>\n",
            "▁\n",
            "1\n",
            "</s>\n"
          ]
        }
      ],
      "source": [
        "for i in tokenizer(text)['input_ids']:\n",
        "    print(tokenizer.convert_ids_to_tokens(i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3l13UKnwK-d"
      },
      "source": [
        "Now let's look at another example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UyfIR3BowU84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (Bilingual transfer dictionary)\n",
            "tokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', '<unk>', 'i', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\n",
            "label tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'Bilingual transfer dictionary', ')']\n",
            "label pos : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n"
          ]
        }
      ],
      "source": [
        "example = orchid[\"train\"][1899]\n",
        "print('sentence :', example[\"sentence\"])\n",
        "tokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print('tokens :',tokens)\n",
        "print('label tokens :', example[\"label_tokens\"])\n",
        "print('label pos :', example[\"pos_tags\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmV6M-vAwew5"
      },
      "source": [
        "Notice how `B` becomes an ``<unk>`` token. This is because this is an uncased model, meaning it only handles small English characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WniJR47ww7a0"
      },
      "source": [
        "# #TODO 0\n",
        "\n",
        "Convert the dataset to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RQWm_iWBxFQ8"
      },
      "outputs": [],
      "source": [
        "# Create a lowercase dataset for uncased BERT\n",
        "def lower_case_sentences(examples):\n",
        "    lower_cased_examples = examples\n",
        "\n",
        "    # fill code here to lower case the \"sentence\" and \"label_tokens\"\n",
        "    lower_cased_examples[\"sentence\"] = lower_cased_examples[\"sentence\"].lower()\n",
        "    lower_cased_examples[\"label_tokens\"] = [label.lower() for label in lower_cased_examples[\"label_tokens\"]]\n",
        "\n",
        "    return lower_cased_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ndBIqEpWuqBP"
      },
      "outputs": [],
      "source": [
        "orchidl = orchid.map(lower_case_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "z8xpcCqTrqbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n",
              "        num_rows: 18500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n",
              "        num_rows: 4625\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "orchidl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ecpDHyTPv2py"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '1899',\n",
              " 'label_tokens': ['โดย',\n",
              "  'พิจารณา',\n",
              "  'จาก',\n",
              "  'พจนานุกรม',\n",
              "  'ภาษา',\n",
              "  'คู่',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'bilingual transfer dictionary',\n",
              "  ')'],\n",
              " 'pos_tags': [25, 39, 38, 26, 26, 5, 37, 37, 26, 37],\n",
              " 'sentence': 'โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)'}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "orchidl[\"train\"][1899]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgV4ohz2xTY9"
      },
      "source": [
        "Now let's examine the labels again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "DoUDQzM7q265"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\n",
            "tokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', 'bi', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\n",
            "label tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\n",
            "label pos : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n"
          ]
        }
      ],
      "source": [
        "example = orchidl[\"train\"][1899]\n",
        "print('sentence :', example[\"sentence\"])\n",
        "tokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print('tokens :',tokens)\n",
        "print('label tokens :', example[\"label_tokens\"])\n",
        "print('label pos :', example[\"pos_tags\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "aEHgBeX7fQFt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentence : การประชุมทางวิชาการ ครั้งที่ 1\n",
            "tokens : ['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']\n",
            "label tokens : ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']\n",
            "label pos : [21, 39, 26, 26, 37, 4, 18]\n"
          ]
        }
      ],
      "source": [
        "example = orchidl[\"train\"][0]\n",
        "print('sentence :', example[\"sentence\"])\n",
        "tokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print('tokens :',tokens)\n",
        "print('label tokens :', example[\"label_tokens\"])\n",
        "print('label pos :', example[\"pos_tags\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dVcLxYbrl4E"
      },
      "source": [
        "In the example above, tokens refer to those tokenized using the pretrained tokenizer, while label tokens refer to tokens tokenized from our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1inxbOYuBpB"
      },
      "source": [
        "**Do you see something?**\n",
        "\n",
        "Yes, the tokens from the two tokenizers do not match.\n",
        "\n",
        "- sentence : `การประชุมทางวิชาการ ครั้งที่ 1`\n",
        "\n",
        "---\n",
        "\n",
        "- tokens : `['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']`\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- label tokens : `['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']`\n",
        "- label pos : `[21, 39, 26, 26, 37, 4, 18]`\n",
        "\n",
        "You can see that in our label tokens, 'การ' has a POS tag of 21, and 'ประชุม' has a POS tag of 39. However, when we tokenize the sentence using WangchanBERTa, we get the token 'การประชุม'. What POS tag should we assign to this new token?\n",
        "\n",
        "**What should we do ?**\n",
        "\n",
        "Based on this example, we found that the tokens from the WangchanBERTa do not directly align with our label tokens. This means we cannot directly use the label POS tags. Therefore, we need to reassign POS tags to the tokens produced by WangchanBERTa tokenization. The method we will use is majority voting:\n",
        "- If a token from the WangchanBERTa matches a label token exactly, we will directly assign the POS tag from the label POS.\n",
        "- If the token generated overlaps or combines multiple label tokens, we assign the POS tag based on the number of characters in each token: If the token contains the most characters from any label token, we assign the POS tag from that label token.\n",
        "\n",
        "**Example :**\n",
        "\n",
        "    # \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n",
        "    # \"การ\" has a POS tag of 21,\n",
        "    # and \"ประชุม\" has a POS tag of 39.\n",
        "    # Therefore, the POS tag for \"การประชุม\" is 39,\n",
        "    # as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n",
        "\n",
        "    # 'ทางวิชาการ' (10 chars) is formed from 'ทาง' (3 chars) + 'วิชาการ' (7 chars)\n",
        "    # \"ทาง\" has a POS tag of 26,\n",
        "    # and \"วิชาการ\" has a POS tag of 2.\n",
        "    # Therefore, the POS tag for \"ทางวิชาการ\" is 2,\n",
        "    # as \"ทางวิชาการ\" is derived more from the \"ทาง\" part than from the \"วิชาการ\" part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTkgye8K8sd8"
      },
      "source": [
        "# #TODO 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgU8Nudh2rUJ"
      },
      "source": [
        "`**Warning: Please be careful of <unk>, an unknown word token.**`\n",
        "\n",
        "`**Warning: Please be careful of \" ำ \", the 'am' vowel. WangchanBERTa's internal preprocessing replaces all \" ำ \" to 'ํ' and 'า'**`\n",
        "\n",
        "Assigning the label -100 to the special tokens `[<s>]` and `[</s>]` and `[_]`  so they’re ignored by the PyTorch loss function (see [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html): ignore_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "95 9601\n"
          ]
        }
      ],
      "source": [
        "print('_' == '▁')\n",
        "print(ord('_'), ord('▁'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bdxxiUU69lDx"
      },
      "outputs": [],
      "source": [
        "def majority_vote_pos(examples):\n",
        "\n",
        "    ####################################################################################################################\n",
        "    # TO DO: Since the tokens from the output of the pretrained tokenizer\n",
        "    # do not match the tokens in the label tokens of the dataset,\n",
        "    # the task is to create a function to determine the POS tags of the tokens generated by the pretrained tokenizer.\n",
        "    # This should be done by referencing the POS tags in the label tokens. If a token partially overlaps with others,\n",
        "    # the POS tag from the segment with the greater number of characters should be assigned.\n",
        "    #\n",
        "    # Example :\n",
        "    # \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n",
        "    # \"การ\" has a POS tag of 21,\n",
        "    # and \"ประชุม\" has a POS tag of 39.\n",
        "    # Therefore, the POS tag for \"การประชุม\" is 39,\n",
        "    # as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n",
        "    #\n",
        "    # 'ทางวิชาการ' (10 chars) is formed from 'ทาง' (3 chars) + 'วิชาการ' (7 chars)\n",
        "    # \"ทาง\" has a POS tag of 26,\n",
        "    # and \"วิชาการ\" has a POS tag of 2.\n",
        "    # Therefore, the POS tag for \"ทางวิชาการ\" is 2,\n",
        "    # as \"ทางวิชาการ\" is derived more from the \"ทาง\" part than from the \"วิชาการ\" part.\n",
        "\n",
        "    # tokenize word by pretrained tokenizer\n",
        "    tokenized_inputs = tokenizer([examples[\"sentence\"]], is_split_into_words=True)\n",
        "\n",
        "    # FILL CODE HERE\n",
        "    new_tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"])\n",
        "    new_pos_result = []\n",
        "    special = ['▁', '<s>', '</s>']\n",
        "    i_tl = 0\n",
        "    lword = 0\n",
        "    last_vote = None\n",
        "    for i in range(len(new_tokens)):\n",
        "        vote = {}\n",
        "        token_l = len(new_tokens[i]) - new_tokens[i].count('ํา') - new_tokens[i].count('▁')\n",
        "        if new_tokens[i] == '<_>':\n",
        "            token_l = 1\n",
        "        if new_tokens[i] in special:\n",
        "            new_pos_result.append(-100)\n",
        "            continue\n",
        "        else:\n",
        "            if lword > 0:\n",
        "                vote[last_vote] = vote.get(last_vote, 0) + min(lword, token_l)\n",
        "                lword, token_l = lword - min(lword, token_l), token_l - min(lword, token_l)\n",
        "            while lword < token_l:\n",
        "                vote[examples[\"pos_tags\"][i_tl]] = vote.get(examples[\"pos_tags\"][i_tl], 0) + min(len(examples[\"label_tokens\"][i_tl]), token_l - lword)\n",
        "                last_vote = examples[\"pos_tags\"][i_tl]\n",
        "                lword += len(examples[\"label_tokens\"][i_tl])\n",
        "                i_tl += 1\n",
        "            new_pos_result.append(max(vote, key=vote.get))\n",
        "            lword -= token_l\n",
        "\n",
        "    tokenized_inputs['tokens'] = new_tokens\n",
        "    tokenized_inputs['labels'] = new_pos_result\n",
        "\n",
        "    return tokenized_inputs\n",
        "    ####################################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 4\n",
            "2\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "a = \"ทำ\"\n",
        "b = \"ทําํ\"\n",
        "print(len(a), len(b))\n",
        "print(b.count('ํ'))\n",
        "print(b.count('ํา'))\n",
        "# b = b.replace('ํ', 'ำ')\n",
        "# print(a, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n",
            "{5: 9, 2: 3}\n"
          ]
        }
      ],
      "source": [
        "a = {}\n",
        "idx = 5\n",
        "a[idx] = a.get(idx, 0) + 3\n",
        "a[idx] = a.get(idx, 0) + 6\n",
        "idx = 2\n",
        "a[idx] = a.get(idx, 0) + 3\n",
        "print(max(a, key=a.get))\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "doFKOhpbGf9N"
      },
      "outputs": [],
      "source": [
        "tokenized_orchid = orchidl.map(majority_vote_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "uvdDnWeOJYpv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'label_tokens', 'pos_tags', 'sentence', 'input_ids', 'attention_mask', 'tokens', 'labels'],\n",
              "        num_rows: 18500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'label_tokens', 'pos_tags', 'sentence', 'input_ids', 'attention_mask', 'tokens', 'labels'],\n",
              "        num_rows: 4625\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_orchid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ojrRF85dJbwf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'label_tokens': ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1'],\n",
              " 'pos_tags': [21, 39, 26, 26, 37, 4, 18],\n",
              " 'sentence': 'การประชุมทางวิชาการ ครั้งที่ 1',\n",
              " 'input_ids': [5, 10, 882, 8222, 8, 10, 1014, 8, 10, 59, 6],\n",
              " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " 'tokens': ['<s>',\n",
              "  '▁',\n",
              "  'การประชุม',\n",
              "  'ทางวิชาการ',\n",
              "  '<_>',\n",
              "  '▁',\n",
              "  'ครั้งที่',\n",
              "  '<_>',\n",
              "  '▁',\n",
              "  '1',\n",
              "  '</s>'],\n",
              " 'labels': [-100, -100, 39, 26, 37, -100, 4, 18, -100, 18, -100]}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_orchid['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "KMfzFnjSdCGI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id : 0\n",
            "label_tokens : ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']\n",
            "pos_tags : [21, 39, 26, 26, 37, 4, 18]\n",
            "sentence : การประชุมทางวิชาการ ครั้งที่ 1\n",
            "input_ids : [5, 10, 882, 8222, 8, 10, 1014, 8, 10, 59, 6]\n",
            "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "tokens : ['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']\n",
            "labels : [-100, -100, 39, 26, 37, -100, 4, 18, -100, 18, -100]\n"
          ]
        }
      ],
      "source": [
        "example = tokenized_orchid[\"train\"][0]\n",
        "for i in example :\n",
        "    print(i, \":\", example[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lhsQcdL6H3J"
      },
      "source": [
        "This is the result after we realigned the POS based on the majority vote.\n",
        "- label_tokens : `['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']`\n",
        "- pos_tags : `[21, 39, 26, 26, 37, 4, 18]`\n",
        "- tokens : `['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']`\n",
        "- labels : `[-100, -100, 39, 26, 37, -100, 4, 18, -100, 18, -100]`\n",
        "\n",
        "`['<s>', '▁', '</s>'] : -100`\n",
        "\n",
        "**Check :**\n",
        "\n",
        "> \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n",
        "\n",
        "\n",
        "> \"การ\" has a POS tag of 21,\n",
        "\n",
        "> and \"ประชุม\" has a POS tag of 39.\n",
        "\n",
        "> Therefore, the POS tag for \"การประชุม\" is 39,\n",
        "\n",
        "> as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "iOE5CEgZdO9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id : 1899\n",
            "label_tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\n",
            "pos_tags : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n",
            "sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\n",
            "input_ids : [5, 489, 15617, 19737, 958, 493, 8, 1241, 4906, 11608, 12177, 8, 10, 11392, 9806, 8, 10, 2951, 15779, 8001, 29, 6]\n",
            "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "tokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', 'bi', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\n",
            "labels : [-100, 25, 39, 26, 26, 5, 37, 37, 26, 26, 26, 26, -100, 26, 26, 26, -100, 26, 26, 26, 37, -100]\n"
          ]
        }
      ],
      "source": [
        "# hard test case\n",
        "example = tokenized_orchid[\"train\"][1899]\n",
        "for i in example :\n",
        "    print(i, \":\", example[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv_NkVQ6qsJe"
      },
      "source": [
        "Expected output\n",
        "\n",
        "\n",
        "```\n",
        "id : 1899\n",
        "label_tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\n",
        "pos_tags : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n",
        "sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\n",
        "input_ids : [5, 489, 15617, 19737, 958, 493, 8, 1241, 4906, 11608, 12177, 8, 10, 11392, 9806, 8, 10, 2951, 15779, 8001, 29, 6]\n",
        "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "tokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', 'bi', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\n",
        "labels : [-100, 25, 39, 26, 26, 5, 37, 37, 26, 26, 26, 26, -100, 26, 26, 26, -100, 26, 26, 26, 37, -100]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pwADd1a85bn"
      },
      "source": [
        "# Train and Evaluate model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsnlIUJvYEy2"
      },
      "source": [
        "We will create a batch of examples using [DataCollatorWithPadding.](https://huggingface.co/docs/transformers/v4.48.0/en/main_classes/data_collator#transformers.DataCollatorWithPadding)  \n",
        "\n",
        "Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset.\n",
        "\n",
        "DataCollatorWithPadding will help us pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length. This allows for efficient computation during each batch.\n",
        "\n",
        "*   DataCollatorForTokenClassification : `padding (bool, str or PaddingStrategy, optional, defaults to True)`\n",
        "*   `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single sequence is provided).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "CcAY4-E2J6e5"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg4v14KcElbY"
      },
      "source": [
        "For evaluating your model’s performance. You can quickly load a evaluation method with the [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [seqeval](https://huggingface.co/spaces/evaluate-metric/seqeval) framework (see the Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric). Seqeval actually produces several scores: precision, recall, F1, and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "cZk3PjndK-Q8"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "seqeval = evaluate.load(\"seqeval\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FllGDNO5RUIA"
      },
      "source": [
        "Huggingface requires us to write a ``compute_metrics()`` function. This will be invoked when huggingface evalutes a model.\n",
        "\n",
        "Note that we ignore to evaluate on -100 labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "vDwNPItNLTM1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "        results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt13vTldvTw4"
      },
      "source": [
        "The total number of labels in our POS tag set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "JD84B79-Lxwf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ADVI': 0,\n",
              " 'ADVN': 1,\n",
              " 'ADVP': 2,\n",
              " 'ADVS': 3,\n",
              " 'CFQC': 4,\n",
              " 'CLTV': 5,\n",
              " 'CMTR': 6,\n",
              " 'CMTR@PUNC': 7,\n",
              " 'CNIT': 8,\n",
              " 'CVBL': 9,\n",
              " 'DCNM': 10,\n",
              " 'DDAC': 11,\n",
              " 'DDAN': 12,\n",
              " 'DDAQ': 13,\n",
              " 'DDBQ': 14,\n",
              " 'DIAC': 15,\n",
              " 'DIAQ': 16,\n",
              " 'DIBQ': 17,\n",
              " 'DONM': 18,\n",
              " 'EAFF': 19,\n",
              " 'EITT': 20,\n",
              " 'FIXN': 21,\n",
              " 'FIXV': 22,\n",
              " 'JCMP': 23,\n",
              " 'JCRG': 24,\n",
              " 'JSBR': 25,\n",
              " 'NCMN': 26,\n",
              " 'NCNM': 27,\n",
              " 'NEG': 28,\n",
              " 'NLBL': 29,\n",
              " 'NONM': 30,\n",
              " 'NPRP': 31,\n",
              " 'NTTL': 32,\n",
              " 'PDMN': 33,\n",
              " 'PNTR': 34,\n",
              " 'PPRS': 35,\n",
              " 'PREL': 36,\n",
              " 'PUNC': 37,\n",
              " 'RPRE': 38,\n",
              " 'VACT': 39,\n",
              " 'VATT': 40,\n",
              " 'VSTA': 41,\n",
              " 'XVAE': 42,\n",
              " 'XVAM': 43,\n",
              " 'XVBB': 44,\n",
              " 'XVBM': 45,\n",
              " 'XVMM': 46}"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id2label = {\n",
        "    0: 'ADVI',\n",
        "    1: 'ADVN',\n",
        "    2: 'ADVP',\n",
        "    3: 'ADVS',\n",
        "    4: 'CFQC',\n",
        "    5: 'CLTV',\n",
        "    6: 'CMTR',\n",
        "    7: 'CMTR@PUNC',\n",
        "    8: 'CNIT',\n",
        "    9: 'CVBL',\n",
        "    10: 'DCNM',\n",
        "    11: 'DDAC',\n",
        "    12: 'DDAN',\n",
        "    13: 'DDAQ',\n",
        "    14: 'DDBQ',\n",
        "    15: 'DIAC',\n",
        "    16: 'DIAQ',\n",
        "    17: 'DIBQ',\n",
        "    18: 'DONM',\n",
        "    19: 'EAFF',\n",
        "    20: 'EITT',\n",
        "    21: 'FIXN',\n",
        "    22: 'FIXV',\n",
        "    23: 'JCMP',\n",
        "    24: 'JCRG',\n",
        "    25: 'JSBR',\n",
        "    26: 'NCMN',\n",
        "    27: 'NCNM',\n",
        "    28: 'NEG',\n",
        "    29: 'NLBL',\n",
        "    30: 'NONM',\n",
        "    31: 'NPRP',\n",
        "    32: 'NTTL',\n",
        "    33: 'PDMN',\n",
        "    34: 'PNTR',\n",
        "    35: 'PPRS',\n",
        "    36: 'PREL',\n",
        "    37: 'PUNC',\n",
        "    38: 'RPRE',\n",
        "    39: 'VACT',\n",
        "    40: 'VATT',\n",
        "    41: 'VSTA',\n",
        "    42: 'XVAE',\n",
        "    43: 'XVAM',\n",
        "    44: 'XVBB',\n",
        "    45: 'XVBM',\n",
        "    46: 'XVMM',\n",
        "    # 47: 'O'\n",
        "}\n",
        "label2id = {}\n",
        "for k, v in id2label.items() :\n",
        "    label2id[v] = k\n",
        "\n",
        "label2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "mQtGN8QQQLME"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ADVI',\n",
              " 'ADVN',\n",
              " 'ADVP',\n",
              " 'ADVS',\n",
              " 'CFQC',\n",
              " 'CLTV',\n",
              " 'CMTR',\n",
              " 'CMTR@PUNC',\n",
              " 'CNIT',\n",
              " 'CVBL',\n",
              " 'DCNM',\n",
              " 'DDAC',\n",
              " 'DDAN',\n",
              " 'DDAQ',\n",
              " 'DDBQ',\n",
              " 'DIAC',\n",
              " 'DIAQ',\n",
              " 'DIBQ',\n",
              " 'DONM',\n",
              " 'EAFF',\n",
              " 'EITT',\n",
              " 'FIXN',\n",
              " 'FIXV',\n",
              " 'JCMP',\n",
              " 'JCRG',\n",
              " 'JSBR',\n",
              " 'NCMN',\n",
              " 'NCNM',\n",
              " 'NEG',\n",
              " 'NLBL',\n",
              " 'NONM',\n",
              " 'NPRP',\n",
              " 'NTTL',\n",
              " 'PDMN',\n",
              " 'PNTR',\n",
              " 'PPRS',\n",
              " 'PREL',\n",
              " 'PUNC',\n",
              " 'RPRE',\n",
              " 'VACT',\n",
              " 'VATT',\n",
              " 'VSTA',\n",
              " 'XVAE',\n",
              " 'XVAM',\n",
              " 'XVBB',\n",
              " 'XVBM',\n",
              " 'XVMM']"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = [i for i in id2label.values()]\n",
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu7Z3QH_BJe4"
      },
      "source": [
        "## Load pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBYlcF-gDZcF"
      },
      "source": [
        "Select a pretrained model for fine-tuning to develop a POS Tagger model using the Orchid corpus dataset.\n",
        "\n",
        "\n",
        "\n",
        "*   model : `wangchanberta-base-att-spm-uncased`\n",
        "*   Don't forget to update the num_labels.\n",
        "\n",
        "You’re ready to start training your model now! Load pretrained model with AutoModelForTokenClassification along with the number of expected labels, and the label mappings:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OOu8s-mO_Fw"
      },
      "source": [
        "`In the first part, we require you to select the wangchanberta-base-att-spm-uncased.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "OOsnubHyDMmA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_names = [\n",
        "    'wangchanberta-base-att-spm-uncased',\n",
        "    'wangchanberta-base-wiki-newmm',\n",
        "    'wangchanberta-base-wiki-ssg',\n",
        "    'wangchanberta-base-wiki-sefr',\n",
        "    'wangchanberta-base-wiki-spm',\n",
        "]\n",
        "\n",
        "#@title Choose Pretrained Model\n",
        "model_name = \"wangchanberta-base-att-spm-uncased\"\n",
        "\n",
        "#create model\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    f\"airesearch/{model_name}\",\n",
        "    revision='main',\n",
        "    num_labels=47, id2label=id2label, label2id=label2id\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H2OExQrCAfX"
      },
      "source": [
        "### #TODO 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBZKrz8nFXyT"
      },
      "source": [
        "* Configure your training hyperparameters using `**TrainingArguments**`. The only required parameter is is `output_dir`, which determines the directory where your model will be saved. To upload the model to the Hugging Face Hub, set push_to_hub=True (note: you must be logged into Hugging Face for this). During training, the Trainer will compute seqeval metrics at the end of each epoch and store the training checkpoint.\n",
        "* Provide the `**Trainer**` with the training arguments, as well as the model, dataset, tokenizer, data collator, and compute_metrics function.\n",
        "* Use `**train()**` to fine-tune the model.\n",
        "\n",
        "\n",
        "Read [huggingface's tutorial](https://huggingface.co/docs/transformers/en/tasks/token_classification) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMUkNHNrCwsl"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    #########################\n",
        "    output_dir=\"./train/wangchanberta-base-att-spm-uncased\",\n",
        "    per_device_eval_batch_size=32,\n",
        "    per_device_train_batch_size=32,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=1e-4\n",
        "\n",
        "    #########################\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    #########################\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_orchid[\"train\"],\n",
        "    eval_dataset=tokenized_orchid[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    ########################\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiXUG6aakihd"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OPVknoQk4wL"
      },
      "source": [
        "With your model fine-tuned, you can now perform inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "mMyq6I9CkZ1R"
      },
      "outputs": [],
      "source": [
        "text = \"การประชุมทางวิชาการ ครั้งที่ 1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL5qxD3EPLFt"
      },
      "source": [
        "`In the first part, we require you to select the wangchanberta-base-att-spm-uncased.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "sKgM-EaGfxA4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
            "The class this function is called from is 'WangchanbertaTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
            "The class this function is called from is 'WangchanbertaTokenizer'.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load pretrained tokenizer from Hugging Face\n",
        "#@title Choose Pretrained Model\n",
        "model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
        "\n",
        "tokenizer = Tokenizer(model_name).from_pretrained(model_name)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "PcRf-Q9nf4-t"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[   5,   10,  882, 8222,    8,   10, 1014,    8,   10,   59,    6]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "6ADY5OuqkkHb"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "## Load your fine-tuned model from Hugging Face\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"./train/wangchanberta-base-att-spm-uncased/checkpoint-1737\") ## your model path from Hugging Face\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "AACsd7VZgT1E"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['PUNC',\n",
              " 'PUNC',\n",
              " 'VACT',\n",
              " 'NCMN',\n",
              " 'PUNC',\n",
              " 'PUNC',\n",
              " 'CFQC',\n",
              " 'DONM',\n",
              " 'DONM',\n",
              " 'DONM',\n",
              " 'PUNC']"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = torch.argmax(logits, dim=2)\n",
        "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
        "predicted_token_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "kJKGbf4Rk0c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'ADVI',\n",
              " 1: 'ADVN',\n",
              " 2: 'ADVP',\n",
              " 3: 'ADVS',\n",
              " 4: 'CFQC',\n",
              " 5: 'CLTV',\n",
              " 6: 'CMTR',\n",
              " 7: 'CMTR@PUNC',\n",
              " 8: 'CNIT',\n",
              " 9: 'CVBL',\n",
              " 10: 'DCNM',\n",
              " 11: 'DDAC',\n",
              " 12: 'DDAN',\n",
              " 13: 'DDAQ',\n",
              " 14: 'DDBQ',\n",
              " 15: 'DIAC',\n",
              " 16: 'DIAQ',\n",
              " 17: 'DIBQ',\n",
              " 18: 'DONM',\n",
              " 19: 'EAFF',\n",
              " 20: 'EITT',\n",
              " 21: 'FIXN',\n",
              " 22: 'FIXV',\n",
              " 23: 'JCMP',\n",
              " 24: 'JCRG',\n",
              " 25: 'JSBR',\n",
              " 26: 'NCMN',\n",
              " 27: 'NCNM',\n",
              " 28: 'NEG',\n",
              " 29: 'NLBL',\n",
              " 30: 'NONM',\n",
              " 31: 'NPRP',\n",
              " 32: 'NTTL',\n",
              " 33: 'PDMN',\n",
              " 34: 'PNTR',\n",
              " 35: 'PPRS',\n",
              " 36: 'PREL',\n",
              " 37: 'PUNC',\n",
              " 38: 'RPRE',\n",
              " 39: 'VACT',\n",
              " 40: 'VATT',\n",
              " 41: 'VSTA',\n",
              " 42: 'XVAE',\n",
              " 43: 'XVAM',\n",
              " 44: 'XVBB',\n",
              " 45: 'XVBM',\n",
              " 46: 'XVMM'}"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OikBqcKTcUY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens : ['<s>', '▁', 'จะว่าไป', 'แล้ว', 'เชิง', 'เทียน', 'ของ', 'ผมก็', 'สวยดี', 'เหมือนกัน', '</s>']\n",
            "predict pos : ['PUNC', 'PUNC', 'ADVS', 'XVAE', 'NCMN', 'NCMN', 'RPRE', 'PPRS', 'VATT', 'ADVN', 'PUNC']\n"
          ]
        }
      ],
      "source": [
        "# Inference\n",
        "# ignore special tokens\n",
        "text = 'จะว่าไปแล้วเชิงเทียนของผมก็สวยดีเหมือนกัน'\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "tokenized_input = tokenizer([text], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print('tokens :', tokens)\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "predictions = torch.argmax(logits, dim=2)\n",
        "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
        "print('predict pos :', predicted_token_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-TtwYTCXs2O"
      },
      "source": [
        "**Evaluate model :**\n",
        "\n",
        "The output from the model is a softmax over classes. We choose the maximum class as the answer for evaluation. Again, we will ignore the -100 labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Hm_adLrcXyOe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "def evaluation_report(y_true, y_pred, get_only_acc=False):\n",
        "    # retrieve all tags in y_true\n",
        "    tag_set = set()\n",
        "    for sent in y_true:\n",
        "        for tag in sent:\n",
        "            tag_set.add(tag)\n",
        "    for sent in y_pred:\n",
        "        for tag in sent:\n",
        "            tag_set.add(tag)\n",
        "    tag_list = sorted(list(tag_set))\n",
        "\n",
        "    # count correct points\n",
        "    tag_info = dict()\n",
        "    for tag in tag_list:\n",
        "        tag_info[tag] = {'correct_tagged': 0, 'y_true': 0, 'y_pred': 0}\n",
        "\n",
        "    all_correct = 0\n",
        "    all_count = sum([len(sent) for sent in y_true])\n",
        "    speacial_tag = 0\n",
        "    for sent_true, sent_pred in zip(y_true, y_pred):\n",
        "        for tag_true, tag_pred in zip(sent_true, sent_pred):\n",
        "            # pass special token\n",
        "            if tag_true == -100 :\n",
        "                speacial_tag += 1\n",
        "                pass\n",
        "            if tag_true == tag_pred:\n",
        "                tag_info[tag_true]['correct_tagged'] += 1\n",
        "                all_correct += 1\n",
        "            tag_info[tag_true]['y_true'] += 1\n",
        "            tag_info[tag_pred]['y_pred'] += 1\n",
        "    print('speacial_tag :',speacial_tag) # delete number of special token from all_count\n",
        "    accuracy = (all_correct / (all_count-speacial_tag))\n",
        "\n",
        "    # get only accuracy for testing\n",
        "    if get_only_acc:\n",
        "      return accuracy\n",
        "\n",
        "    accuracy *= 100\n",
        "\n",
        "\n",
        "    # summarize and make evaluation result\n",
        "    eval_list = list()\n",
        "    for tag in tag_list:\n",
        "        eval_result = dict()\n",
        "        eval_result['tag'] = tag\n",
        "        eval_result['correct_count'] = tag_info[tag]['correct_tagged']\n",
        "        precision = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_pred'])*100 if tag_info[tag]['y_pred'] else '-'\n",
        "        recall = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_true'])*100 if (tag_info[tag]['y_true'] > 0) else 0\n",
        "        eval_result['precision'] = precision\n",
        "        eval_result['recall'] = recall\n",
        "        eval_result['f1_score'] = (2*precision*recall)/(precision+recall) if (type(precision) is float and recall > 0) else '-'\n",
        "\n",
        "        eval_list.append(eval_result)\n",
        "\n",
        "    eval_list.append({'tag': 'accuracy=%.2f' % accuracy, 'correct_count': '', 'precision': '', 'recall': '', 'f1_score': ''})\n",
        "\n",
        "    df = pd.DataFrame.from_dict(eval_list)\n",
        "    df = df[['tag', 'precision', 'recall', 'f1_score', 'correct_count']]\n",
        "\n",
        "    display(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "x7Lryj2aYCdn"
      },
      "outputs": [],
      "source": [
        "# prepare test set\n",
        "test_data = tokenized_orchid[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "FGXWNs9RY2Zv"
      },
      "outputs": [],
      "source": [
        "# labels for test set\n",
        "y_test = []\n",
        "for inputs in test_data:\n",
        "  y_test.append(inputs['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "U6__09qnX1DW"
      },
      "outputs": [],
      "source": [
        "y_pred = []\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "for inputs in test_data:\n",
        "    text = inputs['sentence']\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        pred = model(**inputs).logits\n",
        "        predictions = torch.argmax(pred, dim=2)\n",
        "        # Append padded predictions to y_pred\n",
        "        y_pred.append(predictions.tolist()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "yX0BhOe7g3eh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[37, 29, 39, 26, 26, 26, 37, 37, 26, 26, 26, 41, 37, 37, 26, 26, 39, 26, 37]\n",
            "[-100, 29, 39, 26, 26, 26, 37, -100, 26, 26, 26, 41, 37, -100, 26, 26, 39, 26, -100]\n"
          ]
        }
      ],
      "source": [
        "# check our prediction with label\n",
        "# -100 is special tokens : [<s>, </s>, _]\n",
        "print(y_pred[0])\n",
        "print(y_test[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Na0L6NUdgLaE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speacial_tag : 21039\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tag</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>correct_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-100</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>33.333333</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>78.751369</td>\n",
              "      <td>71.188119</td>\n",
              "      <td>74.778991</td>\n",
              "      <td>719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>62.5</td>\n",
              "      <td>4.310345</td>\n",
              "      <td>8.064516</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>70.454545</td>\n",
              "      <td>53.448276</td>\n",
              "      <td>60.784314</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4</td>\n",
              "      <td>88.333333</td>\n",
              "      <td>94.642857</td>\n",
              "      <td>91.37931</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5</td>\n",
              "      <td>64.912281</td>\n",
              "      <td>64.16185</td>\n",
              "      <td>64.534884</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6</td>\n",
              "      <td>89.922481</td>\n",
              "      <td>97.07113</td>\n",
              "      <td>93.360161</td>\n",
              "      <td>696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7</td>\n",
              "      <td>100.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8</td>\n",
              "      <td>67.042889</td>\n",
              "      <td>75.380711</td>\n",
              "      <td>70.967742</td>\n",
              "      <td>297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>92.70217</td>\n",
              "      <td>90.558767</td>\n",
              "      <td>91.617934</td>\n",
              "      <td>940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>96.774194</td>\n",
              "      <td>85.526316</td>\n",
              "      <td>90.80326</td>\n",
              "      <td>390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>52.258065</td>\n",
              "      <td>77.884615</td>\n",
              "      <td>62.548263</td>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>50.0</td>\n",
              "      <td>27.272727</td>\n",
              "      <td>35.294118</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>70.491803</td>\n",
              "      <td>83.495146</td>\n",
              "      <td>76.444444</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>84.51087</td>\n",
              "      <td>95.398773</td>\n",
              "      <td>89.62536</td>\n",
              "      <td>311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>97.95082</td>\n",
              "      <td>96.370968</td>\n",
              "      <td>97.154472</td>\n",
              "      <td>239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>69.698911</td>\n",
              "      <td>99.6337</td>\n",
              "      <td>82.020354</td>\n",
              "      <td>1088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>100.0</td>\n",
              "      <td>70.588235</td>\n",
              "      <td>82.758621</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>93.629344</td>\n",
              "      <td>96.806387</td>\n",
              "      <td>95.191364</td>\n",
              "      <td>1455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>77.403846</td>\n",
              "      <td>95.833333</td>\n",
              "      <td>85.638298</td>\n",
              "      <td>161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>83.035714</td>\n",
              "      <td>97.894737</td>\n",
              "      <td>89.855072</td>\n",
              "      <td>93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>95.528899</td>\n",
              "      <td>97.225305</td>\n",
              "      <td>96.369637</td>\n",
              "      <td>1752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>83.509235</td>\n",
              "      <td>86.950549</td>\n",
              "      <td>85.195155</td>\n",
              "      <td>1899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>90.027785</td>\n",
              "      <td>94.189241</td>\n",
              "      <td>92.06151</td>\n",
              "      <td>29485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>87.162162</td>\n",
              "      <td>62.926829</td>\n",
              "      <td>73.087819</td>\n",
              "      <td>387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>90.833333</td>\n",
              "      <td>93.965517</td>\n",
              "      <td>92.372881</td>\n",
              "      <td>109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>29</td>\n",
              "      <td>91.798561</td>\n",
              "      <td>99.222395</td>\n",
              "      <td>95.366218</td>\n",
              "      <td>638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>31</td>\n",
              "      <td>79.169574</td>\n",
              "      <td>87.471087</td>\n",
              "      <td>83.113553</td>\n",
              "      <td>2269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>32</td>\n",
              "      <td>91.194969</td>\n",
              "      <td>98.639456</td>\n",
              "      <td>94.771242</td>\n",
              "      <td>145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>33</td>\n",
              "      <td>66.326531</td>\n",
              "      <td>70.652174</td>\n",
              "      <td>68.421053</td>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>34</td>\n",
              "      <td>72.413793</td>\n",
              "      <td>84.0</td>\n",
              "      <td>77.777778</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>35</td>\n",
              "      <td>94.915254</td>\n",
              "      <td>62.222222</td>\n",
              "      <td>75.167785</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>36</td>\n",
              "      <td>93.625914</td>\n",
              "      <td>88.888889</td>\n",
              "      <td>91.195929</td>\n",
              "      <td>896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>37</td>\n",
              "      <td>39.736994</td>\n",
              "      <td>98.478088</td>\n",
              "      <td>56.625126</td>\n",
              "      <td>12359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>38</td>\n",
              "      <td>92.721893</td>\n",
              "      <td>91.717881</td>\n",
              "      <td>92.217155</td>\n",
              "      <td>3134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>39</td>\n",
              "      <td>89.729801</td>\n",
              "      <td>91.48168</td>\n",
              "      <td>90.597272</td>\n",
              "      <td>6841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>40</td>\n",
              "      <td>70.603908</td>\n",
              "      <td>69.736842</td>\n",
              "      <td>70.167696</td>\n",
              "      <td>795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>41</td>\n",
              "      <td>80.635514</td>\n",
              "      <td>79.74122</td>\n",
              "      <td>80.185874</td>\n",
              "      <td>2157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>42</td>\n",
              "      <td>85.158649</td>\n",
              "      <td>93.273543</td>\n",
              "      <td>89.031568</td>\n",
              "      <td>832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>43</td>\n",
              "      <td>91.847826</td>\n",
              "      <td>99.411765</td>\n",
              "      <td>95.480226</td>\n",
              "      <td>676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>45</td>\n",
              "      <td>74.115756</td>\n",
              "      <td>96.645702</td>\n",
              "      <td>83.894449</td>\n",
              "      <td>461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>46</td>\n",
              "      <td>95.283019</td>\n",
              "      <td>94.984326</td>\n",
              "      <td>95.133438</td>\n",
              "      <td>303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>accuracy=92.39</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               tag  precision     recall   f1_score correct_count\n",
              "0             -100          -        0.0          -             0\n",
              "1                0       50.0       25.0  33.333333             4\n",
              "2                1  78.751369  71.188119  74.778991           719\n",
              "3                2       62.5   4.310345   8.064516             5\n",
              "4                3  70.454545  53.448276  60.784314            31\n",
              "5                4  88.333333  94.642857   91.37931            53\n",
              "6                5  64.912281   64.16185  64.534884           111\n",
              "7                6  89.922481   97.07113  93.360161           696\n",
              "8                7      100.0       50.0  66.666667             2\n",
              "9                8  67.042889  75.380711  70.967742           297\n",
              "10              10   92.70217  90.558767  91.617934           940\n",
              "11              11  96.774194  85.526316   90.80326           390\n",
              "12              12  52.258065  77.884615  62.548263            81\n",
              "13              13       50.0  27.272727  35.294118             3\n",
              "14              14  70.491803  83.495146  76.444444            86\n",
              "15              15   84.51087  95.398773   89.62536           311\n",
              "16              16          -        0.0          -             0\n",
              "17              17   97.95082  96.370968  97.154472           239\n",
              "18              18  69.698911    99.6337  82.020354          1088\n",
              "19              19          -        0.0          -             0\n",
              "20              20      100.0  70.588235  82.758621            12\n",
              "21              21  93.629344  96.806387  95.191364          1455\n",
              "22              22  77.403846  95.833333  85.638298           161\n",
              "23              23  83.035714  97.894737  89.855072            93\n",
              "24              24  95.528899  97.225305  96.369637          1752\n",
              "25              25  83.509235  86.950549  85.195155          1899\n",
              "26              26  90.027785  94.189241   92.06151         29485\n",
              "27              27  87.162162  62.926829  73.087819           387\n",
              "28              28  90.833333  93.965517  92.372881           109\n",
              "29              29  91.798561  99.222395  95.366218           638\n",
              "30              31  79.169574  87.471087  83.113553          2269\n",
              "31              32  91.194969  98.639456  94.771242           145\n",
              "32              33  66.326531  70.652174  68.421053            65\n",
              "33              34  72.413793       84.0  77.777778            21\n",
              "34              35  94.915254  62.222222  75.167785            56\n",
              "35              36  93.625914  88.888889  91.195929           896\n",
              "36              37  39.736994  98.478088  56.625126         12359\n",
              "37              38  92.721893  91.717881  92.217155          3134\n",
              "38              39  89.729801   91.48168  90.597272          6841\n",
              "39              40  70.603908  69.736842  70.167696           795\n",
              "40              41  80.635514   79.74122  80.185874          2157\n",
              "41              42  85.158649  93.273543  89.031568           832\n",
              "42              43  91.847826  99.411765  95.480226           676\n",
              "43              45  74.115756  96.645702  83.894449           461\n",
              "44              46  95.283019  94.984326  95.133438           303\n",
              "45  accuracy=92.39                                               "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "evaluation_report(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0BZzVgjm4l_"
      },
      "source": [
        "# Other Pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssZdIst48AwH"
      },
      "source": [
        "In this section, we will experiment by fine-tuning other pretrained models, such as airesearch/wangchanberta-base-wiki-newmm, to see how about their performance.\n",
        "\n",
        "Since each model uses a different word-tokenization method.\n",
        "for example, **airesearch/wangchanberta-base-wiki-newmm uses newmm**,\n",
        "while **airesearch/wangchanberta-base-att-spm-uncased uses SentencePiece**.\n",
        "please try fine-tuning and compare the performance of these models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUCWuhrl91mj"
      },
      "source": [
        "### #TODO 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9etT-A_anBfi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
            "The class this function is called from is 'ThaiRobertaTokenizer'.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
            "The class this function is called from is 'ThaiRobertaTokenizer'.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "model_names = [\n",
        "    'airesearch/wangchanberta-base-att-spm-uncased',\n",
        "    'airesearch/wangchanberta-base-wiki-newmm',\n",
        "    'airesearch/wangchanberta-base-wiki-ssg',\n",
        "    'airesearch/wangchanberta-base-wiki-sefr',\n",
        "    'airesearch/wangchanberta-base-wiki-spm',\n",
        "]\n",
        "\n",
        "#@title Choose Pretrained Model\n",
        "model_name = \"airesearch/wangchanberta-base-wiki-spm\" #@param [\"airesearch/wangchanberta-base-att-spm-uncased\", \"airesearch/wangchanberta-base-wiki-newmm\", \"airesearch/wangchanberta-base-wiki-syllable\", \"airesearch/wangchanberta-base-wiki-sefr\", \"airesearch/wangchanberta-base-wiki-spm\"]\n",
        "\n",
        "#create tokenizer\n",
        "tokenizer = Tokenizer(model_name).from_pretrained(\n",
        "                f'{model_name}',\n",
        "                revision='main',\n",
        "                model_max_length=416,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LFXdV8V5nGk2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\n",
            "tokens : ['<s>', 'โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '(', 'bi', 'ling', 'ual', '<_>', 't', 'ransfer', '<_>', 'di', 'ction', 'ary', ')', '</s>']\n",
            "label tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\n",
            "{'id': '1899', 'label_tokens': ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')'], 'pos_tags': [25, 39, 38, 26, 26, 5, 37, 37, 26, 37], 'sentence': 'โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)'}\n"
          ]
        }
      ],
      "source": [
        "example = orchidl[\"train\"][1899]\n",
        "print('sentence :', example[\"sentence\"])\n",
        "tokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print('tokens :',tokens)\n",
        "print('label tokens :', example[\"label_tokens\"])\n",
        "\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8259\n",
            "sen 6. กลุ่มที่มีความกว้าง 10 ส่วน ได้แก่ ข ข ะ า โ ไ ใ\n",
            "tokens : ['<s>', '6', '.', '<_>', 'กลุ่ม', 'ที่มี', 'ความกว้าง', '<_>', '10', '<_>', 'ส่วน', '<_>', 'ได้แก่', '<_>', 'ข', '<_>', 'ข', '<_>', 'ะ', '<_>', 'า', '<_>', 'โ', '<_>', 'ไ', '<_>', 'ใ', '</s>']\n",
            "label tokens : ['6', '.', ' ', 'กลุ่ม', 'ที่', 'มี', 'ความ', 'กว้าง', ' ', '10', ' ', 'ส่วน', ' ', 'ได้แก่', ' ', 'ข', ' ', 'ข', ' ', 'ะ', ' ', 'า', ' ', 'โ', ' ', 'ไ', ' ', 'ใ']\n",
            "8323\n",
            "sen ระดับที่ 2 เป็นกลุ่มตัวอักษรระดับกลาง ได้แก่ พยัญชนะ สระที่อยู่ระดับเดียวกับพยัญชนะ เช่น ั ะ า ำ เ แ โ ใ ไ\n",
            "tokens : ['<s>', 'ระดับ', 'ที่', '<_>', '2', '<_>', 'เป็นกลุ่ม', 'ตัวอักษร', 'ระดับ', 'กลาง', '<_>', 'ได้แก่', '<_>', 'พยัญชนะ', '<_>', 'สระ', 'ที่อยู่', 'ระดับ', 'เดียวกับ', 'พยัญชนะ', '<_>', 'เช่น', '<_>', 'ั', '<_>', 'ะ', '<_>', 'า', '<_>', 'ํ', 'า', '<_>', 'เ', '<_>', 'แ', '<_>', 'โ', '<_>', 'ใ', '<_>', 'ไ', '</s>']\n",
            "label tokens : ['ระดับ', 'ที่ 2', ' ', 'เป็น', 'กลุ่ม', 'ตัวอักษร', 'ระดับกลาง', ' ', 'ได้แก่', ' ', 'พยัญชนะ', ' ', 'สระ', 'ที่', 'อยู่', 'ระดับ', 'เดียว', 'กับ', 'พยัญชนะ', ' ', 'เช่น', ' ', 'ั', ' ', 'ะ', ' ', 'า', ' ', 'ำ', ' ', 'เ', ' ', 'แ', ' ', 'โ', ' ', 'ใ', ' ', 'ไ']\n"
          ]
        }
      ],
      "source": [
        "all_unk = set()\n",
        "for i in range(len(orchidl[\"train\"])):\n",
        "    example = orchidl[\"train\"][i]\n",
        "    tokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "    if \"ะ\" in tokens and \"า\" in tokens and \"ไ\" in tokens and \"ใ\" in tokens:\n",
        "        print(i)\n",
        "        print('sen', example[\"sentence\"])\n",
        "        print(\"tokens :\", tokens)\n",
        "        print(\"label tokens :\", example[\"label_tokens\"])\n",
        "#     for j in range(len(tokens)):\n",
        "#         if tokens[j] == \"<unk>\" and tokens[j-1] != \"<unk>\":\n",
        "#             l = [tokens[j-1]]\n",
        "#             k = j\n",
        "#             while k < len(tokens) and tokens[k] == \"<unk>\":\n",
        "#                 l.append(tokens[k])\n",
        "#                 k += 1\n",
        "#             l.append(tokens[k])\n",
        "#             all_unk.add(tuple(l))\n",
        "\n",
        "# print(len(all_unk))\n",
        "# for i in all_unk:\n",
        "#     print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7QMwcbK5Ibj"
      },
      "source": [
        "It's the same problem as above.\n",
        "\n",
        "`**Warning: Can we use same function as above ?**`\n",
        "\n",
        "`**Warning: Please beware of <unk>, an unknown word token.**`\n",
        "\n",
        "`**Warning: Please be careful of \" ำ \", the 'am' vowel. WangchanBERTa's internal preprocessing replaces all \" ำ \" to 'ํ' and 'า'**`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SI0fUulE5MH3"
      },
      "outputs": [],
      "source": [
        "def majority_vote_pos(examples):\n",
        "\n",
        "    ####################################################################################################################\n",
        "    # TO DO: Since the tokens from the output of the pretrained tokenizer\n",
        "    # do not match the tokens in the label tokens of the dataset,\n",
        "    # the task is to create a function to determine the POS tags of the tokens generated by the pretrained tokenizer.\n",
        "    # This should be done by referencing the POS tags in the label tokens. If a token partially overlaps with others,\n",
        "    # the POS tag from the segment with the greater number of characters should be assigned.\n",
        "    #\n",
        "    # Example :\n",
        "    # \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n",
        "    # \"การ\" has a POS tag of 21,\n",
        "    # and \"ประชุม\" has a POS tag of 39.\n",
        "    # Therefore, the POS tag for \"การประชุม\" is 39,\n",
        "    # as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n",
        "    #\n",
        "    # 'ทางวิชาการ' (10 chars) is formed from 'ทาง' (3 chars) + 'วิชาการ' (7 chars)\n",
        "    # \"ทาง\" has a POS tag of 26,\n",
        "    # and \"วิชาการ\" has a POS tag of 2.\n",
        "    # Therefore, the POS tag for \"ทางวิชาการ\" is 2,\n",
        "    # as \"ทางวิชาการ\" is derived more from the \"ทาง\" part than from the \"วิชาการ\" part.\n",
        "\n",
        "    # FILL CODE HERE\n",
        "    tokenized_inputs = tokenizer([examples[\"sentence\"]], is_split_into_words=True)\n",
        "    new_tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"])\n",
        "    new_pos_result = []\n",
        "    special = ['▁', '<s>', '</s>']\n",
        "    i_tl = 0\n",
        "    lword = 0\n",
        "    last_vote = None\n",
        "    # print(examples[\"sentence\"])\n",
        "    # print(\"old\", len(examples[\"label_tokens\"]), examples[\"label_tokens\"])\n",
        "    # print(\"new\", len(new_tokens), new_tokens)\n",
        "\n",
        "    # for i in range(len(examples[\"label_tokens\"])):\n",
        "    #     examples[\"label_tokens\"][i] = examples[\"label_tokens\"][i].replace('ำ','ํา')\n",
        "    for i in range(len(new_tokens)):\n",
        "        vote = {}\n",
        "        token_l = len(new_tokens[i]) - new_tokens[i].count('▁')\n",
        "        # for j in range(len(new_tokens[i])-1):\n",
        "            # if new_tokens[i][j] == 'ํ' and new_tokens[i][j+1] == 'า':\n",
        "            #     token_l -= 1\n",
        "        if new_tokens[i] == '<_>':\n",
        "            token_l = 1\n",
        "        if new_tokens[i] in special:\n",
        "            new_pos_result.append(-100)\n",
        "            continue\n",
        "        else:\n",
        "            if lword > 0:\n",
        "                vote[last_vote] = vote.get(last_vote, 0) + min(lword, token_l)\n",
        "                lword, token_l = lword - min(lword, token_l), token_l - min(lword, token_l)\n",
        "            while lword < token_l:\n",
        "                vote[examples[\"pos_tags\"][i_tl]] = vote.get(examples[\"pos_tags\"][i_tl], 0) + min(len(examples[\"label_tokens\"][i_tl]), token_l - lword)\n",
        "                last_vote = examples[\"pos_tags\"][i_tl]\n",
        "                lword += len(examples[\"label_tokens\"][i_tl]) + examples[\"label_tokens\"][i_tl].count('ำ')\n",
        "                # print(f\"{i_tl}, {i}: {examples['label_tokens'][i_tl]}\", lword, token_l)\n",
        "                i_tl += 1\n",
        "            new_pos_result.append(max(vote, key=vote.get))\n",
        "            lword -= token_l\n",
        "    assert len(new_pos_result) == len(new_tokens)\n",
        "    tokenized_inputs['input_ids'][0] = 0\n",
        "    tokenized_inputs['tokens'] = new_tokens\n",
        "    tokenized_inputs['labels'] = new_pos_result\n",
        "\n",
        "    return tokenized_inputs\n",
        "    \n",
        "    ####################################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': '8323', 'label_tokens': ['ระดับ', 'ที่ 2', ' ', 'เป็น', 'กลุ่ม', 'ตัวอักษร', 'ระดับกลาง', ' ', 'ได้แก่', ' ', 'พยัญชนะ', ' ', 'สระ', 'ที่', 'อยู่', 'ระดับ', 'เดียว', 'กับ', 'พยัญชนะ', ' ', 'เช่น', ' ', 'ั', ' ', 'ะ', ' ', 'า', ' ', 'ำ', ' ', 'เ', ' ', 'แ', ' ', 'โ', ' ', 'ใ', ' ', 'ไ'], 'pos_tags': [26, 18, 37, 41, 26, 26, 26, 37, 41, 37, 26, 37, 26, 36, 41, 26, 41, 38, 26, 37, 38, 37, 26, 37, 26, 37, 26, 37, 26, 37, 26, 37, 26, 37, 26, 37, 26, 37, 26], 'sentence': 'ระดับที่ 2 เป็นกลุ่มตัวอักษรระดับกลาง ได้แก่ พยัญชนะ สระที่อยู่ระดับเดียวกับพยัญชนะ เช่น ั ะ า ำ เ แ โ ใ ไ'}\n",
            "39\n",
            "39\n"
          ]
        }
      ],
      "source": [
        "ex = orchidl[\"train\"][8323]\n",
        "print(ex)\n",
        "print(len(ex[\"label_tokens\"]))\n",
        "print(len(ex[\"pos_tags\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [0, 176, 13, 9, 39, 9, 2470, 2346, 176, 336, 9, 172, 9, 4558, 9, 2219, 1156, 176, 900, 4558, 9, 67, 9, 6040, 9, 369, 9, 10, 9, 3489, 10, 9, 77, 9, 1514, 9, 3121, 9, 9199, 9, 4329, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'tokens': ['<s>', 'ระดับ', 'ที่', '<_>', '2', '<_>', 'เป็นกลุ่ม', 'ตัวอักษร', 'ระดับ', 'กลาง', '<_>', 'ได้แก่', '<_>', 'พยัญชนะ', '<_>', 'สระ', 'ที่อยู่', 'ระดับ', 'เดียวกับ', 'พยัญชนะ', '<_>', 'เช่น', '<_>', 'ั', '<_>', 'ะ', '<_>', 'า', '<_>', 'ํ', 'า', '<_>', 'เ', '<_>', 'แ', '<_>', 'โ', '<_>', 'ใ', '<_>', 'ไ', '</s>'], 'labels': [-100, 26, 18, 18, 18, 37, 26, 26, 26, 26, 37, 41, 37, 26, 37, 26, 41, 26, 41, 26, 37, 38, 37, 26, 37, 26, 37, 26, 37, 26, 26, 37, 26, 37, 26, 37, 26, 37, 26, 37, 26, -100]}\n",
            "42\n",
            "42\n"
          ]
        }
      ],
      "source": [
        "ex_t = majority_vote_pos(ex)\n",
        "print(ex_t)\n",
        "print(len(ex_t[\"labels\"]))\n",
        "print(len(ex_t[\"tokens\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<s>NOTUSED'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "andc\n",
            "2\n",
            "ทํา\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "a = \"abc\"\n",
        "a = a.replace('b', 'nd')\n",
        "print(a)\n",
        "\n",
        "th = \"ทำ\"\n",
        "print(len(th))\n",
        "th = th.replace('ำ','ํา')\n",
        "print(th)\n",
        "print(len(th))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "O5n4veYxo3rR"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46fb01fdd8094079bf86440e695e62f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/18500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7b292a64d924157a79e171576b664aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4625 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_orchid = orchidl.map(majority_vote_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ashRh72szWiM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id : 1899\n",
            "label_tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\n",
            "pos_tags : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n",
            "sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\n",
            "input_ids : [0, 26, 1189, 30, 7156, 217, 505, 9, 16, 3586, 8061, 10340, 9, 401, 23060, 9, 2276, 6497, 3488, 18, 6]\n",
            "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "tokens : ['<s>', 'โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '(', 'bi', 'ling', 'ual', '<_>', 't', 'ransfer', '<_>', 'di', 'ction', 'ary', ')', '</s>']\n",
            "labels : [-100, 25, 39, 38, 26, 26, 5, 37, 37, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 37, -100]\n"
          ]
        }
      ],
      "source": [
        "# hard test case\n",
        "example = tokenized_orchid[\"train\"][1899]\n",
        "for i in example :\n",
        "    print(i, \":\", example[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "7AL0Vqbv7cfb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/andre/anaconda3/envs/ML10/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at airesearch/wangchanberta-base-wiki-spm were not used when initializing RobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-wiki-spm and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_names = [\n",
        "    'wangchanberta-base-att-spm-uncased',\n",
        "    'wangchanberta-base-wiki-newmm',\n",
        "    'wangchanberta-base-wiki-ssg',\n",
        "    'wangchanberta-base-wiki-sefr',\n",
        "    'wangchanberta-base-wiki-spm',\n",
        "]\n",
        "\n",
        "#@title Choose Pretrained Model\n",
        "model_name = \"wangchanberta-base-wiki-spm\" #@param [\"wangchanberta-base-att-spm-uncased\", \"wangchanberta-base-wiki-newmm\", \"wangchanberta-base-wiki-syllable\", \"wangchanberta-base-wiki-sefr\", \"wangchanberta-base-wiki-spm\"]\n",
        "\n",
        "#create model\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    f\"airesearch/{model_name}\",\n",
        "    revision='main',\n",
        "    num_labels=47, id2label=id2label, label2id=label2id\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "pWTh0bMfP70g"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataCollatorForTokenClassification(tokenizer=ThaiRobertaTokenizer(name_or_path='airesearch/wangchanberta-base-wiki-spm', vocab_size=24005, model_max_length=416, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['<_>']}, clean_up_tokenization_spaces=True), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/andre/anaconda3/envs/ML10/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
            "The class this function is called from is 'ThaiRobertaTokenizer'.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
            "The class this function is called from is 'ThaiRobertaTokenizer'.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[   0, 1582, 4738,    9,  223,    9,   43,    6]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "text = \"การประชุมทางวิชาการ ครั้งที่ 1\"\n",
        "# model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
        "model_name2 = \"airesearch/wangchanberta-base-wiki-spm\"\n",
        "tokenizer_ = Tokenizer(model_name2).from_pretrained(model_name2)\n",
        "inputs = tokenizer_(text, return_tensors=\"pt\")\n",
        "inputs['input_ids'][0][0] = 0\n",
        "print(inputs)\n",
        "logits = model(**inputs).logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkhYDS4q7oxK"
      },
      "source": [
        "### #TODO 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVdITM5E7tQ5"
      },
      "source": [
        "Fine-tuning other pretrained model with our orchid corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "hBHlUamr7syk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/andre/anaconda3/envs/ML10/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31af34b57d674989a9e94f7140070ece",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1737 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9607, 'learning_rate': 7.121473805411629e-05, 'epoch': 0.86}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62426ef6e81948919ac8d9484d0b31dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/145 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.5800346732139587, 'eval_precision': 0.7470191907339415, 'eval_recall': 0.7605348850652228, 'eval_f1': 0.7537164516837411, 'eval_accuracy': 0.8382574307947442, 'eval_runtime': 3.6724, 'eval_samples_per_second': 1259.377, 'eval_steps_per_second': 39.483, 'epoch': 1.0}\n",
            "{'loss': 0.4203, 'learning_rate': 4.242947610823259e-05, 'epoch': 1.73}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1df68642170943a98ac9da7f22a6ffc1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/145 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.48804739117622375, 'eval_precision': 0.7820672710012472, 'eval_recall': 0.8095531705813214, 'eval_f1': 0.795572892011134, 'eval_accuracy': 0.8627758642684016, 'eval_runtime': 3.6575, 'eval_samples_per_second': 1264.52, 'eval_steps_per_second': 39.644, 'epoch': 2.0}\n",
            "{'loss': 0.2655, 'learning_rate': 1.3644214162348878e-05, 'epoch': 2.59}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c60a8f0332a743b18f828486daea457b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/145 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.45762985944747925, 'eval_precision': 0.7991593153886752, 'eval_recall': 0.8169136206863331, 'eval_f1': 0.8079389429352746, 'eval_accuracy': 0.8748309733384361, 'eval_runtime': 3.6726, 'eval_samples_per_second': 1259.327, 'eval_steps_per_second': 39.482, 'epoch': 3.0}\n",
            "{'train_runtime': 108.8086, 'train_samples_per_second': 510.07, 'train_steps_per_second': 15.964, 'train_loss': 0.5044170109513304, 'epoch': 3.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1737, training_loss=0.5044170109513304, metrics={'train_runtime': 108.8086, 'train_samples_per_second': 510.07, 'train_steps_per_second': 15.964, 'train_loss': 0.5044170109513304, 'epoch': 3.0})"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    #########################\n",
        "    output_dir=\"./train/wangchanberta-base-wiki-spm\",\n",
        "    per_device_eval_batch_size=32,\n",
        "    per_device_train_batch_size=32,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=1e-4\n",
        "\n",
        "    #########################\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    #########################\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_orchid[\"train\"],\n",
        "    eval_dataset=tokenized_orchid[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    ########################\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "KIdxvgUGCVsm"
      },
      "outputs": [],
      "source": [
        "######## EVALUATE YOUR MODEL ########\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"./train/wangchanberta-base-wiki-spm/checkpoint-1737\") \n",
        "\n",
        "test_data = tokenized_orchid[\"test\"]\n",
        "\n",
        "y_test = []\n",
        "for inputs in test_data:\n",
        "  y_test.append(inputs['labels'])\n",
        "\n",
        "y_pred = []\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "for inputs in test_data:\n",
        "    text = inputs['sentence']\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    inputs['input_ids'][0][0] = 0\n",
        "    with torch.no_grad():\n",
        "        pred = model(**inputs).logits\n",
        "        predictions = torch.argmax(pred, dim=2)\n",
        "        y_pred.append(predictions.tolist()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[26, 29, 29, 39, 26, 26, 26, 37, 26, 26, 26, 41, 37, 26, 26, 41, 26, 1]\n",
            "[-100, 29, 37, 39, 26, 26, 26, 37, 26, 26, 26, 41, 37, 26, 26, 39, 26, -100]\n"
          ]
        }
      ],
      "source": [
        "print(y_pred[0])\n",
        "print(y_test[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speacial_tag : 9250\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tag</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>correct_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-100</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>35.294118</td>\n",
              "      <td>25.531915</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>24.186593</td>\n",
              "      <td>60.490196</td>\n",
              "      <td>34.556147</td>\n",
              "      <td>617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>8.450704</td>\n",
              "      <td>5.172414</td>\n",
              "      <td>6.417112</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>42.222222</td>\n",
              "      <td>33.333333</td>\n",
              "      <td>37.254902</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4</td>\n",
              "      <td>64.935065</td>\n",
              "      <td>80.645161</td>\n",
              "      <td>71.942446</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5</td>\n",
              "      <td>50.694444</td>\n",
              "      <td>46.496815</td>\n",
              "      <td>48.504983</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6</td>\n",
              "      <td>80.301129</td>\n",
              "      <td>83.989501</td>\n",
              "      <td>82.103913</td>\n",
              "      <td>640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8</td>\n",
              "      <td>53.674833</td>\n",
              "      <td>62.924282</td>\n",
              "      <td>57.932692</td>\n",
              "      <td>241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>77.887198</td>\n",
              "      <td>83.253589</td>\n",
              "      <td>80.481036</td>\n",
              "      <td>870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>79.035639</td>\n",
              "      <td>80.728051</td>\n",
              "      <td>79.872881</td>\n",
              "      <td>377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>38.916256</td>\n",
              "      <td>75.961538</td>\n",
              "      <td>51.465798</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>56.923077</td>\n",
              "      <td>77.894737</td>\n",
              "      <td>65.777778</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>79.365079</td>\n",
              "      <td>93.75</td>\n",
              "      <td>85.959885</td>\n",
              "      <td>300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>78.888889</td>\n",
              "      <td>83.203125</td>\n",
              "      <td>80.988593</td>\n",
              "      <td>213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>86.671132</td>\n",
              "      <td>95.851852</td>\n",
              "      <td>91.030601</td>\n",
              "      <td>1294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>85.714286</td>\n",
              "      <td>70.588235</td>\n",
              "      <td>77.419355</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>88.612565</td>\n",
              "      <td>90.266667</td>\n",
              "      <td>89.431968</td>\n",
              "      <td>1354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>64.414414</td>\n",
              "      <td>89.375</td>\n",
              "      <td>74.86911</td>\n",
              "      <td>143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>79.381443</td>\n",
              "      <td>81.052632</td>\n",
              "      <td>80.208333</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>92.203035</td>\n",
              "      <td>95.501355</td>\n",
              "      <td>93.823216</td>\n",
              "      <td>1762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>70.12987</td>\n",
              "      <td>82.309469</td>\n",
              "      <td>75.733107</td>\n",
              "      <td>1782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>84.896245</td>\n",
              "      <td>90.044212</td>\n",
              "      <td>87.394484</td>\n",
              "      <td>27902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>48.731642</td>\n",
              "      <td>57.389937</td>\n",
              "      <td>52.707581</td>\n",
              "      <td>365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>65.408805</td>\n",
              "      <td>83.2</td>\n",
              "      <td>73.239437</td>\n",
              "      <td>104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>29</td>\n",
              "      <td>71.705426</td>\n",
              "      <td>95.979248</td>\n",
              "      <td>82.085413</td>\n",
              "      <td>740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>31</td>\n",
              "      <td>64.693878</td>\n",
              "      <td>72.264438</td>\n",
              "      <td>68.269921</td>\n",
              "      <td>1902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>32</td>\n",
              "      <td>74.248927</td>\n",
              "      <td>95.58011</td>\n",
              "      <td>83.574879</td>\n",
              "      <td>173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>33</td>\n",
              "      <td>38.926174</td>\n",
              "      <td>59.793814</td>\n",
              "      <td>47.154472</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>34</td>\n",
              "      <td>81.818182</td>\n",
              "      <td>64.285714</td>\n",
              "      <td>72.0</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>35</td>\n",
              "      <td>87.654321</td>\n",
              "      <td>63.392857</td>\n",
              "      <td>73.57513</td>\n",
              "      <td>71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>36</td>\n",
              "      <td>87.301587</td>\n",
              "      <td>85.9375</td>\n",
              "      <td>86.614173</td>\n",
              "      <td>880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>37</td>\n",
              "      <td>88.697196</td>\n",
              "      <td>97.616815</td>\n",
              "      <td>92.943496</td>\n",
              "      <td>12493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>38</td>\n",
              "      <td>76.097057</td>\n",
              "      <td>86.527737</td>\n",
              "      <td>80.977888</td>\n",
              "      <td>2948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>39</td>\n",
              "      <td>76.963999</td>\n",
              "      <td>84.352542</td>\n",
              "      <td>80.489067</td>\n",
              "      <td>6221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>40</td>\n",
              "      <td>52.507837</td>\n",
              "      <td>57.858377</td>\n",
              "      <td>55.05341</td>\n",
              "      <td>670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>41</td>\n",
              "      <td>53.082379</td>\n",
              "      <td>71.49155</td>\n",
              "      <td>60.926738</td>\n",
              "      <td>1946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>42</td>\n",
              "      <td>56.830189</td>\n",
              "      <td>86.452354</td>\n",
              "      <td>68.579235</td>\n",
              "      <td>753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>43</td>\n",
              "      <td>87.535817</td>\n",
              "      <td>94.290123</td>\n",
              "      <td>90.787519</td>\n",
              "      <td>611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>45</td>\n",
              "      <td>85.828343</td>\n",
              "      <td>92.274678</td>\n",
              "      <td>88.93485</td>\n",
              "      <td>430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>46</td>\n",
              "      <td>87.356322</td>\n",
              "      <td>89.940828</td>\n",
              "      <td>88.629738</td>\n",
              "      <td>304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>accuracy=87.48</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               tag  precision     recall   f1_score correct_count\n",
              "0             -100          -        0.0          -             0\n",
              "1                0       20.0  35.294118  25.531915             6\n",
              "2                1  24.186593  60.490196  34.556147           617\n",
              "3                2   8.450704   5.172414   6.417112             6\n",
              "4                3  42.222222  33.333333  37.254902            19\n",
              "5                4  64.935065  80.645161  71.942446            50\n",
              "6                5  50.694444  46.496815  48.504983            73\n",
              "7                6  80.301129  83.989501  82.103913           640\n",
              "8                7          -        0.0          -             0\n",
              "9                8  53.674833  62.924282  57.932692           241\n",
              "10              10  77.887198  83.253589  80.481036           870\n",
              "11              11  79.035639  80.728051  79.872881           377\n",
              "12              12  38.916256  75.961538  51.465798            79\n",
              "13              13          -        0.0          -             0\n",
              "14              14  56.923077  77.894737  65.777778            74\n",
              "15              15  79.365079      93.75  85.959885           300\n",
              "16              16          -        0.0          -             0\n",
              "17              17  78.888889  83.203125  80.988593           213\n",
              "18              18  86.671132  95.851852  91.030601          1294\n",
              "19              19          -        0.0          -             0\n",
              "20              20  85.714286  70.588235  77.419355            12\n",
              "21              21  88.612565  90.266667  89.431968          1354\n",
              "22              22  64.414414     89.375   74.86911           143\n",
              "23              23  79.381443  81.052632  80.208333            77\n",
              "24              24  92.203035  95.501355  93.823216          1762\n",
              "25              25   70.12987  82.309469  75.733107          1782\n",
              "26              26  84.896245  90.044212  87.394484         27902\n",
              "27              27  48.731642  57.389937  52.707581           365\n",
              "28              28  65.408805       83.2  73.239437           104\n",
              "29              29  71.705426  95.979248  82.085413           740\n",
              "30              31  64.693878  72.264438  68.269921          1902\n",
              "31              32  74.248927   95.58011  83.574879           173\n",
              "32              33  38.926174  59.793814  47.154472            58\n",
              "33              34  81.818182  64.285714       72.0            18\n",
              "34              35  87.654321  63.392857   73.57513            71\n",
              "35              36  87.301587    85.9375  86.614173           880\n",
              "36              37  88.697196  97.616815  92.943496         12493\n",
              "37              38  76.097057  86.527737  80.977888          2948\n",
              "38              39  76.963999  84.352542  80.489067          6221\n",
              "39              40  52.507837  57.858377   55.05341           670\n",
              "40              41  53.082379   71.49155  60.926738          1946\n",
              "41              42  56.830189  86.452354  68.579235           753\n",
              "42              43  87.535817  94.290123  90.787519           611\n",
              "43              45  85.828343  92.274678   88.93485           430\n",
              "44              46  87.356322  89.940828  88.629738           304\n",
              "45  accuracy=87.48                                               "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "evaluation_report(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMpXErqPBv2I"
      },
      "source": [
        "### #TODO 5\n",
        "\n",
        "Compare the results between both models. Are they comparable? (Think about the ground truths of both models).\n",
        "\n",
        "Propose a way to fairly evaluate the models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UigMzZVNCDuS"
      },
      "source": [
        "<b>Write your answer here :</b>\n",
        "wangchanberta-base-att-spm-uncased come from wider data scope which cause model better in generalization. On the other hand, wangchanberta-base-wiki-spm come from just only wikipedia data which cause model better in specific domain. So, wangchanberta-base-att-spm-uncased reached higher accuracy than wangchanberta-base-wiki-spm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYN0X9HWVuUe"
      },
      "source": [
        "A note on preprocessing data.\n",
        "\n",
        "``process_transformers`` in ``thaixtransformers.preprocess`` also provides a preprocess code that deals with many issues such as casing, text cleaning, and white space replacement with <_>. You can also use this to preprocess your text. Note that space replacement is done automatically without preprocessing in thaixtransformers.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "ML10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
