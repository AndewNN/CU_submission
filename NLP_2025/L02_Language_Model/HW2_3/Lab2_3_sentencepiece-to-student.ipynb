{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iU5fRQwhEdJy"
   },
   "source": [
    "# Subword Tokenization\n",
    "\n",
    "In this exercise, we will learn how to train our own subword tokenizers with different algorithms: BPE and Unigram. We will use `sentencepiece`, a library from Google to help create our tokenizers.\n",
    "\n",
    "## Ref:\n",
    "https://github.com/google/sentencepiece/blob/master/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI9gRZlUE80g"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "1pOsV-jaW975"
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n",
    "# !wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/kratoo-40000000-40002000.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSiDpG9WE-cT"
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "OQd7M6gLWPLN"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OifbmMIstzs8"
   },
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "-FnIDvb1lMuh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1060318"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pantip_text = []\n",
    "with open('kratoo-40000000-40002000.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "    for json_str in json_list:\n",
    "        result = json.loads(json_str)\n",
    "        pantip_text.append(f\"{result['title']}\\n{result['content']}\\n\")\n",
    "sum([len(t) for t in pantip_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "yaaQVXZ8A0j1"
   },
   "outputs": [],
   "source": [
    "with open(\"pra-apai-manee-ch1-50.txt\") as f:\n",
    "  pra_apai_manee_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "LksJKc9MA5F_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100605"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(t) for t in pra_apai_manee_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "RbdfkF-vAoie"
   },
   "outputs": [],
   "source": [
    "pantip_train_text = pantip_text[:int(len(pantip_text)*0.8)]\n",
    "pantip_test_text = pantip_text[int(len(pantip_text)*0.8):]\n",
    "\n",
    "pam_train_text = pra_apai_manee_data[:int(len(pra_apai_manee_data)*0.8)] #pam = pra_apai_manee\n",
    "pam_test_text = pra_apai_manee_data[int(len(pra_apai_manee_data)*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhwcH0Aot1XI"
   },
   "source": [
    "## Run tokenizer training\n",
    "\n",
    "The Python wrapper provides multiple APIs for training our tokenizers\n",
    "\n",
    "1. `spm.SentencePieceTrainer.train(input='input.txt', model_prefix='m', vocab_size=vocab_size, model_type=model_type)`\n",
    "  <br> This will output the tokenizer files `m.model` and `m.vocab` that can be later loaded into `SentencePieceProcessor`.\n",
    "  <br><br>\n",
    "2. `spm.SentencePieceTrainer.train(sentence_iterator=iterator, model_writer=obj_with_write_method, vocab_size=vocab_size, model_type=model_type)`\n",
    "  <br> This method will require a file object e.g. `obj_with_write_method = io.BytesIO()`. The advantage of this method is you can run sentencepiece on environments that have limited access to the local file system. But you will still have to save the model file if you want to re-use the model else you will have to train it again.\n",
    "<br><br>\n",
    "3.  `spm.SentencePieceTrainer.train('--input=input.txt --model_prefix=m --vocab_size=vocab_size --model_type=model_type')`\n",
    "<br> Same as no.1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3XeFFYw-T_0"
   },
   "source": [
    "### Unigram tokenizer\n",
    "\n",
    "We are going to start with training a unigram tokenizer. You can use any method of training one. Make sure to set vocab_size to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFCfHphd15g9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./pra-apai-manee-ch1-50.txt\n",
      "  input_format: \n",
      "  model_prefix: pam_unigram\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ./pra-apai-manee-ch1-50.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 20444 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1107746\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9585% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=64\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999585\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 20395 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=515438\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 337290 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 20395\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 40105\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 40105 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=59234 obj=41.9184 num_tokens=156714 num_tokens/piece=2.64568\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=48977 obj=36.3873 num_tokens=158307 num_tokens/piece=3.23227\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=36278 obj=36.5994 num_tokens=169490 num_tokens/piece=4.67198\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=35643 obj=36.1411 num_tokens=169808 num_tokens/piece=4.76413\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=26466 obj=37.2587 num_tokens=181764 num_tokens/piece=6.86783\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=26351 obj=36.8674 num_tokens=181930 num_tokens/piece=6.9041\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19682 obj=38.263 num_tokens=195339 num_tokens/piece=9.92475\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19645 obj=37.8741 num_tokens=195478 num_tokens/piece=9.95052\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14712 obj=39.4633 num_tokens=209250 num_tokens/piece=14.2231\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14705 obj=39.1029 num_tokens=209261 num_tokens/piece=14.2306\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11020 obj=40.791 num_tokens=223395 num_tokens/piece=20.2718\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11019 obj=40.4838 num_tokens=223398 num_tokens/piece=20.2739\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8262 obj=42.28 num_tokens=238436 num_tokens/piece=28.8594\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8262 obj=41.9667 num_tokens=238438 num_tokens/piece=28.8596\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6195 obj=43.8823 num_tokens=254127 num_tokens/piece=41.0213\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6195 obj=43.5648 num_tokens=254127 num_tokens/piece=41.0213\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4646 obj=45.6703 num_tokens=271152 num_tokens/piece=58.3625\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4646 obj=45.3445 num_tokens=271162 num_tokens/piece=58.3646\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3484 obj=47.6443 num_tokens=290036 num_tokens/piece=83.248\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3484 obj=47.2582 num_tokens=290269 num_tokens/piece=83.3149\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2613 obj=49.9819 num_tokens=313814 num_tokens/piece=120.097\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2613 obj=49.503 num_tokens=313815 num_tokens/piece=120.098\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1959 obj=52.6734 num_tokens=341963 num_tokens/piece=174.56\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1959 obj=52.1029 num_tokens=341963 num_tokens/piece=174.56\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1469 obj=55.2904 num_tokens=376088 num_tokens/piece=256.016\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1469 obj=54.6205 num_tokens=376088 num_tokens/piece=256.016\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1101 obj=58.1626 num_tokens=412729 num_tokens/piece=374.867\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1101 obj=57.4646 num_tokens=412729 num_tokens/piece=374.867\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=57.4714 num_tokens=412821 num_tokens/piece=375.292\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=57.4691 num_tokens=412821 num_tokens/piece=375.292\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: pam_unigram.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: pam_unigram.vocab\n"
     ]
    }
   ],
   "source": [
    "## Train\n",
    "spm.SentencePieceTrainer.train(input='./pra-apai-manee-ch1-50.txt', model_prefix='pam_unigram', vocab_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdXPaoW3_v2T"
   },
   "source": [
    "### Q1 MCV\n",
    "\n",
    "How many tokens did you get when tokenizing the following sentence with your unigram tokenizer: <br>\n",
    "'อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_pam = spm.SentencePieceProcessor(model_file='pam_unigram.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "J1bO3s-z-PLb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sp_pam.encode('อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม', out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKkc1D-hAFxl"
   },
   "source": [
    "### BPE Tokenizer\n",
    "\n",
    "Now try training a BPE tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AiXj57rh-PIv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./pra-apai-manee-ch1-50.txt\n",
      "  input_format: \n",
      "  model_prefix: pam_bpe\n",
      "  model_type: BPE\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ./pra-apai-manee-ch1-50.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 20444 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1107746\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9585% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=64\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999585\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 20395 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 20395\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 40105\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11016 min_freq=152\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4830 size=20 all=3702 active=2369 piece=อน\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3105 size=40 all=5576 active=4243 piece=้อง\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2281 size=60 all=7579 active=6246 piece=ิน\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1841 size=80 all=9727 active=8394 piece=▁จ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1549 size=100 all=11898 active=10565 piece=้วย\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1457 min_freq=176\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1244 size=120 all=14376 active=3390 piece=น้\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1079 size=140 all=17120 active=6134 piece=▁ให้\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=938 size=160 all=19801 active=8815 piece=รับ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=857 size=180 all=22793 active=11807 piece=หน้า\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=777 size=200 all=25427 active=14441 piece=ื่อ\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=776 min_freq=104\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=708 size=220 all=28516 active=4252 piece=ฟัง\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=642 size=240 all=31587 active=7323 piece=▁มา\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=587 size=260 all=34270 active=10006 piece=ตา\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=536 size=280 all=37022 active=12758 piece=สั่ง\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=507 size=300 all=40012 active=15748 piece=ตรี\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=507 min_freq=57\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=473 size=320 all=42625 active=4502 piece=แม่\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=439 size=340 all=45126 active=7003 piece=แท\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=419 size=360 all=48076 active=9953 piece=ห์\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=399 size=380 all=50800 active=12677 piece=ถือ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=377 size=400 all=53400 active=15277 piece=ทหาร\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=377 min_freq=39\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=359 size=420 all=55779 active=4892 piece=พู\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=347 size=440 all=57576 active=6689 piece=กรา\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=325 size=460 all=60101 active=9214 piece=พบ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=309 size=480 all=62475 active=11588 piece=��ั้ง\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=288 size=500 all=65085 active=14198 piece=เหลือ\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=287 min_freq=30\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=274 size=520 all=67642 active=5645 piece=วงศ์\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=262 size=540 all=69943 active=7946 piece=หลาย\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=251 size=560 all=72140 active=10143 piece=็ด\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=243 size=580 all=74483 active=12486 piece=อ่อน\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=233 size=600 all=76550 active=14553 piece=ทัย\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=233 min_freq=23\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=224 size=620 all=78579 active=5792 piece=สัญ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=215 size=640 all=80380 active=7593 piece=ไฉน\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=207 size=660 all=82361 active=9574 piece=เสร็จ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=200 size=680 all=84352 active=11565 piece=พรั่ง\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=192 size=700 all=86335 active=13548 piece=พาน\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=192 min_freq=20\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=186 size=720 all=88362 active=6207 piece=กิน\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=179 size=740 all=90242 active=8087 piece=ทธิ์\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=175 size=760 all=92373 active=10218 piece=บิตุ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=170 size=780 all=93920 active=11765 piece=ล้อม\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=164 size=800 all=96019 active=13864 piece=ค่ํา\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=164 min_freq=17\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=159 size=820 all=97878 active=6575 piece=▁เอา\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=153 size=840 all=99253 active=7950 piece=ไหล\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=149 size=860 all=100916 active=9613 piece=ผลึก\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=146 size=880 all=102508 active=11205 piece=กําปั่น\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=141 size=900 all=104394 active=13091 piece=พลับพล\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=140 min_freq=15\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=137 size=920 all=105770 active=6549 piece=เขต\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: pam_bpe.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: pam_bpe.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(input='./pra-apai-manee-ch1-50.txt', model_prefix='pam_bpe', vocab_size=1000, model_type='bpe')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrQwGmL5AMXc"
   },
   "source": [
    "### Q2 MCV\n",
    "\n",
    "How many tokens did you get when tokenizing the following sentence with your BPE tokenizer: <br>\n",
    "'อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "0AXuzyaN-PEr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe = spm.SentencePieceProcessor(model_file='pam_bpe.model')\n",
    "len(bpe.encode('อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม', out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbb6C6-IS_Ly"
   },
   "source": [
    "These are some of your vocabs. Note that you will see \"▁\" (U+2581) in every type of tokenizer in SentencePiece since it makes it possible to perform detokenization \\(unsplit your sentences\\) without relying on language-specific resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Aa9j6XrTKjyA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> | <s> | </s> | ่า | ้า | อง | ระ | ํา | รา | อย | ่ง | าย | จะ | ัง | ัน | ับ | ▁เ | มา | ้ว | ี่ | ม่ | ให | อน | ีย | ้น | ็น | าม | พระ | าง | ้ง | กล | ให้ | หน | ัก | ไม่ | หล | ่น | ึง | ทั | ตร | ▁แ | หม | ้อง | ไป | าร | ิด | ข้า | ว่า | คร | ือ | เส | ล้ว | เป | ประ | าน | ั่ง | อก | ที่ | ▁ฯ | ▁๏ | พล | เล | ิน | ัด | ได | นาง | ▁จะ | ู่ | ได้ | ึก | ทร | ค์ | ี้ | พร | ทั้ง | อม | เป็น | สุ | ัย | ื่ | าว | กร | ▁จ | ห็น | ิง | เร | ่าง | วง | ู้ | ใจ | ต่ | ก็ | ▁พระ | ือน | เจ | ียง | รร | อา | อยู่ | ตาม | วน | ▁พ | ้วย | ัว | ลา | รี | ถึง | ั้น | ด้วย | เช | องค์ | ย์ | เข | สน | สม | ▁แล้ว | คว | าก | ใน | ่าย | อบ | หมือน | น้ | ▁ส | ูก | เจ้า | เค | มี | กระ | กัน | เม | ิ่ง | เข้า | ทรง | อด | พรา | วย | คิด | เห็น | เก | ▁นาง | ณ์ | ▁ให้ | ้ม | ิต | เด | สง | ลง | ่าน | ุก | ทํา | วา | ดู | ▁อ | พี่ | หา | รัก | วก | ฝ่าย | ▁จึง | คน | ้อม | รับ | ความ | เหมือน | มิ | น้อง | ▁ไม่ | ยง | ือง | ช่ | แล้ว | อย่า | ดี | ั่น | การ | คล | ▁ต | ศรี | ้าง | ืน | พา | หน้า | ▁ทั้ง | ั้ง | ่อน | ัญ | ยว | ัส | แก | ปรา | ▁ฝ่าย | ื่น | ยา | หว | กับ | กํา | รู้ | เหล | กา | ดา | ้าย | ื่อ | หร | แต่ | เน | นี้ | สํา | ึ่ง | ้อ | สาร | ละ | ลูก | เสีย | บุ | ลัง | กลับ | สา | ทัพ | ขึ | ▁เห็น | เท | ฟัง | ▁แต่ | ไว | ทุก | นั่ง | ขึ้น | ้อย | นา | ัต | สุด | ใคร | สอง | ไร | ชา | ุด | รบ | วิ | หย | ปร | สิ | ▁มา | วัน | ัตร | ช่วย | น้ํา | ูล | ▁บ | ออก | ิย์ | รง | เพ | ไว้ | ้อน | ล่ | หรือ | จร | าด | กษ | เย | ไพร | ตา | แจ | ็จ | ดัง | วรร | ยัง | ▁พอ | นั้น | หลัง | เว | ัตริย์ | เห | เขา | ขอ | คํา | ค่ | ลี | ทูล | ▁เป | ข้าง | สั่ง | วาย | วัง | ื้อ | หมณ์ | ▁ถึง | ตาย | ผู้ | มาร | แล | ทาง | ▁แม | ื่อง | ปล | สาม | ตัว | พร้อม | ฝร | จน | ่ม | ตรี | ฝรั่ง | ิ์ | ้ํา | ▁เป็น | อภ | จิต | โฉ | ราช | พราหมณ์ | หาร | ตี | ชี | ฝ้า | รุ | ที | แต | ื้น | ลม | โฉม | แม่ | ▁ถ | นาย | ้าว | ▁ช | ่อ | น้อย | ของ | สร | ษา | เลี้ | ียน | หญ | กษัตริย์ | ศ์ | โย | สาว | หมาย | หาย | อย่าง | แท | ฉัน | เรา | เมือง | ขว | วรรณ | ุทร | เอ | รํา | ต้อง | ▁ด้วย | จึง | ลัย | บา | สมุทร | พลาง | นึก | สะ | มัน | สุวรรณ | ห์ | ถาม | ศึก | หญิง | ▁ต่าง | กุ | พิ | เฝ้า | ขัด | สัย | อุ | แค | แจ้ง | ียว | ตรัส | ▁อย่า | ิญ | สิ้น | เลย | ห้า | ถือ | ผล | กลาง | เสียง | อั | คอย | คุ | ไหน | ่ว | อภัย | หนี | ชม | บรร | ชาย | ▁เส | แน | ค่อย | โอ | ท้าว | ▁ย | ทหาร | เล่า | พวก | ติ | ตรา | นิ | ุง | ภา | รม | ่ํา | ัล | ญา | ิดา | ิ้ม | เอา | ่อง | ๋ย | จํา | จับ | ุ่ง | พู | ทธ | ไข | ▁ประ | ▁พวก | ลังกา | ▁อัน | ษฐ | ้าน | ินสมุทร | ศร | จริง | เปล | คง | ักษ | ▁บ้าง | ▁เข้า | มน | ▁เจ | สี | กรา | บอก | ไม | ที่ยว | รู | ทอง | เลี้ยง | ▁ไป | แสน | จา | ไพร่ | ใหญ | กาย | ▁จน | กลัว | ใส | ครั้น | นี | เกล | ซึ่ง | พบ | เคย | พัก | พราะ | พัน | ่าว | ▁เหมือน | เคร | คา | กล้ง | ครา | เคียง | ัณ | ชาว | หนัก | ▁เจ้า | ริ | ร้อง | ื่อน | จง | ตั้ง | ต่าง | ▁ที่ | กรุง | จัด | ▁จง | บน | อาย | ล้วน | ดิ์ | เพล | ทรา | ียบ | ▁ครั้น | ทิ | กอง | ▁ก็ | ▁ได้'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_vocabs = [sp_pam.id_to_piece(id) for id in range(sp_pam.get_piece_size())]\n",
    "\" | \".join(unigram_vocabs[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "2TsXA0UqN5LN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> | <s> | </s> | ่า | ้า | อง | ระ | ํา | รา | อย | ่ง | าย | จะ | ัง | ัน | ับ | ▁เ | มา | ้ว | ี่ | ม่ | ให | อน | ีย | ้น | ็น | าม | พระ | าง | ้ง | กล | ให้ | หน | ัก | ไม่ | หล | ่น | ึง | ทั | ตร | ▁แ | หม | ้อง | ไป | าร | ิด | ข้า | ว่า | คร | ือ | เส | ล้ว | เป | ประ | าน | ั่ง | อก | ที่ | ▁ฯ | ▁๏ | พล | เล | ิน | ัด | ได | นาง | ▁จะ | ู่ | ได้ | ึก | ทร | ค์ | ี้ | พร | ทั้ง | อม | เป็น | สุ | ัย | ื่ | าว | กร | ▁จ | ห็น | ิง | เร | ่าง | วง | ู้ | ใจ | ต่ | ก็ | ▁พระ | ือน | เจ | ียง | รร | อา | อยู่ | ตาม | วน | ▁พ | ้วย | ัว | ลา | รี | ถึง | ั้น | ด้วย | เช | องค์ | ย์ | เข | สน | สม | ▁แล้ว | คว | าก | ใน | ่าย | อบ | หมือน | น้ | ▁ส | ูก | เจ้า | เค | มี | กระ | กัน | เม | ิ่ง | เข้า | ทรง | อด | พรา | วย | คิด | เห็น | เก | ▁นาง | ณ์ | ▁ให้ | ้ม | ิต | เด | สง | ลง | ่าน | ุก | ทํา | วา | ดู | ▁อ | พี่ | หา | รัก | วก | ฝ่าย | ▁จึง | คน | ้อม | รับ | ความ | เหมือน | มิ | น้อง | ▁ไม่ | ยง | ือง | ช่ | แล้ว | อย่า | ดี | ั่น | การ | คล | ▁ต | ศรี | ้าง | ืน | พา | หน้า | ▁ทั้ง | ั้ง | ่อน | ัญ | ยว | ัส | แก | ปรา | ▁ฝ่าย | ื่น | ยา | หว | กับ | กํา | รู้ | เหล | กา | ดา | ้าย | ื่อ | หร | แต่ | เน | นี้ | สํา | ึ่ง | ้อ | สาร | ละ | ลูก | เสีย | บุ | ลัง | กลับ | สา | ทัพ | ขึ | ▁เห็น | เท | ฟัง | ▁แต่ | ไว | ทุก | นั่ง | ขึ้น | ้อย | นา | ัต | สุด | ใคร | สอง | ไร | ชา | ุด | รบ | วิ | หย | ปร | สิ | ▁มา | วัน | ัตร | ช่วย | น้ํา | ูล | ▁บ | ออก | ิย์ | รง | เพ | ไว้ | ้อน | ล่ | หรือ | จร | าด | กษ | เย | ไพร | ตา | แจ | ็จ | ดัง | วรร | ยัง | ▁พอ | นั้น | หลัง | เว | ัตริย์ | เห | เขา | ขอ | คํา | ค่ | ลี | ทูล | ▁เป | ข้าง | สั่ง | วาย | วัง | ื้อ | หมณ์ | ▁ถึง | ตาย | ผู้ | มาร | แล | ทาง | ▁แม | ื่อง | ปล | สาม | ตัว | พร้อม | ฝร | จน | ่ม | ตรี | ฝรั่ง | ิ์ | ้ํา | ▁เป็น | อภ | จิต | โฉ | ราช | พราหมณ์ | หาร | ตี | ชี | ฝ้า | รุ | ที | แต | ื้น | ลม | โฉม | แม่ | ▁ถ | นาย | ้าว | ▁ช | ่อ | น้อย | ของ | สร | ษา | เลี้ | ียน | หญ | กษัตริย์ | ศ์ | โย | สาว | หมาย | หาย | อย่าง | แท | ฉัน | เรา | เมือง | ขว | วรรณ | ุทร | เอ | รํา | ต้อง | ▁ด้วย | จึง | ลัย | บา | สมุทร | พลาง | นึก | สะ | มัน | สุวรรณ | ห์ | ถาม | ศึก | หญิง | ▁ต่าง | กุ | พิ | เฝ้า | ขัด | สัย | อุ | แค | แจ้ง | ียว | ตรัส | ▁อย่า | ิญ | สิ้น | เลย | ห้า | ถือ | ผล | กลาง | เสียง | อั | คอย | คุ | ไหน | ่ว | อภัย | หนี | ชม | บรร | ชาย | ▁เส | แน | ค่อย | โอ | ท้าว | ▁ย | ทหาร | เล่า | พวก | ติ | ตรา | นิ | ุง | ภา | รม | ่ํา | ัล | ญา | ิดา | ิ้ม | เอา | ่อง | ๋ย | จํา | จับ | ุ่ง | พู | ทธ | ไข | ▁ประ | ▁พวก | ลังกา | ▁อัน | ษฐ | ้าน | ินสมุทร | ศร | จริง | เปล | คง | ักษ | ▁บ้าง | ▁เข้า | มน | ▁เจ | สี | กรา | บอก | ไม | ที่ยว | รู | ทอง | เลี้ยง | ▁ไป | แสน | จา | ไพร่ | ใหญ | กาย | ▁จน | กลัว | ใส | ครั้น | นี | เกล | ซึ่ง | พบ | เคย | พัก | พราะ | พัน | ่าว | ▁เหมือน | เคร | คา | กล้ง | ครา | เคียง | ัณ | ชาว | หนัก | ▁เจ้า | ริ | ร้อง | ื่อน | จง | ตั้ง | ต่าง | ▁ที่ | กรุง | จัด | ▁จง | บน | อาย | ล้วน | ดิ์ | เพล | ทรา | ียบ | ▁ครั้น | ทิ | กอง | ▁ก็ | ▁ได้'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_vocabs = [bpe.id_to_piece(id) for id in range(bpe.get_piece_size())]\n",
    "\" | \".join(bpe_vocabs[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu6QnnRfQyFj"
   },
   "source": [
    "### User-defined symbols\n",
    "\n",
    "Another important concept to know of is User-defined symbols. These special symbols are reserved for a special purpose \\(e.g.\\, the \\<MASK\\> token used in BERT) and will always be tokenized into one token.\n",
    "\n",
    "Refer to the documentation for ways to add these special tokens to your tokenizer.\n",
    "\n",
    "https://github.com/google/sentencepiece/blob/master/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEFOj62ZEdzT"
   },
   "source": [
    "## Train another tokenizer on another domain\n",
    "\n",
    "Now try training another unigram tokenizer on `pantip_text` and we will use it to compare with the unigram tokenizer we trained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7-QkA1eMZFf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./kratoo-40000000-40002000.jsonl\n",
      "  input_format: \n",
      "  model_prefix: pantip_unigram\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ./kratoo-40000000-40002000.jsonl\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (7030 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1770 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 143 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=680466\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=198\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1770 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=289188\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 166087 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1770\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 23832\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 23832 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=34900 obj=45.8518 num_tokens=134860 num_tokens/piece=3.86418\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=29451 obj=37.1869 num_tokens=136021 num_tokens/piece=4.61855\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=21881 obj=37.5171 num_tokens=142714 num_tokens/piece=6.52228\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=21655 obj=37.0263 num_tokens=143377 num_tokens/piece=6.62097\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=16145 obj=37.9659 num_tokens=151111 num_tokens/piece=9.35962\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=16104 obj=37.6151 num_tokens=151193 num_tokens/piece=9.38854\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12048 obj=38.6857 num_tokens=159229 num_tokens/piece=13.2162\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12036 obj=38.3612 num_tokens=159439 num_tokens/piece=13.2468\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9021 obj=39.6208 num_tokens=168200 num_tokens/piece=18.6454\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9019 obj=39.298 num_tokens=168218 num_tokens/piece=18.6515\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6764 obj=40.6584 num_tokens=177335 num_tokens/piece=26.2175\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6764 obj=40.3735 num_tokens=177343 num_tokens/piece=26.2187\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5073 obj=41.8694 num_tokens=186786 num_tokens/piece=36.8196\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5073 obj=41.5891 num_tokens=186788 num_tokens/piece=36.82\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3803 obj=43.1752 num_tokens=197104 num_tokens/piece=51.8286\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3803 obj=42.8659 num_tokens=197108 num_tokens/piece=51.8296\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2852 obj=44.5519 num_tokens=208434 num_tokens/piece=73.0835\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2852 obj=44.2409 num_tokens=208435 num_tokens/piece=73.0838\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2139 obj=46.1468 num_tokens=221052 num_tokens/piece=103.344\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2139 obj=45.7957 num_tokens=221053 num_tokens/piece=103.344\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1604 obj=47.922 num_tokens=235591 num_tokens/piece=146.877\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1604 obj=47.5003 num_tokens=235591 num_tokens/piece=146.877\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1203 obj=49.8125 num_tokens=252465 num_tokens/piece=209.863\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1203 obj=49.3519 num_tokens=252465 num_tokens/piece=209.863\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=50.0345 num_tokens=257878 num_tokens/piece=234.435\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=49.9257 num_tokens=257878 num_tokens/piece=234.435\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: pantip_unigram.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: pantip_unigram.vocab\n"
     ]
    }
   ],
   "source": [
    "## Train\n",
    "spm.SentencePieceTrainer.train(input='./kratoo-40000000-40002000.jsonl', model_prefix='pantip_unigram', vocab_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5WOVMbONnYv"
   },
   "source": [
    "## Analyse top tokens on different datasets\n",
    "\n",
    "Use your tokenizers to tokenize the datasets and analyse your most common vocabularies (try 300-400 vocabs with len>1). Hint: tokenize your data and count the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbfkGcsUrPYS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz0GdZ-5YYM9"
   },
   "source": [
    "### To answer\n",
    "What are some notable differences you see between the two vocabs?\n",
    "\n",
    "Write your answer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxxYr0QLbDoU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipjO87HPYl4N"
   },
   "source": [
    "## Using tokenizer across domains\n",
    "\n",
    "One problem you may face is your dataset is very specialized. In that case the tokenizer trained on a general domain may not perform as good as it should when used on your dataset.\n",
    "\n",
    "Next you will try using tokenizers trained on one general domain (on Pantip) and use it on a specialized domain (พระอภัยมณี) and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4_6JG_l5BXh"
   },
   "source": [
    "### Q3 MCV\n",
    "\n",
    "What percentage increase do you observe when tokenizing the whole พระอภัยมณี dataset with a tokenizer trained on Pantip compared to the one trained on พระอภัยมณี."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tCh1RaZrTAM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duaCJRO96SX1"
   },
   "source": [
    "### Q4 MCV\n",
    "\n",
    "What percentage increase do you observe when tokenizing the whole Pantip dataset with a tokenizer trained on พระอภัยมณี compared to the one trained on Pantip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axk9gOIgrTYd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZYKuamv7-wI"
   },
   "source": [
    "### To answer\n",
    "Why do you think the number of tokens tokenized by the general tokenizer (the one trained on Pantip) has a higher percentage increase compared to the number of tokens tokenized by the specialized tokenizer? (Hint: we fixed vocab size.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gh9a6d7Q8ivJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7j_Cc0p9-5S"
   },
   "source": [
    "## The effect on language models\n",
    "\n",
    "Next, we will see the effect of using \"cross-domain\" tokenizers on Language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiWztANvohhn"
   },
   "source": [
    "### Setup\n",
    "We are going to reuse the code from the last assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pVtSbmVpwOo"
   },
   "outputs": [],
   "source": [
    "# !pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMt5GzLrW4x3"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import lightning as L\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OIs_VS_oo1M"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "  def __init__(self, data, tokenizer, seq_len = 128):\n",
    "\n",
    "    token_ids = [tokenizer.encode(d, add_bos=True, add_eos=True) for d in data]\n",
    "    flatten_token_ids = list(itertools.chain(*token_ids))\n",
    "    encoded = torch.LongTensor(flatten_token_ids)\n",
    "\n",
    "    left_over = len(encoded) % seq_len\n",
    "    encoded = encoded[:len(encoded)-left_over]\n",
    "    self.encoded = encoded.view(-1, seq_len)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.encoded[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hk6vEPiMq34n"
   },
   "outputs": [],
   "source": [
    "class LSTM(L.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, learning_rate, criterion):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size=vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def forward(self, src):\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        src = batch[:, :-1]\n",
    "        target = batch[:, 1:]\n",
    "        prediction = self(src)\n",
    "        prediction = prediction.reshape(-1, self.vocab_size)\n",
    "        target = target.reshape(-1)\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "\n",
    "        src = batch[:, :-1]\n",
    "        target = batch[:, 1:]\n",
    "        with torch.no_grad():\n",
    "          prediction = self(src)\n",
    "        prediction = prediction.reshape(-1, self.vocab_size)\n",
    "        target = target.reshape(-1)\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKhuOygixndB"
   },
   "outputs": [],
   "source": [
    "vocab_size = sp_pam.get_piece_size()\n",
    "embedding_dim = 200\n",
    "hidden_dim = 512\n",
    "num_layers = 3\n",
    "dropout_rate = 0.2\n",
    "lr = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_batch_size = 64\n",
    "test_batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOtOE7mr-heY"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8-x9HiPDcpE"
   },
   "source": [
    "<a name=\"no1\"></a>\n",
    "#### 1. Training on Pantip data with Pantip tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUv_A4MTx0Ob"
   },
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True\n",
    ")\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, sp_pantip)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, sp_pantip)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, sp_pantip)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, sp_pantip)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pantip_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1e-Y1_GYy65g"
   },
   "outputs": [],
   "source": [
    "test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s3AmE4nDjmL"
   },
   "source": [
    "<a name=\"no2\"></a>\n",
    "#### 2. Training on Pantip data with Pra apai manee tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfRdW3m1Dmj_"
   },
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True\n",
    ")\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, sp_pam)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, sp_pam)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, sp_pam)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, sp_pam)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pantip_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwLN1IarD3g9"
   },
   "outputs": [],
   "source": [
    "test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB8zqptTWcA6"
   },
   "source": [
    "#### To answer\n",
    "\n",
    "The perplexity numbers should indicate that:\n",
    "1. Training the LM with Pra apai manee tokenizer on Pantip (no. [2](#no2)) results in overfitting to Pantip and poor generalization to the Pra apai manee dataset.\n",
    "2. However using the Pantip tokenizer (no. [1](#no1)) results in a much better generalization.\n",
    "\n",
    "Try and come up with some reasons for the results above. <br>\n",
    "Hint:\n",
    "1. think about \"general\" vocabs and domain-specific vocabs.\n",
    "2. what do you think happens to the model when the token ids become longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmHGQf2saPj_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8VPMm7pLdSl"
   },
   "source": [
    "\n",
    "<a name=\"no3\"></a>\n",
    "#### 3. Training on Pra apai manee data with Pantip tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oR5fp-YCLnnU"
   },
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True\n",
    ")\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, sp_pantip)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, sp_pantip)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, sp_pantip)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, sp_pantip)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pam_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_LhF7w7Lxwo"
   },
   "outputs": [],
   "source": [
    "test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apk9crJjMLoW"
   },
   "source": [
    "<a name=\"no4\"></a>\n",
    "#### 4. Training on Pra apai manee data with Pra apai manee tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_G7GMBIKLzGK"
   },
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True\n",
    ")\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, sp_pam)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, sp_pam)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, sp_pam)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, sp_pam)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pam_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9H753o_JMRFw"
   },
   "outputs": [],
   "source": [
    "test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en9Lmmj4dZ-1"
   },
   "source": [
    "#### To answer\n",
    "\n",
    "The perplexity numbers should indicate that:\n",
    "1. Both LM overfits on Pra apai manee data and performs really bad on Pantip data.\n",
    "2. However using the Pra apai manee tokenizer (no. [4](#no4)) results in a  better generalization than the Pantip tokenizer(no. [3](#no3)).\n",
    "\n",
    "Try and come up with some reasons for the results above. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlE-mWSMfbv3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
