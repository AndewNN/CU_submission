{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YbUOJOIJEiEc"
      },
      "outputs": [],
      "source": [
        "# ! pip install transformers datasets peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_rMUycDEXIc"
      },
      "source": [
        "# HW 8: Low Rank Adaptation (LoRA)\n",
        "\n",
        "In this assignment, you will learn to implement low-rank adaptation both from scratch and using a libraryâ€”specifically, with PyTorch and the PEFT library, respectively.\n",
        "\n",
        "This assignment is divided into two sections:\n",
        "\n",
        "In the first section, we introduce the parameter-efficient transfer learning (PET) method. We use LoRA to adapt the GPT2 model for the SST-2 dataset. This section will teach you how LoRA works and how to implement it from scratch using forward_hook.\n",
        "\n",
        "In the second section, we introduce the PEFT library, which allows us to perform LoRA easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smZEXsKKEyCA"
      },
      "source": [
        "# Part 1: LoRA from Scratch\n",
        "With the discovery of scaling properties in deep learning models, several researchers tend to increase model size to achieve emergent properties, especially in the natural language processing (NLP) field. For example, GPT-3 contains 175 billion parameters, making it nearly impossible to fine-tune on limited resources. This trend prevents students like us from adapting these enormous foundation models on a single GPU (or with small resources).\n",
        "\n",
        "To alleviate this problem, researchers have developed new fine-tuning methods, known as parameter-efficient transfer learning, which allow us to train large models with limited resources. The benefits of these methods extend not only to the training process but also to deployment. After fine-tuning, we only need to save a small number of parameters (the LoRA weights), enabling us to deploy the foundation model to various downstream tasks using minimal storage. One of the prevailing methods is Low Rank Adaptation (LoRA).\n",
        "\n",
        "Another popular option is prompt tuning, where we only train special tokens that are prepended to the input. However, this is not the focus of this homework.\n",
        "\n",
        "In this section, we will introduce Low-Rank Adaptation. You are assigned to implement LoRA on GPT2 model. We will finetune the model to SST-2 dataset using the traditional and LoRA method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAPm3OUpHhLF"
      },
      "source": [
        "## Load Dataset and Model\n",
        "In this step, we will prepare the GPT-2 model and the SST-2 dataset.\n",
        "\n",
        "SST-2 is a widely used dataset for sentiment analysis, extracted from movie reviews, containing sentences labeled as either positive or negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__O2H4x6Cf4h"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-27 09:27:20.892471: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-03-27 09:27:20.979866: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743042441.015319    6467 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743042441.025634    6467 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-27 09:27:21.110160: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample sentence:\n",
            "{'sentence': \"it 's a charming and often affecting journey . \", 'label': 1, 'idx': 0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c717c69556a94e3693d61d433a60cb8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/53879 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47da374b763842fdbdec9b1bdb973220",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/13470 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2ForSequenceClassification, GPT2TokenizerFast\n",
        "from datasets import load_dataset\n",
        "from tqdm.autonotebook import tqdm\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, AdamW\n",
        "\n",
        "# Load the GPT-2 model for sequence classification and its tokenizer\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "# GPT-2 does not have a pad token by default so we set it to the EOS token.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Load the SST-2 dataset\n",
        "train_dataset_raw = load_dataset(\"glue\", \"sst2\", split=\"train\")\n",
        "train_dataset_raw, val_dataset_raw = train_dataset_raw.train_test_split(test_size=0.2).values()\n",
        "test_dataset_raw = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
        "\n",
        "# Preview dataset\n",
        "print(\"Sample sentence:\")\n",
        "for data in test_dataset_raw:\n",
        "    print(data)\n",
        "    break\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "train_dataset = train_dataset_raw.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset_raw.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset_raw.map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTqlSrsePb0T"
      },
      "source": [
        "## Traditional Fine tuning\n",
        "In the traditional fine-tuning method, the entire model is trained, which is computationally expensive. An alternative approach is to fine-tune only certain layers of the model to reduce resource usage while still adapting the model to a specific task.\n",
        "\n",
        "To keep the implementation simple, you are assigned to train only the attention weights in the self-attention layers.\n",
        "\n",
        "The code below displays the names of all layers in the GPT-2 model. This will help you identify which layers to set as trainable or keep frozen. For more details on the attention layers in GPT-2, please refer to the following link: [GPT-2 Attention Layer Details](https://huggingface.co/transformers/v4.9.2/_modules/transformers/models/gpt2/modeling_gpt2.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformer.wte.weight 38597376\n",
            "transformer.wpe.weight 786432\n",
            "transformer.h.0.ln_1.weight 768\n",
            "transformer.h.0.ln_1.bias 768\n",
            "transformer.h.0.attn.c_attn.weight 1769472\n",
            "transformer.h.0.attn.c_attn.bias 2304\n",
            "transformer.h.0.attn.c_proj.weight 589824\n",
            "transformer.h.0.attn.c_proj.bias 768\n",
            "transformer.h.0.ln_2.weight 768\n",
            "transformer.h.0.ln_2.bias 768\n",
            "transformer.h.0.mlp.c_fc.weight 2359296\n",
            "transformer.h.0.mlp.c_fc.bias 3072\n",
            "transformer.h.0.mlp.c_proj.weight 2359296\n",
            "transformer.h.0.mlp.c_proj.bias 768\n",
            "transformer.h.1.ln_1.weight 768\n",
            "transformer.h.1.ln_1.bias 768\n",
            "transformer.h.1.attn.c_attn.weight 1769472\n",
            "transformer.h.1.attn.c_attn.bias 2304\n",
            "transformer.h.1.attn.c_proj.weight 589824\n",
            "transformer.h.1.attn.c_proj.bias 768\n",
            "transformer.h.1.ln_2.weight 768\n",
            "transformer.h.1.ln_2.bias 768\n",
            "transformer.h.1.mlp.c_fc.weight 2359296\n",
            "transformer.h.1.mlp.c_fc.bias 3072\n",
            "transformer.h.1.mlp.c_proj.weight 2359296\n",
            "transformer.h.1.mlp.c_proj.bias 768\n",
            "transformer.h.2.ln_1.weight 768\n",
            "transformer.h.2.ln_1.bias 768\n",
            "transformer.h.2.attn.c_attn.weight 1769472\n",
            "transformer.h.2.attn.c_attn.bias 2304\n",
            "transformer.h.2.attn.c_proj.weight 589824\n",
            "transformer.h.2.attn.c_proj.bias 768\n",
            "transformer.h.2.ln_2.weight 768\n",
            "transformer.h.2.ln_2.bias 768\n",
            "transformer.h.2.mlp.c_fc.weight 2359296\n",
            "transformer.h.2.mlp.c_fc.bias 3072\n",
            "transformer.h.2.mlp.c_proj.weight 2359296\n",
            "transformer.h.2.mlp.c_proj.bias 768\n",
            "transformer.h.3.ln_1.weight 768\n",
            "transformer.h.3.ln_1.bias 768\n",
            "transformer.h.3.attn.c_attn.weight 1769472\n",
            "transformer.h.3.attn.c_attn.bias 2304\n",
            "transformer.h.3.attn.c_proj.weight 589824\n",
            "transformer.h.3.attn.c_proj.bias 768\n",
            "transformer.h.3.ln_2.weight 768\n",
            "transformer.h.3.ln_2.bias 768\n",
            "transformer.h.3.mlp.c_fc.weight 2359296\n",
            "transformer.h.3.mlp.c_fc.bias 3072\n",
            "transformer.h.3.mlp.c_proj.weight 2359296\n",
            "transformer.h.3.mlp.c_proj.bias 768\n",
            "transformer.h.4.ln_1.weight 768\n",
            "transformer.h.4.ln_1.bias 768\n",
            "transformer.h.4.attn.c_attn.weight 1769472\n",
            "transformer.h.4.attn.c_attn.bias 2304\n",
            "transformer.h.4.attn.c_proj.weight 589824\n",
            "transformer.h.4.attn.c_proj.bias 768\n",
            "transformer.h.4.ln_2.weight 768\n",
            "transformer.h.4.ln_2.bias 768\n",
            "transformer.h.4.mlp.c_fc.weight 2359296\n",
            "transformer.h.4.mlp.c_fc.bias 3072\n",
            "transformer.h.4.mlp.c_proj.weight 2359296\n",
            "transformer.h.4.mlp.c_proj.bias 768\n",
            "transformer.h.5.ln_1.weight 768\n",
            "transformer.h.5.ln_1.bias 768\n",
            "transformer.h.5.attn.c_attn.weight 1769472\n",
            "transformer.h.5.attn.c_attn.bias 2304\n",
            "transformer.h.5.attn.c_proj.weight 589824\n",
            "transformer.h.5.attn.c_proj.bias 768\n",
            "transformer.h.5.ln_2.weight 768\n",
            "transformer.h.5.ln_2.bias 768\n",
            "transformer.h.5.mlp.c_fc.weight 2359296\n",
            "transformer.h.5.mlp.c_fc.bias 3072\n",
            "transformer.h.5.mlp.c_proj.weight 2359296\n",
            "transformer.h.5.mlp.c_proj.bias 768\n",
            "transformer.h.6.ln_1.weight 768\n",
            "transformer.h.6.ln_1.bias 768\n",
            "transformer.h.6.attn.c_attn.weight 1769472\n",
            "transformer.h.6.attn.c_attn.bias 2304\n",
            "transformer.h.6.attn.c_proj.weight 589824\n",
            "transformer.h.6.attn.c_proj.bias 768\n",
            "transformer.h.6.ln_2.weight 768\n",
            "transformer.h.6.ln_2.bias 768\n",
            "transformer.h.6.mlp.c_fc.weight 2359296\n",
            "transformer.h.6.mlp.c_fc.bias 3072\n",
            "transformer.h.6.mlp.c_proj.weight 2359296\n",
            "transformer.h.6.mlp.c_proj.bias 768\n",
            "transformer.h.7.ln_1.weight 768\n",
            "transformer.h.7.ln_1.bias 768\n",
            "transformer.h.7.attn.c_attn.weight 1769472\n",
            "transformer.h.7.attn.c_attn.bias 2304\n",
            "transformer.h.7.attn.c_proj.weight 589824\n",
            "transformer.h.7.attn.c_proj.bias 768\n",
            "transformer.h.7.ln_2.weight 768\n",
            "transformer.h.7.ln_2.bias 768\n",
            "transformer.h.7.mlp.c_fc.weight 2359296\n",
            "transformer.h.7.mlp.c_fc.bias 3072\n",
            "transformer.h.7.mlp.c_proj.weight 2359296\n",
            "transformer.h.7.mlp.c_proj.bias 768\n",
            "transformer.h.8.ln_1.weight 768\n",
            "transformer.h.8.ln_1.bias 768\n",
            "transformer.h.8.attn.c_attn.weight 1769472\n",
            "transformer.h.8.attn.c_attn.bias 2304\n",
            "transformer.h.8.attn.c_proj.weight 589824\n",
            "transformer.h.8.attn.c_proj.bias 768\n",
            "transformer.h.8.ln_2.weight 768\n",
            "transformer.h.8.ln_2.bias 768\n",
            "transformer.h.8.mlp.c_fc.weight 2359296\n",
            "transformer.h.8.mlp.c_fc.bias 3072\n",
            "transformer.h.8.mlp.c_proj.weight 2359296\n",
            "transformer.h.8.mlp.c_proj.bias 768\n",
            "transformer.h.9.ln_1.weight 768\n",
            "transformer.h.9.ln_1.bias 768\n",
            "transformer.h.9.attn.c_attn.weight 1769472\n",
            "transformer.h.9.attn.c_attn.bias 2304\n",
            "transformer.h.9.attn.c_proj.weight 589824\n",
            "transformer.h.9.attn.c_proj.bias 768\n",
            "transformer.h.9.ln_2.weight 768\n",
            "transformer.h.9.ln_2.bias 768\n",
            "transformer.h.9.mlp.c_fc.weight 2359296\n",
            "transformer.h.9.mlp.c_fc.bias 3072\n",
            "transformer.h.9.mlp.c_proj.weight 2359296\n",
            "transformer.h.9.mlp.c_proj.bias 768\n",
            "transformer.h.10.ln_1.weight 768\n",
            "transformer.h.10.ln_1.bias 768\n",
            "transformer.h.10.attn.c_attn.weight 1769472\n",
            "transformer.h.10.attn.c_attn.bias 2304\n",
            "transformer.h.10.attn.c_proj.weight 589824\n",
            "transformer.h.10.attn.c_proj.bias 768\n",
            "transformer.h.10.ln_2.weight 768\n",
            "transformer.h.10.ln_2.bias 768\n",
            "transformer.h.10.mlp.c_fc.weight 2359296\n",
            "transformer.h.10.mlp.c_fc.bias 3072\n",
            "transformer.h.10.mlp.c_proj.weight 2359296\n",
            "transformer.h.10.mlp.c_proj.bias 768\n",
            "transformer.h.11.ln_1.weight 768\n",
            "transformer.h.11.ln_1.bias 768\n",
            "transformer.h.11.attn.c_attn.weight 1769472\n",
            "transformer.h.11.attn.c_attn.bias 2304\n",
            "transformer.h.11.attn.c_proj.weight 589824\n",
            "transformer.h.11.attn.c_proj.bias 768\n",
            "transformer.h.11.ln_2.weight 768\n",
            "transformer.h.11.ln_2.bias 768\n",
            "transformer.h.11.mlp.c_fc.weight 2359296\n",
            "transformer.h.11.mlp.c_fc.bias 3072\n",
            "transformer.h.11.mlp.c_proj.weight 2359296\n",
            "transformer.h.11.mlp.c_proj.bias 768\n",
            "transformer.ln_f.weight 768\n",
            "transformer.ln_f.bias 768\n",
            "score.weight 1536\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(name, param.numel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "MKM2IWJ4PBRc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " <class 'transformers.models.gpt2.modeling_gpt2.GPT2ForSequenceClassification'>\n",
            "transformer <class 'transformers.models.gpt2.modeling_gpt2.GPT2Model'>\n",
            "transformer.wte <class 'torch.nn.modules.sparse.Embedding'>\n",
            "transformer.wpe <class 'torch.nn.modules.sparse.Embedding'>\n",
            "transformer.drop <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h <class 'torch.nn.modules.container.ModuleList'>\n",
            "transformer.h.0 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "transformer.h.0.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.0.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>\n",
            "transformer.h.0.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.0.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.0.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.0.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.0.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.0.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "transformer.h.0.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.0.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.0.mlp.act <class 'transformers.activations.NewGELUActivation'>\n",
            "transformer.h.0.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.1 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "transformer.h.1.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.1.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>\n",
            "transformer.h.1.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.1.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.1.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.1.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.1.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.1.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "transformer.h.1.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.1.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.1.mlp.act <class 'transformers.activations.NewGELUActivation'>\n",
            "transformer.h.1.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.2 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "transformer.h.2.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.2.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>\n",
            "transformer.h.2.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.2.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.2.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.2.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.2.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.2.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "transformer.h.2.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.2.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.2.mlp.act <class 'transformers.activations.NewGELUActivation'>\n",
            "transformer.h.2.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.3 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "transformer.h.3.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.3.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>\n",
            "transformer.h.3.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.3.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.3.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.3.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.3.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.3.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "transformer.h.3.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.3.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.3.mlp.act <class 'transformers.activations.NewGELUActivation'>\n",
            "transformer.h.3.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.4 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "transformer.h.4.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.4.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>\n",
            "transformer.h.4.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.4.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.4.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.4.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.4.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.4.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "transformer.h.4.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.4.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.4.mlp.act <class 'transformers.activations.NewGELUActivation'>\n",
            "transformer.h.4.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.5 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "transformer.h.5.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.5.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>\n",
            "transformer.h.5.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.5.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.5.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.5.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.5.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.5.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "transformer.h.5.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.5.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.5.mlp.act <class 'transformers.activations.NewGELUActivation'>\n",
            "transformer.h.5.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.6 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "transformer.h.6.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.6.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>\n",
            "transformer.h.6.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.6.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.6.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.6.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.6.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.6.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "transformer.h.6.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.6.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.6.mlp.act <class 'transformers.activations.NewGELUActivation'>\n",
            "transformer.h.6.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.7 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "transformer.h.7.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.7.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>\n",
            "transformer.h.7.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.7.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.7.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.7.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.7.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.7.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "transformer.h.7.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.7.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.7.mlp.act <class 'transformers.activations.NewGELUActivation'>\n",
            "transformer.h.7.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.8 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "transformer.h.8.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.8.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>\n",
            "transformer.h.8.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.8.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.8.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.8.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.8.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.8.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "transformer.h.8.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.8.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.8.mlp.act <class 'transformers.activations.NewGELUActivation'>\n",
            "transformer.h.8.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.9 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "transformer.h.9.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.9.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>\n",
            "transformer.h.9.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.9.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.9.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.9.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.9.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.9.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "transformer.h.9.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.9.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.9.mlp.act <class 'transformers.activations.NewGELUActivation'>\n",
            "transformer.h.9.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.10 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "transformer.h.10.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.10.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>\n",
            "transformer.h.10.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.10.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.10.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.10.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.10.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.10.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "transformer.h.10.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.10.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.10.mlp.act <class 'transformers.activations.NewGELUActivation'>\n",
            "transformer.h.10.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.11 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "transformer.h.11.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.11.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>\n",
            "transformer.h.11.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.11.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.11.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.11.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.h.11.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "transformer.h.11.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "transformer.h.11.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.11.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\n",
            "transformer.h.11.mlp.act <class 'transformers.activations.NewGELUActivation'>\n",
            "transformer.h.11.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
            "transformer.ln_f <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "score <class 'torch.nn.modules.linear.Linear'>\n"
          ]
        }
      ],
      "source": [
        "for name, module in model.named_modules():\n",
        "    print(name, type(module))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuhxNv79ctMz"
      },
      "source": [
        "###TODO 1: Freeze the Model and Train Only Attention Weights\n",
        "You are assigned to freeze the entire model, except for the last two attention weights and the classification head. Note that, in this context, the attention weights do not include the projection layer of the transformer. Instead, they refer only to the weights of the query, key, and value.\n",
        "\n",
        "**HINT**: `c_proj` is projection layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ds1GqFA4R5ok"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformer.h.10.attn.c_attn.weight\n",
            "transformer.h.10.attn.c_attn.bias\n",
            "transformer.h.11.attn.c_attn.weight\n",
            "transformer.h.11.attn.c_attn.bias\n",
            "score.weight\n"
          ]
        }
      ],
      "source": [
        "import regex as re\n",
        "for n, p in model.named_parameters():\n",
        "    # TODO 1`: freeze every layer except the last two attention weights and classification head.\n",
        "    if re.search(r\"(10|11)\\.attn\\.c_attn|score\", n) or n == \"score\":\n",
        "        p.requires_grad = True\n",
        "        print(n)\n",
        "    else:\n",
        "        p.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlV2DoY0f1sS"
      },
      "source": [
        "**Check Your Answer:** The number of learnable parameters is around 3545088."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vaZ4l7Cmfo2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Trainable Parameters: 3545088\n"
          ]
        }
      ],
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Number of Trainable Parameters:\", pytorch_total_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAbUMyXaiUjS"
      },
      "source": [
        "You are assigned to train the GPT-2 model on the SST-2 dataset. Due to the long training time, you will train the model for only 3 epochs. Your model should have around 86-88% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dWOmFS6QgGaJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/andre/anaconda3/envs/ML10/lib/python3.10/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c336b12f1c354c09b36e45ebb507bb9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5c03c53ece74176ac04c2359ce5df88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3368 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3 - Accuracy: 0.8761\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "269409e3c46048ea8e4a5856150688db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3368 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/3 - Accuracy: 0.8796\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c352aae35d364b048b77cd5df4159daf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3368 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3 - Accuracy: 0.8842\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 3\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} - Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCFVbkuajL-g"
      },
      "source": [
        "As you can see, fine-tuning in the traditional way takes a long time to complete and also requires a high-computation GPU to fine-tune the entire model. Therefore, it is not feasible for most people.\n",
        "\n",
        "In the next part, we will introduce a better method: parameter-efficient learning, which requires lower computation. We will focus on the state-of-the-art method, Low-Rank Adaptation (LoRA)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhCP0ZqrpeHO"
      },
      "source": [
        "## Low Rank Adaptation\n",
        "The concept of LoRA is that we are going to estimate the gradient (adaptation matrix) with two smaller matrices ($A$ and $B$):\n",
        "\n",
        "$$\n",
        "\\text{Adaptation Matrix} = B \\times A\n",
        "$$\n",
        "\n",
        "where $\\text{Adaptation Matrix} \\in \\mathbb{R}^{m \\times n}$, $A \\in \\mathbb{R}^{r \\times n}$, and $B \\in \\mathbb{R}^{m \\times r}$. We could make this approximation based on the assumption that $\\text{Adaptation Matrix}$ has a rank of $r$. Therefore, the fine-tuned weight becomes:\n",
        "\n",
        "$$\n",
        "W = W_0 + \\Delta W\n",
        "$$\n",
        "$$\n",
        "= W_0 + \\frac{\\alpha}{r} BA\n",
        "$$\n",
        "\n",
        "where $W$ denotes the fine-tuned weight, $W_0$ represents pre-trained weight, $\\Delta W$ is the gradient and $\\alpha$ can be seen as a learning rate. $A$ is initialized using a common initialization, like Kaiming initialization, during the initialization process. On the other hand, $B$ is set to 0 such that the model's output remains the same after injecting LoRA, resulting in a stabilized training process.\n",
        "\n",
        "To summarize, when injecting LoRA into a layer, we insert new parameters called matrix A and B and initialize them using the above description. Then, we modify the forward pass with `forward_hook` such that the output becomes:\n",
        "\n",
        "$$\n",
        "h = W x + \\frac{\\alpha}{r} BA x\n",
        "$$\n",
        "\n",
        "where $x$ and $h$ are the input and output, respectively. We recommend you read this [blog](https://web.stanford.edu/~nanbhas/blog/forward-hooks-pytorch/#forward-hooks-101) to learn more about `forward_hook`.\n",
        "\n",
        "**LoRA on Linear Layer**\n",
        "\n",
        "- TODO 2: initialize A and B to ones (every entry in the matrix is one), such that we can verify your forward pass after attaching the hook.\n",
        "- TODO 3: implement the forward hook such that new output $h$ is\n",
        "\n",
        "$$\n",
        "h = W x + \\frac{\\alpha}{r} BA x\n",
        "$$\n",
        "\n",
        "**Hint**: When you want to declare and initialize a parameter, you can use `torch.nn.Parameter` and `torch.nn.init`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "fI1r0lZqs0A-"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# Initialize LoRA and attach a hook.\n",
        "def attach_lora(layer, r, lora_alpha, in_features, out_features):\n",
        "    assert r > 0, \"rank must greater than 0.\"\n",
        "    # TODO 2: Declare A and B matrices and initialize A and B to ones.\n",
        "    layer.lora_A = torch.nn.Parameter(torch.ones((r, out_features)))\n",
        "    layer.lora_B = torch.nn.Parameter(torch.ones((in_features, r)))\n",
        "\n",
        "    def hook(model, input, output):\n",
        "        assert len(input) == 1, \"The length of the input must be 1.\"\n",
        "        # TODO 3: Compute adapatation matrix (BA) and modify the forward pass.\n",
        "        adaptation_matrix = layer.lora_B @ layer.lora_A\n",
        "        output = output + (lora_alpha / r) * input[0] @ adaptation_matrix\n",
        "        return output\n",
        "\n",
        "\n",
        "    return hook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nl2dlr8ttef"
      },
      "source": [
        "To test your `forward_hook`, we will check the difference of the output before and after injecting the LoRA when you initialize matrices A and B with ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "ZW8gI58KjLiw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your forward hook seems to be correct.\n"
          ]
        }
      ],
      "source": [
        "from transformers.modeling_utils import Conv1D\n",
        "# The Conv1D layer from the Transformer library is actually a linear layer. (https://github.com/huggingface/transformers/blob/main/src/transformers/pytorch_utils.py#L100)\n",
        "\n",
        "class DummyLinear(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = Conv1D(20, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)\n",
        "\n",
        "r, lora_alpha = 1, 4\n",
        "input_ = torch.arange(10, dtype=torch.float32).unsqueeze(0)\n",
        "dummy_linear = DummyLinear()\n",
        "output_before = dummy_linear(input_)\n",
        "for name, module in dummy_linear.named_modules():\n",
        "  if isinstance(module, Conv1D):\n",
        "    in_features, out_features = module.weight.shape\n",
        "    h = module.register_forward_hook(attach_lora(module, r, lora_alpha, in_features, out_features))\n",
        "output_after = dummy_linear(input_)\n",
        "\n",
        "if torch.all(torch.isclose(output_after - output_before, lora_alpha * input_.sum() * torch.ones_like(output_before))):\n",
        "  print(\"Your forward hook seems to be correct.\")\n",
        "else:\n",
        "  print(\"There is something wrong with your forward hook.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq_ucaXctylG"
      },
      "source": [
        "**Instruction**\n",
        "\n",
        "TODO 4: Change the initialization of A and B where A is initialized with Kaiming Uniform (a = sqrt(5)), and B is set to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "emqf4yyWtyY5"
      },
      "outputs": [],
      "source": [
        "# Initialize LoRA and attach a hook.\n",
        "def attach_lora(layer, r, lora_alpha, in_features, out_features):\n",
        "    assert r > 0, \"rank must greater than 0.\"\n",
        "    # TODO 4: initialize A with kaiming uniform with a = sqrt(5) and initialize B to 0.\n",
        "    layer.lora_A = torch.nn.Parameter(torch.empty((r, out_features)).uniform_(-math.sqrt(5), math.sqrt(5)))\n",
        "    layer.lora_B = torch.nn.Parameter(torch.zeros((in_features, r)))\n",
        "\n",
        "    def hook(model, input, output):\n",
        "        assert len(input) == 1, \"The length of the input must be 1.\"\n",
        "        # Copy from TODO 3\n",
        "        adaptation_matrix = layer.lora_B @ layer.lora_A\n",
        "        output = output + (lora_alpha / r) * input[0] @ adaptation_matrix\n",
        "        return output\n",
        "\n",
        "    return hook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EnhdRMjt_63"
      },
      "source": [
        "Similar to TODO 1, You are assigned to inject lora into the last two attention weights.\n",
        "\n",
        "TODO 5: inject lora into the last two attention weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "lOCO97bJhffz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformer.h.10.attn.c_attn\n",
            "transformer.h.11.attn.c_attn\n"
          ]
        }
      ],
      "source": [
        "r, lora_alpha = 1, 4\n",
        "def attach_lora_to_maskformer(model, r, lora_alpha):\n",
        "    hooks = []\n",
        "    for name, module in model.named_modules():\n",
        "        # TODO 5: inject lora into the last two attention weights\n",
        "        if re.search(r\"(10|11)\\.attn\\.c_attn\", name):\n",
        "            print(name)\n",
        "            in_features, out_features = module.weight.shape\n",
        "            hooks.append(module.register_forward_hook(attach_lora(module, r, lora_alpha, in_features, out_features)))\n",
        "    return hooks\n",
        "\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "hooks = attach_lora_to_maskformer(model, r, lora_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for n, p in model.named_parameters():\n",
        "#     print(n, p.numel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiQdCaQLwqjS"
      },
      "source": [
        "###TODO 6: Freeze the Model and Train Only LoRA Weights\n",
        "You are assigned to freeze the entire model, except for the bias of the last two attention weights, LoRA weights, and the classification head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "KsxEx0fowBhm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformer.h.10.attn.c_attn.bias\n",
            "transformer.h.10.attn.c_attn.lora_A\n",
            "transformer.h.10.attn.c_attn.lora_B\n",
            "transformer.h.11.attn.c_attn.bias\n",
            "transformer.h.11.attn.c_attn.lora_A\n",
            "transformer.h.11.attn.c_attn.lora_B\n",
            "score.weight\n"
          ]
        }
      ],
      "source": [
        "for n, p in model.named_parameters():\n",
        "    # TODO 6: freeze every layer except the bias of the last two attention weights, LoRA weights, and classification head.\n",
        "    if re.search(r\"(10|11)\\.attn\\.c_attn\\.bias|score|lora\", n):\n",
        "        p.requires_grad = True\n",
        "        print(n)\n",
        "    else:\n",
        "        p.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXsHsYzR7TVW"
      },
      "source": [
        "**Check Your Answer:** The number of learnable parameters is around 12288."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "sy_KtJTgyPul"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12288\n"
          ]
        }
      ],
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "PEObZRzFyWC8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14c9f681742345e9853d9482b417bab6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b887eac094c440e9fb1c2b017fd9059",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3368 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3 - Accuracy: 0.8383\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10a163eb44e74ca4a98bcb3b2b36abfa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3368 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/3 - Accuracy: 0.8394\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0a68a28d6254f47b235745d4ace97e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3368 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3 - Accuracy: 0.8555\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 3\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} - Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O1O6Y8hzFLQ"
      },
      "source": [
        "# Part 2: PEFT Library\n",
        "\n",
        "In the first part, you learned how to implement LoRA from scratch. However, in real-world applications, we can simplify this process by using pre-built libraries. One such library is [`peft`](https://huggingface.co/docs/peft/main/en/quicktour), which allows us to inject LoRA into a model more efficiently. By declaring the injected modules in the LoRAConfig, we can easily integrate LoRA without having to implement it ourselves. In this section, you will use the `peft` library to apply LoRA to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETbzxntZ2p0L"
      },
      "source": [
        "## TODO 7-8: Initialize the LoRA Config and Set trainable parameters\n",
        "\n",
        "Your task is to initialize `LoRAConfig` using the same hyperparameters as in TODO 6 (`r=1`, `lora_alpha=4`). Apply LoRA only to the last two attention layers. Then, make sure to freeze the entire model except for the LoRA weights, the bias in the LoRA-injected layers, and the classification head. You only need to set the classification head to be trainable where the rest parameters are already set according to our `LoRAConfig`.  \n",
        "\n",
        "**HINT**: The total number of trainable parameters should match the result from Part 1 (TODO 6).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "pJGuWtHOyY0n"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/andre/anaconda3/envs/ML10/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# TODO 7: Initialize LoRAConfig\n",
        "lora_config = LoraConfig(\n",
        "    # Insert the parameters\n",
        "    r=1,\n",
        "    lora_alpha=4,\n",
        "    target_modules=[\"transformer.h.10.attn.c_attn\", \"transformer.h.11.attn.c_attn\"],\n",
        ")\n",
        "\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model = model.to(device)\n",
        "\n",
        "# TODO 8: Set classification head to trainable\n",
        "for n, p in model.named_parameters():\n",
        "    if re.search(r\"(10|11)\\.attn\\.c_attn\\.base_layer\\.bias|score|lora\", n):\n",
        "        p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base_model.model.transformer.h.10.attn.c_attn.base_layer.bias 2304\n",
            "base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight 768\n",
            "base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight 2304\n",
            "base_model.model.transformer.h.11.attn.c_attn.base_layer.bias 2304\n",
            "base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight 768\n",
            "base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight 2304\n",
            "base_model.model.score.weight 1536\n"
          ]
        }
      ],
      "source": [
        "for n, p in model.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(n, p.numel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ow2eIkHz4438"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12288\n"
          ]
        }
      ],
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "KSPiI4u45yJq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/andre/anaconda3/envs/ML10/lib/python3.10/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "358b8fd8f56744eb8dbb2f8dfbdd7dac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f60eb49837f94e489a62e804955980e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3368 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3 - Accuracy: 0.8211\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a0e5cd040e3486e91d021a4ca460c93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3368 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/3 - Accuracy: 0.8440\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc96b62d770c431c8224c172e8f61112",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3368 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3 - Accuracy: 0.8567\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 3\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} - Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRJ5pWrX6kKd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ML10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
