{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ8FRFIYMc5X"
      },
      "source": [
        "# HOMEWORK 6: TEXT CLASSIFICATION\n",
        "In this homework, you will create models to classify texts from TRUE call-center. There are two classification tasks:\n",
        "1. Action Classification: Identify which action the customer would like to take (e.g. enquire, report, cancle)\n",
        "2. Object Classification: Identify which object the customer is referring to (e.g. payment, truemoney, internet, roaming)\n",
        "\n",
        "We will focus only on the Object Classification task for this homework.\n",
        "\n",
        "In this homework, you are asked compare different text classification models in terms of accuracy and inference time.\n",
        "\n",
        "You will need to build 3 different models.\n",
        "\n",
        "1. A model based on tf-idf\n",
        "2. A model based on MUSE\n",
        "3. A model based on wangchanBERTa\n",
        "\n",
        "**You will be ask to submit 3 different files (.pdf from .ipynb) that does the 3 different models. Finally, answer the accuracy and runtime numbers in MCV.**\n",
        "\n",
        "This homework is quite free form, and your answer may vary. We hope that the processing during the course of this assignment will make you think more about the design choices in text classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHqkFSyaNvOt",
        "outputId": "879b17f1-0fb2-455c-ca37-b5a4aecd7b1c"
      },
      "outputs": [],
      "source": [
        "# !wget --no-check-certificate https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRlx5Mb5zkXw",
        "outputId": "18d913e0-aa6d-435b-931d-591386cb4ba8"
      },
      "outputs": [],
      "source": [
        "# !pip install pythainlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YprqbOPMc5a"
      },
      "source": [
        "## Import Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "heICP79cMc5e"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import pandas\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from IPython.display import display\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPaUf4PLMc5k"
      },
      "source": [
        "## Loading data\n",
        "First, we load the data from disk into a Dataframe.\n",
        "\n",
        "A Dataframe is essentially a table, or 2D-array/Matrix with a name for each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "JhZ2eBAWMc5l"
      },
      "outputs": [],
      "source": [
        "data_df = pd.read_csv('clean-phone-data-for-students.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cje3yruTMc5p"
      },
      "source": [
        "Let's preview the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "aNqRNz1PMc5q",
        "outputId": "e129a502-1420-476c-dc50-46c293a01b56"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence Utterance</th>\n",
              "      <th>Action</th>\n",
              "      <th>Object</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;PHONE_NUMBER_REMOVED&gt; ผมไปจ่ายเงินที่ Counte...</td>\n",
              "      <td>enquire</td>\n",
              "      <td>payment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>internet ยังความเร็วอยุ่เท่าไหร ครับ</td>\n",
              "      <td>enquire</td>\n",
              "      <td>package</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...</td>\n",
              "      <td>report</td>\n",
              "      <td>suspend</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...</td>\n",
              "      <td>enquire</td>\n",
              "      <td>internet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...</td>\n",
              "      <td>report</td>\n",
              "      <td>phone_issues</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  Sentence Utterance   Action        Object\n",
              "0   <PHONE_NUMBER_REMOVED> ผมไปจ่ายเงินที่ Counte...  enquire       payment\n",
              "1               internet ยังความเร็วอยุ่เท่าไหร ครับ  enquire       package\n",
              "2   ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...   report       suspend\n",
              "3   พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...  enquire      internet\n",
              "4   ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...   report  phone_issues"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence Utterance</th>\n",
              "      <th>Action</th>\n",
              "      <th>Object</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>16175</td>\n",
              "      <td>16175</td>\n",
              "      <td>16175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>13389</td>\n",
              "      <td>10</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>บริการอื่นๆ</td>\n",
              "      <td>enquire</td>\n",
              "      <td>service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>97</td>\n",
              "      <td>10377</td>\n",
              "      <td>2525</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Sentence Utterance   Action   Object\n",
              "count               16175    16175    16175\n",
              "unique              13389       10       33\n",
              "top           บริการอื่นๆ  enquire  service\n",
              "freq                   97    10377     2525"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show the top 5 rows\n",
        "display(data_df.head())\n",
        "# Summarize the data\n",
        "data_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGd8BNvMMc5y"
      },
      "source": [
        "## Data cleaning\n",
        "\n",
        "We call the DataFrame.describe() again.\n",
        "Notice that there are 33 unique labels/classes for object and 10 unique labels for action that the model will try to predict.\n",
        "But there are unwanted duplications e.g. Idd,idd,lotalty_card,Lotalty_card\n",
        "\n",
        "Also note that, there are 13389 unqiue sentence utterances from 16175 utterances. You have to clean that too!\n",
        "\n",
        "## #TODO 0.1:\n",
        "You will have to remove unwanted label duplications as well as duplications in text inputs.\n",
        "Also, you will have to trim out unwanted whitespaces from the text inputs.\n",
        "This shouldn't be too hard, as you have already seen it in the demo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "V0bGLblVMc5z",
        "outputId": "1a65aff5-6196-4674-fb5d-36aa1afcfdba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence Utterance</th>\n",
              "      <th>Action</th>\n",
              "      <th>Object</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>16175</td>\n",
              "      <td>16175</td>\n",
              "      <td>16175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>13389</td>\n",
              "      <td>10</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>บริการอื่นๆ</td>\n",
              "      <td>enquire</td>\n",
              "      <td>service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>97</td>\n",
              "      <td>10377</td>\n",
              "      <td>2525</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Sentence Utterance   Action   Object\n",
              "count               16175    16175    16175\n",
              "unique              13389       10       33\n",
              "top           บริการอื่นๆ  enquire  service\n",
              "freq                   97    10377     2525"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n",
              "       'service', 'nonTrueMove', 'balance', 'detail', 'bill', 'credit',\n",
              "       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n",
              "       'information', 'lost_stolen', 'balance_minutes', 'idd',\n",
              "       'TrueMoney', 'garbage', 'Payment', 'IDD', 'ringtone', 'Idd',\n",
              "       'rate', 'loyalty_card', 'contact', 'officer', 'Balance', 'Service',\n",
              "       'Loyalty_card'], dtype=object)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array(['enquire', 'report', 'cancel', 'Enquire', 'buy', 'activate',\n",
              "       'request', 'Report', 'garbage', 'change'], dtype=object)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(data_df.describe())\n",
        "display(data_df.Object.unique())\n",
        "display(data_df.Action.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "19onNNUZMc54"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean Sentence Utterance</th>\n",
              "      <th>clean Object</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>13367</td>\n",
              "      <td>13367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>13367</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ</td>\n",
              "      <td>service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>2108</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    clean Sentence Utterance clean Object\n",
              "count                                  13367        13367\n",
              "unique                                 13367           26\n",
              "top     สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ      service\n",
              "freq                                       1         2108"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO.1: Data cleaning\n",
        "data_df['clean Sentence Utterance'] = data_df['Sentence Utterance'].str.strip().copy()\n",
        "# data_df['clean Action'] = data_df['Action'].str.lower().copy()\n",
        "data_df['clean Object'] = data_df['Object'].str.lower().copy()\n",
        "\n",
        "# data_df.drop_duplicates(\"Sentence Utterance\", keep=\"first\", inplace=True)\n",
        "data_df.drop_duplicates(\"clean Sentence Utterance\", keep=\"first\", inplace=True)\n",
        "\n",
        "data_df.drop('Sentence Utterance', axis=1, inplace=True)\n",
        "data_df.drop('Action', axis=1, inplace=True)\n",
        "data_df.drop('Object', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "data_df.describe()\n",
        "\n",
        "# idx = 1\n",
        "# print(f'\"{data_df[\"Sentence Utterance\"][idx]}\"')\n",
        "# print(f'\"{data_df[\"clean Sentence Utterance\"][idx]}\"')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'payment': 0,\n",
              " 'package': 1,\n",
              " 'suspend': 2,\n",
              " 'internet': 3,\n",
              " 'phone_issues': 4,\n",
              " 'service': 5,\n",
              " 'nontruemove': 6,\n",
              " 'balance': 7,\n",
              " 'detail': 8,\n",
              " 'bill': 9,\n",
              " 'credit': 10,\n",
              " 'promotion': 11,\n",
              " 'mobile_setting': 12,\n",
              " 'iservice': 13,\n",
              " 'roaming': 14,\n",
              " 'truemoney': 15,\n",
              " 'information': 16,\n",
              " 'lost_stolen': 17,\n",
              " 'balance_minutes': 18,\n",
              " 'idd': 19,\n",
              " 'garbage': 20,\n",
              " 'ringtone': 21,\n",
              " 'rate': 22,\n",
              " 'loyalty_card': 23,\n",
              " 'contact': 24,\n",
              " 'officer': 25}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{0: 'payment',\n",
              " 1: 'package',\n",
              " 2: 'suspend',\n",
              " 3: 'internet',\n",
              " 4: 'phone_issues',\n",
              " 5: 'service',\n",
              " 6: 'nontruemove',\n",
              " 7: 'balance',\n",
              " 8: 'detail',\n",
              " 9: 'bill',\n",
              " 10: 'credit',\n",
              " 11: 'promotion',\n",
              " 12: 'mobile_setting',\n",
              " 13: 'iservice',\n",
              " 14: 'roaming',\n",
              " 15: 'truemoney',\n",
              " 16: 'information',\n",
              " 17: 'lost_stolen',\n",
              " 18: 'balance_minutes',\n",
              " 19: 'idd',\n",
              " 20: 'garbage',\n",
              " 21: 'ringtone',\n",
              " 22: 'rate',\n",
              " 23: 'loyalty_card',\n",
              " 24: 'contact',\n",
              " 25: 'officer'}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array(['payment', 'package', 'suspend', ..., 'balance', 'balance',\n",
              "       'package'], dtype=object)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array([0, 1, 2, ..., 7, 7, 1], dtype=object)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data = data_df.to_numpy()\n",
        "unique_label = data_df['clean Object'].unique()\n",
        "\n",
        "label_2_num = dict(zip(unique_label, range(len(unique_label))))\n",
        "num_2_label = dict(zip(range(len(unique_label)), unique_label))\n",
        "\n",
        "display(label_2_num)\n",
        "display(num_2_label)\n",
        "\n",
        "display(data[:, 1])\n",
        "data[:, 1]\n",
        "data[:, 1] = np.vectorize(label_2_num.get)(data[:, 1])\n",
        "display(data[:, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIxnPRiAmrhN"
      },
      "source": [
        "Split data into train, valdation, and test sets (normally the ratio will be 80:10:10 , respectively). We recommend to use train_test_spilt from scikit-learn to split the data into train, validation, test set.\n",
        "\n",
        "In addition, it should split the data that distribution of the labels in train, validation, test set are similar. There is **stratify** option to handle this issue.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "Make sure the same data splitting is used for all models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 641 1791  730 1786  581 2108  246 1478  327  540  173 1142  280   22\n",
            "  246  248  296  231   50  206   49   79   36   67    4   10]\n"
          ]
        }
      ],
      "source": [
        "bin_label = np.bincount(np.array(data[:, 1], dtype=int))\n",
        "# print(data[:, 1])\n",
        "print(bin_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "EYzMrvb7nYR2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(13367, 2)\n",
            "(10693, 2) (1337, 2) (1337, 2)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "\n",
        "sss_train_valtest = StratifiedShuffleSplit(n_splits=1, test_size=1/10)\n",
        "sss_val_test = StratifiedShuffleSplit(n_splits=1, test_size=1/9)\n",
        "\n",
        "print(data.shape)\n",
        "trainval_idx, test_idx = next(sss_train_valtest.split(data[:, 0], data[:, 1]))\n",
        "trainval_raw = data[trainval_idx]\n",
        "test_raw = data[test_idx]\n",
        "# print(trainval_raw.shape, test_raw.shape)\n",
        "\n",
        "train_idx, val_idx = next(sss_val_test.split(trainval_raw[:, 0], trainval_raw[:, 1]))\n",
        "train_raw = trainval_raw[train_idx]\n",
        "val_raw = trainval_raw[val_idx]\n",
        "\n",
        "print(train_raw.shape, val_raw.shape, test_raw.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ทําให้', 'ทาง', 'สิ่งนั้น', 'แยะ', 'ล่าสุด', 'จัง', 'ในที่', 'เช่นกัน', 'อย่างไรเสีย', 'ถึงแม้จะ', 'ทั้งปวง', 'ซึ่งๆ', 'แหละ', 'ไป่', 'ใหญ่', 'คราวใด', 'จรดกับ', 'ทุกวัน', 'พอแล้ว', 'เสียจน', 'คุณ', 'เป็นต้นไป', 'ปรากฏว่า', 'มึง', 'การ', 'เพิ่มเติม', 'ทั้งนี้', 'กู', 'เกี่ยวกัน', 'กลุ่ม', 'เสียด้วย', 'ขวาง', 'ช่วงถัดไป', 'ไม่ค่อย', 'ทั้งนั้นด้วย', 'บ้าง', 'วันนั้น', 'ประมาณ', 'เป็นอาทิ', 'เช่นดังก่อน', 'เนี่ย', 'เท่าที่', 'ตลอดจน', 'เห็นควร', 'คราวหลัง', 'มุ่งหมาย', 'คิด', 'แค่ว่า', 'เพื่อว่า', 'ข้าง', 'คล้ายกับว่า', 'จริง', 'ยังคง', 'เพื่อ', 'จู่ๆ', 'ให้ไป', 'ทุกตัว', 'พบว่า', 'ทั้งที', 'ทันทีทันใด', 'ของ', 'หมดกัน', 'เยอะ', 'อาจ', 'ใคร', 'ไฉน', 'ส่วนที่', 'นัก', 'ถึงเมื่อ', 'มัก', 'เสียจนกระทั่ง', 'นํา', 'สิ่งใด', 'เมื่อคราวที่', 'แค่ไหน', 'ค่ะ', 'ให้แก่', 'ต่างๆ', 'เล็กๆ', 'ถือว่า', 'ที่สุด', 'รวมกัน', 'ขณะนั้น', 'ที่ใด', 'บางคราว', 'ซะจน', 'สูงกว่า', 'เป็นเพียง', 'ไม่ใช่', 'ยืนยัน', 'ครบถ้วน', 'นี่ไง', 'ครั้งหลัง', 'เร็ว', 'นาย', 'นางสาว', 'นักๆ', 'ขอ', 'ถ้าจะ', 'เมื่อคืน', 'นับแต่นี้', 'ประการฉะนี้', 'เต็มไปหมด', 'สิ้น', 'พยายาม', 'อย่างไหน', 'ฯ', 'ที่ได้', 'หรือยัง', 'ช่วง', 'ภาคฯ', 'นำพา', 'กระผม', 'ไกลๆ', 'โดย', 'อันละ', 'เท่าใด', 'ให้แด่', 'ช่วงนั้น', 'คือ', 'ก็ตามที', 'คราวหน้า', 'ภายนอก', 'แก่', 'ก่อน', 'กลับ', 'พอสมควร', 'เช่นเดียวกัน', 'พวกคุณ', 'ไม่ค่อยเป็น', 'อีก', 'ยอมรับ', 'จากนี้ไป', 'มั้ยเนี่ย', 'ได้แต่', 'แต่ละ', 'เดียว', 'เป็นอันมาก', 'ได้', 'นานๆ', 'นู้น', 'พอที่', 'พวกนั้น', 'กว้าง', 'ให้ดี', 'เกินๆ', 'ทั้งคน', 'เป็นเพียงว่า', 'ซึ่งกันและกัน', 'แต่ทว่า', 'เพียงไร', 'ทั้งหลาย', 'ตลอดปี', 'พร้อมด้วย', 'ค่อยๆ', 'ตามแต่', 'คุณๆ', 'สิ่ง', 'เกือบจะ', 'มาก', 'ก็', 'ปิด', 'ยังแต่', 'ครั้งก่อน', 'ดังเก่า', 'ตน', 'จวนจะ', 'บน', 'แสดง', 'ตลอดไป', 'ประการ', 'หากแม้', 'กำลังจะ', 'ทั้งที่', 'ขณะที่', 'ที่ไหน', 'จึง', 'จัดให้', 'สูง', 'ภายใต้', 'เพียงพอ', 'อันๆ', 'เนื่องจาก', 'ยังงั้น', 'ก็แล้วแต่', 'เข้า', 'ครั้งละ', 'แบบ', 'จง', 'มุ่งเน้น', 'ฯล', 'สุด', 'ใครๆ', 'ยังโง้น', 'เรา', 'ขณะ', 'เมื่อใด', 'พอๆ', 'ซึ่งก็คือ', 'เกี่ยวกับ', 'ครา', 'ระยะๆ', 'ตลอดระยะเวลา', 'ต่าง', 'เมื่อครั้งก่อน', 'อย่าง', 'เคยๆ', 'ในระหว่าง', 'แค่จะ', 'นับ', 'ฝ่าย', 'แต่ไร', 'จนบัดนี้', 'เมื่อวันวาน', 'เหล่านั้น', 'พวก', 'นอก', 'หนอ', 'ทำไม', 'อื่นๆ', 'เท่ากัน', 'เสมือนว่า', 'กว้างขวาง', 'นี่', 'ในช่วง', 'ทั้งหมด', 'สบาย', 'ภายภาค', 'ปฏิบัติ', 'จับ', 'อันจะ', 'คล้ายกันกับ', 'มั้ยนั่น', 'เอา', 'ด้วยที่', 'ดั่งเก่า', 'ครั้งหลังสุด', 'สมัยก่อน', 'อย่างเดียว', 'รวมๆ', 'ถูกๆ', 'บัดนี้', 'มั้ย', 'แล้ว', 'แต่เดิม', 'กันและกัน', 'กลุ่มๆ', 'แต่', 'โตๆ', 'มั้ยนะ', 'ยืนนาน', 'เป็นดัง', 'กัน', 'จนถึง', 'ฉะนี้', 'ตลอดทั่วทั้ง', 'ถึงเมื่อใด', 'เห็นว่า', 'เช่นไร', 'ไม่', 'มิใช่', 'ทุกครา', 'รับรอง', 'ที่จริง', 'พร้อมเพียง', 'มักจะ', 'จวบกับ', 'เปลี่ยน', 'ทั้งๆ', 'ดั่ง', 'ยอม', 'บ่อย', 'ด้วยเหตุนั้น', 'นั่นไง', 'แท้จริง', 'สั้นๆ', 'หรือไร', 'ประการหนึ่ง', 'เอ็ง', 'จากนี้', 'แล้วแต่', 'เช่นนั้น', 'ร่วมมือ', 'อย่างนั้น', 'ที่ๆ', 'ภายหลัง', 'มากกว่า', 'ทุกครั้ง', 'ความ', 'ช่วงระหว่าง', 'ด้วยเหตุที่', 'ด้วยเหตุเพราะ', 'แก้ไข', 'ที่ละ', 'จัดตั้ง', 'บางกว่า', 'กันนะ', 'ทีไร', 'พวกเขา', 'หรือเปล่า', 'ค่อนข้างจะ', 'ภายหน้า', 'เน้น', 'ถึง', 'สิ่งไหน', 'อย่างดี', 'พวกกู', 'เพราะฉะนั้น', 'รับ', 'สําหรับ', 'อาจจะ', 'ค่อยไปทาง', 'มัน', 'เสียยิ่ง', 'หรือไง', 'สมัยนี้', 'เพราะว่า', 'น่าจะ', 'แต่ก็', 'พอควร', 'สมัยนั้น', 'ส่วนเกิน', 'ต้อง', 'ภายใน', 'ยังจะ', 'เมื่อคราว', 'เหตุนั้น', 'พวกที่', 'พร้อม', 'เราๆ', 'ยก', 'ลง', 'น่ะ', 'พวกนี้', 'ยืนยาว', 'ออก', 'เป็นต้น', 'จวบจน', 'จะได้', 'ข้า', 'ครั้งหนึ่ง', 'ได้ที่', 'ทุกเมื่อ', 'เป็นที่สุด', 'กล่าว', 'จวน', 'ค่อย', 'ถ้าหาก', 'มากมาย', 'เป็นอันๆ', 'จริงๆ', 'ครั้งนั้น', 'นี่แหละ', 'เพื่อให้', 'กันไหม', 'จัดการ', 'ใช้', 'ยืนยง', 'เพราะ', 'พวกเธอ', 'ฝ่ายใด', 'ให้มา', 'เห็น', 'กว่า', 'ใหม่ๆ', 'เสียยิ่งนัก', 'ผิด', 'ยัง', 'พอตัว', 'พร้อมกับ', 'คราไหน', 'เป็นการ', 'อย่างละ', 'ดังกับ', 'หน่อย', 'เต็มๆ', 'เมื่อไหร่', 'บางๆ', 'เมื่อคราวก่อน', 'อย่างน้อย', 'ก็คือ', 'ทุกที่', 'เรื่อยๆ', 'เพื่อที่', 'นอกนั้น', 'พวกแก', 'เป็นอัน', 'มี', 'น้อย', 'ง่ายๆ', 'จ๋า', 'จนตลอด', 'ทั้งสิ้น', 'คราวโน้น', 'บางครา', 'คง', 'คงจะ', 'ได้มา', 'ถูก', 'จนกว่า', 'ทีเถอะ', 'นั่นแหละ', 'หารือ', 'เพียงแค่', 'หาใช่', 'สูงๆ', 'แม้แต่', 'รวมถึง', 'ครั้งนี้', 'ดังกับว่า', 'นอกจากว่า', 'เช่นที่ว่า', 'เช่นที่เคย', 'ระหว่าง', 'ง่าย', 'เช่นเคย', 'แต่เมื่อ', 'จัดหา', 'เชื่อ', 'ส่วนดี', 'ภาค', 'ช้านาน', 'เชื่อถือ', 'สมัยโน้น', 'เห็นจะ', 'ผู้ใด', 'มั๊ย', 'หรือไม่', 'เผื่อว่า', 'ใดๆ', 'เป็นเพราะ', 'ไง', 'ไหนๆ', 'ราย', 'ภายภาคหน้า', 'พื้นๆ', 'เป็นแต่', 'กันเถอะ', 'ยาก', 'ชาว', 'ต่างก็', 'เพื่อที่จะ', 'ผิดๆ', 'ใกล้ๆ', 'เถิด', 'คล้ายกับ', 'เหตุ', 'กระนั้น', 'ล้วนจน', 'สิ่งนี้', 'เป็นอันว่า', 'เหลือ', 'ล้วนแต่', 'ต่อกัน', 'ยังไง', 'บัดดล', 'จด', 'เข้าใจ', 'ทั้งเป็น', 'เกี่ยวเนื่อง', 'ครานั้น', 'ปรับ', 'บางแห่ง', 'จะ', 'ไว้', 'ยาว', 'ขั้น', 'ทั้ง', 'อย่างมาก', 'เป็น', 'นี่เอง', 'บอกว่า', 'ช่วงก่อน', 'ถึงเมื่อไร', 'ตั้งแต่', 'แห่งโน้น', 'คราวที่', 'นี้แหล่', 'ตาม', 'วันไหน', 'เหตุไร', 'ข้างต้น', 'ยิ่งเมื่อ', 'ครั้งคราว', 'แต่อย่างใด', 'ยิ่งจน', 'นี้เอง', 'ทุกอย่าง', 'โต', 'ส่วนนั้น', 'ก็ดี', 'หมดสิ้น', 'หาความ', 'นับจากนั้น', 'อย่างเช่น', 'ซะก่อน', 'เป็นแต่เพียง', 'ใกล้', 'เป็นที', 'ตลอดกาลนาน', 'จึงจะ', 'พอที', 'รึว่า', 'เพิ่ม', 'แม้ว่า', 'อะไร', 'ร่วม', 'ส่วนด้อย', 'เรียบ', 'ผล', 'นับจากนี้', 'ถึงแก่', 'คล้ายกัน', 'แค่นั้น', 'เนี่ยเอง', 'ครั้งๆ', 'คล้าย', 'น้อยๆ', 'อันเนื่องมาจาก', 'อย่างไร', 'มิ', 'พวกกัน', 'มั้ยล่ะ', 'คราวก่อน', 'นี่นา', 'ไหน', 'เผื่อที่', 'เช่นที่', 'ซะจนกระทั่ง', 'แห่ง', 'เมื่อนี้', 'ด้วยประการฉะนี้', 'ดั่งกับว่า', 'สำคัญ', 'อื่น', 'แท้', 'พึง', 'ทุกสิ่ง', 'ดังกล่าว', 'ร่วมด้วย', 'อันที่จริง', 'สูงสุด', 'แสดงว่า', 'ช่วงท้าย', 'เมื่อเย็น', 'ถึงแม้', 'ร่วมกัน', 'ทว่า', 'ตลอดมา', 'อดีต', 'ควร', 'จากนั้น', 'บางขณะ', 'เท่าไร', 'ทุกวันนี้', 'จัดทำ', 'อยาก', 'เมื่อวาน', 'ณ', 'อันที่จะ', 'กับ', 'จรด', 'จ้า', 'ตั้ง', 'ที', 'ถึงแม้ว่า', 'กำหนด', 'ทั้งมวล', 'ตลอดเวลา', 'แรก', 'เสีย', 'เรียก', 'ใหญ่ๆ', '\\ufeffๆ', 'แต่ที่', 'ช่วงหน้า', 'เฉย', 'บาง', 'หมด', 'พูด', 'ยิ่งนัก', 'ประสบ', 'แค่นี้', 'ด้าน', 'เช่นก่อน', 'ถึงบัดนั้น', 'ก่อนหน้า', 'เยอะๆ', 'ทั้งนั้น', 'ตามๆ', 'นั้นๆ', 'มิได้', 'ขึ้น', 'คราหนึ่ง', 'ที่', 'จนแม้', 'ได้แก่', 'นั่นเอง', 'แม้กระทั่ง', 'เช่นเดียวกับ', 'เช่นเมื่อ', 'เพียงแต่', 'คราวหนึ่ง', 'ค่อน', 'บางที่', 'เธอ', 'เมื่อก่อน', 'เสร็จสิ้น', 'ขณะนี้', 'เสียนี่กระไร', 'เสมือนกับ', 'อยู่', 'เพียงเพื่อ', 'เปลี่ยนแปลง', 'ซึ่งก็', 'จำ', 'ยิ่งจะ', 'ครั้งที่', 'ทุกที', 'ที่นั้น', 'และ', 'พอเพียง', 'คิดว่า', 'อย่างๆ', 'ข้างล่าง', 'ปัจจุบัน', 'ด้วยเหตุว่า', 'เสียนี่', 'ใน', 'เป็นที่', 'เขา', 'บอกแล้ว', 'ดังเคย', 'ยาวนาน', 'ตลอดกาล', 'รวมทั้ง', 'แม้นว่า', 'นอกจากที่', 'ต่างหาก', 'พึ่ง', 'อนึ่ง', 'เกือบๆ', 'สูงส่ง', 'หลังจาก', 'ส่วนใหญ่', 'แยะๆ', 'เป็นต้นมา', 'ไม่เป็นไร', 'เกี่ยวข้อง', 'ช่วงนี้', 'เยอะแยะ', 'คราวนั้น', 'ตลอดวัน', 'พวกนู้น', 'นับตั้งแต่', 'ถือ', 'หากว่า', 'มันๆ', 'ละ', 'ไป', 'คงอยู่', 'นั่นเป็น', 'ทั้งนั้นเพราะ', 'นู่น', 'ประกอบ', 'ใช่', 'มีแต่', 'ส่ง', 'เมื่อเช้า', 'รือ', 'เสร็จ', 'พวกมึง', 'พร้อมกัน', 'เท่านั้น', 'ครั้งกระนั้น', 'เหตุนี้', 'เช่น', 'อย่างที่', 'กลุ่มก้อน', 'ทุกอัน', 'ที่แท้จริง', 'เพิ่งจะ', 'เกี่ยวๆ', 'เท่ากับ', 'ยิ่งกว่า', 'ถ้า', 'ส่วนมาก', 'อย่างไรก็ได้', 'พวกฉัน', 'ถึงบัดนี้', 'จวบ', 'บัดเดี๋ยวนี้', 'เมื่อนั้น', 'เท่าไหร่', 'พา', 'ทํา', 'ซะ', 'ครบครัน', 'ไม่ค่อยจะ', 'แห่งนี้', 'เร็วๆ', 'เช่นดังที่', 'ต่อ', 'ไร', 'คะ', 'พอสม', 'นอกจาก', 'ซึ่งกัน', 'ทุกแห่ง', 'พวกโน้น', 'นิดหน่อย', 'แล้วเสร็จ', 'พอจะ', 'คราวละ', 'เสียนั่นเอง', 'แต่ไหน', 'พบ', 'ระยะ', 'เมื่อไร', 'พอดี', 'ค่อนมาทาง', 'นั่น', 'นั้น', 'ช่วงที่', 'ข้างเคียง', 'เพิ่ง', 'อย่างยิ่ง', 'ที่แท้', 'เถอะ', 'ที่แห่งนั้น', 'ขณะหนึ่ง', 'หนอย', 'อย่างโน้น', 'ล้วน', 'ด้วยเหมือนกัน', 'นาน', 'ทีละ', 'รือว่า', 'จ้ะ', 'ทรง', 'กระทำ', 'นิด', 'เสียแล้ว', 'บางครั้ง', 'สามารถ', 'เชื่อว่า', 'แต่ต้อง', 'แล้วกัน', 'บ่อยกว่า', 'ก็ได้', 'ด้วยเช่นกัน', 'ขณะเดียวกัน', 'เอง', 'ซึ่ง', 'ก็ตามแต่', 'เสียนั่น', 'ขณะใดๆ', 'คราที่', 'บ่อยครั้ง', 'รวดเร็ว', 'เกิน', 'ก็แค่', 'จ๊ะ', 'คราใด', 'ส่วนน้อย', 'อันได้แก่', 'นิดๆ', 'แม้', 'ที่นี้', 'เป็นเพราะว่า', 'รวมด้วย', 'สิ้นกาลนาน', 'รึ', 'มิฉะนั้น', 'ครั้งใด', 'ซะจนถึง', 'อย่างหนึ่ง', 'เล็ก', 'ก็ตาม', 'เล่าว่า', 'มุ่ง', 'เริ่ม', 'แต่ถ้า', 'คราว', 'ซึ่งได้แก่', 'จึงเป็น', 'ผู้', 'กล่าวคือ', 'นับแต่', 'พวกท่าน', 'อาจเป็น', 'เฉกเช่น', 'จวนเจียน', 'เดียวกัน', 'สมัย', 'เกือบ', 'ช่วงๆ', 'กันเอง', 'อันที่', 'นอกเหนือ', 'เลย', 'ทีเดียว', 'วันนี้', 'แห่งไหน', 'จังๆ', 'ครบ', 'อัน', 'เผื่อ', 'เก็บ', 'ใคร่จะ', 'สุดๆ', 'แต่นั้น', 'ผ่านๆ', 'รวม', 'แค่เพียง', 'บอก', 'ช่วงหลัง', 'เรื่อย', 'ทำไร', 'เหตุผล', 'เพียงใด', 'นับแต่ที่', 'เพียง', 'ค่อนข้าง', 'แก', 'ด้วยเพราะ', 'ข้าพเจ้า', 'สืบเนื่อง', 'วันใด', 'เช่นดังว่า', 'ยิ่งใหญ่', 'ช่วย', 'ทำให้', 'เหล่านี้', 'ครานี้', 'เท่า', 'เพียงไหน', 'ว่า', 'พอกัน', 'สั้น', 'ทำๆ', 'เฉยๆ', 'เหลือเกิน', 'ทีๆ', 'ตนเอง', 'ไม่ว่า', 'จำเป็น', 'ฉะนั้น', 'อย่างนี้', 'กว้างๆ', 'วัน', 'ส่วน', 'จัดแจง', 'แห่งใด', 'เช่นใด', 'เปิดเผย', 'จำพวก', 'อันไหน', 'รวด', 'นี่แน่ะ', 'ครั้งครา', 'น้อยกว่า', 'นะ', 'พอเหมาะ', 'จริงๆจังๆ', 'เห็นแก่', 'ตนฯ', 'ครั้ง', 'เมื่อครั้ง', 'หรือ', 'ตลอดทั่ว', 'แต่ว่า', 'ช่วงต่อไป', 'ข้างๆ', 'กันดีกว่า', 'ตรงๆ', 'จน', 'ดัง', 'ส่วนใด', 'ถึงอย่างไร', 'ประการใด', 'บัดนั้น', 'คล้ายว่า', 'หน', 'ตรง', 'ดั่งเคย', 'ทั่ว', 'สู่', 'จนกระทั่ง', 'ด้วยว่า', 'ตามที่', 'น่า', 'เคย', 'ขณะใด', 'เท่านี้', 'ทุกชิ้น', 'เขียน', 'ครั้งไหน', 'ขวางๆ', 'เป็นเพื่อ', 'เสร็จกัน', 'ใหม่', 'ย่อม', 'ก่อนหน้านี้', 'ยิ่งขึ้นไป', 'เสร็จสมบูรณ์', 'ไกล', 'ช้าๆ', 'พร้อมทั้ง', 'ก่อนๆ', 'ช่วงแรก', 'หลาย', 'ตามด้วย', 'ยังงี้', 'คราวไหน', 'ใหญ่โต', 'ทันใดนั้น', 'นำมา', 'อย่างใด', 'นอกเหนือจาก', 'จัดงาน', 'ปรากฏ', 'จนทั่ว', 'ในเมื่อ', 'เต็มไปด้วย', 'คราวๆ', 'ฉัน', 'พวกมัน', 'กระทั่ง', 'อาจเป็นด้วย', 'นำ', 'ก็ต่อเมื่อ', 'เล็กน้อย', 'ผ่าน', 'นอกจากนี้', 'ตลอด', 'ขาด', 'ด้วยกัน', 'เช่นนั้นเอง', 'บางที', 'ทุก', 'หากแม้นว่า', 'ครัน', 'ถึงจะ', 'ทุกคราว', 'ที่ซึ่ง', 'นั้นไว', 'เสียจนถึง', 'จนขณะนี้', 'ทุกทาง', 'ด้วยเหตุนี้', 'เป็นด้วย', 'ใต้', 'หลัง', 'ทุกหน', 'เมื่อ', 'ฯลฯ', 'แต่เพียง', 'แค่', 'แต่ก่อน', 'ให้', 'จนเมื่อ', 'เช่นนี้', 'แต่จะ', 'อย่างไรก็', 'จนแม้น', 'ข้าฯ', 'เช่นดัง', 'ทัน', 'เช่นดังเก่า', 'อันใด', 'บ่อยๆ', 'เชื่อมั่น', 'เหล่า', 'แห่งนั้น', 'ยิ่ง', 'เผื่อจะ', 'ตลอดทั้ง', 'ๆ', 'ทีใด', 'กันดีไหม', 'เปิด', 'จาก', 'ยกให้', 'จัด', 'ที่ว่า', 'ข้างบน', 'ช้า', 'ตลอดศก', 'นี้', 'ยิ่งขึ้น', 'นับแต่นั้น', 'คราวนี้', 'ยิ่งแล้ว', 'ด้วย', 'หนึ่ง', 'นอกจากนั้น', 'เกิด', 'ได้รับ', 'จริงจัง', 'เสียก่อน', 'ภาย', 'ตลอดถึง', 'หาก', 'หากแม้น', 'พอ', 'เสร็จแล้ว', 'ถูกต้อง', 'มองว่า', 'ที่แล้ว', 'ตลอดทั่วถึง', 'มา', 'คำ', 'ก็จะ', 'ทันที', 'ใช่ไหม', 'ย่อย', 'ทุกคน', 'เป็นๆ', 'ดั่งกับ', 'ทั้งตัว', 'พร้อมที่', 'ใคร่', 'เฉพาะ', 'มอง', 'นาง', 'กำลัง', 'เพียงเพราะ', 'ทุกๆ', 'ครับ']\n",
            "['ตะกี้', 'ไป', 'ชำระ', 'ค่าบริการ', 'ไป', 'แล้ว', ' ', 'แต่', 'ยัง', 'ใช้งาน', 'ไม่', 'ได้']\n"
          ]
        }
      ],
      "source": [
        "import pythainlp\n",
        "from pythainlp import word_tokenize\n",
        "\n",
        "thai_stopwords = pythainlp.corpus.thai_stopwords()\n",
        "thai_stopwords = list(thai_stopwords)\n",
        "print(thai_stopwords)\n",
        "tokenizer = pythainlp.tokenize.word_tokenize\n",
        "\n",
        "print(tokenizer('ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx6gllzrnVVU"
      },
      "source": [
        "#Model 1 TF-IDF\n",
        "\n",
        "Build a model to train a tf-idf text classifier. Use a simple logistic regression model for the classifier.\n",
        "\n",
        "For this part, you may find this [tutorial](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py) helpful.\n",
        "\n",
        "Below are some design choices you need to consider to accomplish this task. Be sure to answer them when you submit your model.\n",
        "\n",
        "What tokenizer will you use? Why?\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "Will you ignore some stop words (a, an, the, to, etc. for English) in your tf-idf? Is it important?\n",
        "PythaiNLP provides a list of stopwords if you want to use (https://pythainlp.org/docs/2.0/api/corpus.html#pythainlp.corpus.common.thai_stopwords)\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "The dictionary of TF-IDF is usually based on the training data. How many words in the test set are OOVs?\n",
        "\n",
        "**Ans:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "9vOqTqmfufsT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10693, 3226) (1337, 3226) (1337, 3226)\n",
            "(10693,) (1337,) (1337,)\n",
            " \n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenizer, stop_words=thai_stopwords)\n",
        "\n",
        "X_train = vectorizer.fit_transform(train_raw[:, 0])\n",
        "X_val = vectorizer.transform(val_raw[:, 0])\n",
        "X_test = vectorizer.transform(test_raw[:, 0])\n",
        "\n",
        "y_train = train_raw[:, 1].astype(int)\n",
        "y_val = val_raw[:, 1].astype(int)\n",
        "y_test = test_raw[:, 1].astype(int)\n",
        "\n",
        "print(X_train.shape, X_val.shape, X_test.shape)\n",
        "print(y_train.shape, y_val.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'ถ้าซื้อซิมทรูมูฟเอชที่ <PHONE_NUMBER_REMOVED> จะใช้งานได้เลยมั้ยค่ะ'\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 7 stored elements and shape (1, 3226)>\n",
            "  Coords\tValues\n",
            "  (0, 1)\t0.2665589869300581\n",
            "  (0, 299)\t0.4155297793485175\n",
            "  (0, 1110)\t0.3731131585317235\n",
            "  (0, 1119)\t0.3944249728376926\n",
            "  (0, 1331)\t0.3727641962056353\n",
            "  (0, 2869)\t0.4402965463437427\n",
            "  (0, 3129)\t0.3587236066929508\n",
            "[' ' '<phone_number_removed>' 'ซิม' 'ซื้อ' 'ทรูมูฟ' 'เอช' 'ใช้งาน']\n"
          ]
        }
      ],
      "source": [
        "example = train_raw[4, 0]\n",
        "print(f\"'{example}'\")\n",
        "print(vectorizer.transform([example]))\n",
        "print(vectorizer.get_feature_names_out()[np.where(vectorizer.transform([example]).toarray()[0] > 0)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wql2YeU8qFQ6"
      },
      "source": [
        "# Model 2 MUSE\n",
        "\n",
        "Build a simple logistic regression model using features from the MUSE model.\n",
        "\n",
        "Which MUSE model will you use? Why?\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "MUSE is typically used with tensorflow. However, there are some pytorch conversions made by some people.\n",
        "\n",
        "https://huggingface.co/sentence-transformers/use-cmlm-multilingual\n",
        "https://huggingface.co/dayyass/universal-sentence-encoder-multilingual-large-3-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3UtkpaLnctH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDHfX377rnp_"
      },
      "source": [
        "# Model 3 WangchanBERTa\n",
        "\n",
        "We ask you to train a WangchanBERTa-based model.\n",
        "\n",
        "We recommend you use the thaixtransformers fork (which we used in the PoS homework).\n",
        "https://github.com/PyThaiNLP/thaixtransformers\n",
        "\n",
        "The structure of the code will be very similar to the PoS homework. You will also find the huggingface [tutorial](https://huggingface.co/docs/transformers/en/tasks/sequence_classification) useful. Or you can also add a softmax layer by yourself just like in the previous homework.\n",
        "\n",
        "Which WangchanBERTa model will you use? Why? (Don't forget to clean your text accordingly).\n",
        "\n",
        "**Ans:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI8SvILyub0m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D7qsVL0BaXS"
      },
      "source": [
        "After you"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr9_0DnMBcFZ"
      },
      "source": [
        "# Comparison\n",
        "\n",
        "After you have completed the 3 models, compare the accuracy, ease of implementation, and inference speed (from cleaning, tokenization, till model compute) between the three models in mycourseville."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ML10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
