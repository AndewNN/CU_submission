{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ8FRFIYMc5X"
      },
      "source": [
        "# HOMEWORK 6: TEXT CLASSIFICATION\n",
        "In this homework, you will create models to classify texts from TRUE call-center. There are two classification tasks:\n",
        "1. Action Classification: Identify which action the customer would like to take (e.g. enquire, report, cancle)\n",
        "2. Object Classification: Identify which object the customer is referring to (e.g. payment, truemoney, internet, roaming)\n",
        "\n",
        "We will focus only on the Object Classification task for this homework.\n",
        "\n",
        "In this homework, you are asked compare different text classification models in terms of accuracy and inference time.\n",
        "\n",
        "You will need to build 3 different models.\n",
        "\n",
        "1. A model based on tf-idf\n",
        "2. A model based on MUSE\n",
        "3. A model based on wangchanBERTa\n",
        "\n",
        "**You will be ask to submit 3 different files (.pdf from .ipynb) that does the 3 different models. Finally, answer the accuracy and runtime numbers in MCV.**\n",
        "\n",
        "This homework is quite free form, and your answer may vary. We hope that the processing during the course of this assignment will make you think more about the design choices in text classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHqkFSyaNvOt",
        "outputId": "879b17f1-0fb2-455c-ca37-b5a4aecd7b1c"
      },
      "outputs": [],
      "source": [
        "# !wget --no-check-certificate https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRlx5Mb5zkXw",
        "outputId": "18d913e0-aa6d-435b-931d-591386cb4ba8"
      },
      "outputs": [],
      "source": [
        "# !pip install pythainlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YprqbOPMc5a"
      },
      "source": [
        "## Import Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "heICP79cMc5e"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import pandas\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from IPython.display import display\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPaUf4PLMc5k"
      },
      "source": [
        "## Loading data\n",
        "First, we load the data from disk into a Dataframe.\n",
        "\n",
        "A Dataframe is essentially a table, or 2D-array/Matrix with a name for each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JhZ2eBAWMc5l"
      },
      "outputs": [],
      "source": [
        "data_df = pd.read_csv('clean-phone-data-for-students.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cje3yruTMc5p"
      },
      "source": [
        "Let's preview the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "aNqRNz1PMc5q",
        "outputId": "e129a502-1420-476c-dc50-46c293a01b56"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence Utterance</th>\n",
              "      <th>Action</th>\n",
              "      <th>Object</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;PHONE_NUMBER_REMOVED&gt; ผมไปจ่ายเงินที่ Counte...</td>\n",
              "      <td>enquire</td>\n",
              "      <td>payment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>internet ยังความเร็วอยุ่เท่าไหร ครับ</td>\n",
              "      <td>enquire</td>\n",
              "      <td>package</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...</td>\n",
              "      <td>report</td>\n",
              "      <td>suspend</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...</td>\n",
              "      <td>enquire</td>\n",
              "      <td>internet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...</td>\n",
              "      <td>report</td>\n",
              "      <td>phone_issues</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  Sentence Utterance   Action        Object\n",
              "0   <PHONE_NUMBER_REMOVED> ผมไปจ่ายเงินที่ Counte...  enquire       payment\n",
              "1               internet ยังความเร็วอยุ่เท่าไหร ครับ  enquire       package\n",
              "2   ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...   report       suspend\n",
              "3   พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...  enquire      internet\n",
              "4   ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...   report  phone_issues"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence Utterance</th>\n",
              "      <th>Action</th>\n",
              "      <th>Object</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>16175</td>\n",
              "      <td>16175</td>\n",
              "      <td>16175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>13389</td>\n",
              "      <td>10</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>บริการอื่นๆ</td>\n",
              "      <td>enquire</td>\n",
              "      <td>service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>97</td>\n",
              "      <td>10377</td>\n",
              "      <td>2525</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Sentence Utterance   Action   Object\n",
              "count               16175    16175    16175\n",
              "unique              13389       10       33\n",
              "top           บริการอื่นๆ  enquire  service\n",
              "freq                   97    10377     2525"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show the top 5 rows\n",
        "display(data_df.head())\n",
        "# Summarize the data\n",
        "data_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGd8BNvMMc5y"
      },
      "source": [
        "## Data cleaning\n",
        "\n",
        "We call the DataFrame.describe() again.\n",
        "Notice that there are 33 unique labels/classes for object and 10 unique labels for action that the model will try to predict.\n",
        "But there are unwanted duplications e.g. Idd,idd,lotalty_card,Lotalty_card\n",
        "\n",
        "Also note that, there are 13389 unqiue sentence utterances from 16175 utterances. You have to clean that too!\n",
        "\n",
        "## #TODO 0.1:\n",
        "You will have to remove unwanted label duplications as well as duplications in text inputs.\n",
        "Also, you will have to trim out unwanted whitespaces from the text inputs.\n",
        "This shouldn't be too hard, as you have already seen it in the demo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "V0bGLblVMc5z",
        "outputId": "1a65aff5-6196-4674-fb5d-36aa1afcfdba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence Utterance</th>\n",
              "      <th>Action</th>\n",
              "      <th>Object</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>16175</td>\n",
              "      <td>16175</td>\n",
              "      <td>16175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>13389</td>\n",
              "      <td>10</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>บริการอื่นๆ</td>\n",
              "      <td>enquire</td>\n",
              "      <td>service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>97</td>\n",
              "      <td>10377</td>\n",
              "      <td>2525</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Sentence Utterance   Action   Object\n",
              "count               16175    16175    16175\n",
              "unique              13389       10       33\n",
              "top           บริการอื่นๆ  enquire  service\n",
              "freq                   97    10377     2525"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n",
              "       'service', 'nonTrueMove', 'balance', 'detail', 'bill', 'credit',\n",
              "       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n",
              "       'information', 'lost_stolen', 'balance_minutes', 'idd',\n",
              "       'TrueMoney', 'garbage', 'Payment', 'IDD', 'ringtone', 'Idd',\n",
              "       'rate', 'loyalty_card', 'contact', 'officer', 'Balance', 'Service',\n",
              "       'Loyalty_card'], dtype=object)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array(['enquire', 'report', 'cancel', 'Enquire', 'buy', 'activate',\n",
              "       'request', 'Report', 'garbage', 'change'], dtype=object)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(data_df.describe())\n",
        "display(data_df.Object.unique())\n",
        "display(data_df.Action.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "19onNNUZMc54"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean Sentence Utterance</th>\n",
              "      <th>clean Object</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>13367</td>\n",
              "      <td>13367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>13367</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ</td>\n",
              "      <td>service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>2108</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    clean Sentence Utterance clean Object\n",
              "count                                  13367        13367\n",
              "unique                                 13367           26\n",
              "top     สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ      service\n",
              "freq                                       1         2108"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO.1: Data cleaning\n",
        "clean_time = 0\n",
        "t_start = time.time()\n",
        "data_df['clean Sentence Utterance'] = data_df['Sentence Utterance'].str.strip().copy()\n",
        "# data_df['clean Action'] = data_df['Action'].str.lower().copy()\n",
        "data_df['clean Object'] = data_df['Object'].str.lower().copy()\n",
        "\n",
        "# data_df.drop_duplicates(\"Sentence Utterance\", keep=\"first\", inplace=True)\n",
        "data_df.drop_duplicates(\"clean Sentence Utterance\", keep=\"first\", inplace=True)\n",
        "\n",
        "data_df.drop('Sentence Utterance', axis=1, inplace=True)\n",
        "data_df.drop('Action', axis=1, inplace=True)\n",
        "data_df.drop('Object', axis=1, inplace=True)\n",
        "t_end = time.time()\n",
        "clean_time += t_end - t_start\n",
        "\n",
        "\n",
        "data_df.describe()\n",
        "\n",
        "# idx = 1\n",
        "# print(f'\"{data_df[\"Sentence Utterance\"][idx]}\"')\n",
        "# print(f'\"{data_df[\"clean Sentence Utterance\"][idx]}\"')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "t_start = time.time()\n",
        "data = data_df.to_numpy()\n",
        "unique_label = data_df['clean Object'].unique()\n",
        "\n",
        "label_2_num = dict(zip(unique_label, range(len(unique_label))))\n",
        "num_2_label = dict(zip(range(len(unique_label)), unique_label))\n",
        "\n",
        "# display(label_2_num)\n",
        "# display(num_2_label)\n",
        "\n",
        "# display(data[:, 1])\n",
        "data[:, 1] = np.vectorize(label_2_num.get)(data[:, 1])\n",
        "# display(data[:, 1])\n",
        "\n",
        "t_end = time.time()\n",
        "clean_time += t_end - t_start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIxnPRiAmrhN"
      },
      "source": [
        "Split data into train, valdation, and test sets (normally the ratio will be 80:10:10 , respectively). We recommend to use train_test_spilt from scikit-learn to split the data into train, validation, test set.\n",
        "\n",
        "In addition, it should split the data that distribution of the labels in train, validation, test set are similar. There is **stratify** option to handle this issue.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "Make sure the same data splitting is used for all models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 641 1791  730 1786  581 2108  246 1478  327  540  173 1142  280   22\n",
            "  246  248  296  231   50  206   49   79   36   67    4   10]\n"
          ]
        }
      ],
      "source": [
        "bin_label = np.bincount(np.array(data[:, 1], dtype=int))\n",
        "# print(data[:, 1])\n",
        "print(bin_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "EYzMrvb7nYR2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "t_start = time.time()\n",
        "sss_train_valtest = StratifiedShuffleSplit(n_splits=1, test_size=1/10, random_state=42)\n",
        "sss_val_test = StratifiedShuffleSplit(n_splits=1, test_size=1/9, random_state=42)\n",
        "\n",
        "# print(data.shape)\n",
        "trainval_idx, test_idx = next(sss_train_valtest.split(data[:, 0], data[:, 1]))\n",
        "trainval_raw = data[trainval_idx]\n",
        "test_raw = data[test_idx]\n",
        "# print(trainval_raw.shape, test_raw.shape)\n",
        "\n",
        "train_idx, val_idx = next(sss_val_test.split(trainval_raw[:, 0], trainval_raw[:, 1]))\n",
        "train_raw = trainval_raw[train_idx]\n",
        "val_raw = trainval_raw[val_idx]\n",
        "\n",
        "# print(train_raw.shape, val_raw.shape, test_raw.shape)\n",
        "t_end = time.time()\n",
        "clean_time += t_end - t_start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ครั้งครา', 'ที่นั้น', 'แค่ไหน', 'ช้าๆ', 'ทั้งนี้', 'กลุ่มก้อน', 'เหล่านั้น', 'จับ', 'แห่งไหน', 'เมื่อนั้น', 'พร้อมที่', 'เดียวกัน', 'ข้าฯ', 'อาจเป็น', 'เช่นดังก่อน', 'จนแม้', 'นู่น', 'พวกมัน', 'เมื่อคืน', 'ทุกคน', 'อันได้แก่', 'ครั้ง', 'เป็นที่', 'ตลอดปี', 'พบ', 'เฉพาะ', 'พวกคุณ', 'เท่าที่', 'วันนั้น', 'จนบัดนี้', 'กัน', 'หรือ', 'เช่นเดียวกับ', 'ไกล', 'นะ', 'กว้างๆ', 'จึง', 'พึ่ง', 'เฉยๆ', 'แต่ก่อน', 'ก็แค่', 'ฝ่ายใด', 'มั้ยเนี่ย', 'เป็นแต่', 'มุ่งหมาย', 'บ่อยๆ', 'เช่นเมื่อ', 'ยาว', 'และ', 'คล้ายกัน', 'ใหญ่ๆ', 'ถูก', 'นักๆ', 'ไม่ค่อยเป็น', 'สิ่งไหน', 'ทุกที', 'ครั้งหลัง', 'ครั้งหลังสุด', 'เริ่ม', 'เช่นที่เคย', 'ใครๆ', 'ทุกครั้ง', 'ก่อนหน้านี้', 'แต่ไหน', 'ความ', 'พอ', 'ครบครัน', 'นิดๆ', 'อัน', 'นี่', 'นั้น', 'ฉัน', 'สั้น', 'สมัย', 'จวบกับ', 'ครั้งนี้', 'ร่วมด้วย', 'ข้างเคียง', 'เพียงเพื่อ', 'ที่', 'ขาด', 'สุดๆ', 'ใกล้', 'ทำๆ', 'เลย', 'ใช่ไหม', 'รือว่า', 'กำลังจะ', 'มิใช่', 'นอกจากนั้น', 'เต็มไปหมด', 'แยะ', 'เป็นแต่เพียง', 'พอแล้ว', 'ไม่ว่า', 'พอควร', 'สำคัญ', 'เสียแล้ว', 'รือ', 'กับ', 'เป็นดัง', 'พอที่', 'นี้', 'อย่างไรก็ได้', 'ส่วนมาก', 'หนอย', 'เช่นนั้นเอง', 'ทำไม', 'เป็นการ', 'บางครา', 'กว่า', 'บางที่', 'ส่วน', 'ยอม', 'หากว่า', 'จากนี้ไป', 'ทำไร', 'พร้อมทั้ง', 'ช่วงก่อน', 'คุณๆ', 'ถึงเมื่อ', 'น้อย', 'น้อยกว่า', 'ตลอดกาล', 'ประกอบ', 'กำลัง', 'จัดงาน', 'ก่อน', 'ทุกหน', 'พอสม', 'ล้วนจน', 'เปิดเผย', 'ใดๆ', 'นี่ไง', 'ตลอดมา', 'คิด', 'เพิ่ม', 'จัดให้', 'สมัยก่อน', 'บ่อยครั้ง', 'ถือ', 'ไว้', 'ขณะนี้', 'รวมถึง', 'เร็ว', 'เหตุนี้', 'เมื่อวาน', 'ตามๆ', 'สิ่ง', 'แห่ง', 'อันใด', 'ซึ่ง', 'เรียก', 'นอกเหนือจาก', 'ยิ่งนัก', 'เอ็ง', 'ตลอดกาลนาน', 'นับจากนี้', 'ตลอดวัน', 'เสียก่อน', 'น่าจะ', 'จริง', 'ประสบ', 'ช่วงนั้น', 'รวมกัน', 'ใคร่จะ', 'เฉย', 'เสียนั่นเอง', 'ก็จะ', 'ก็คือ', 'เสียยิ่งนัก', '\\ufeffๆ', 'ประการหนึ่ง', 'ยิ่งจน', 'ทํา', 'จึงเป็น', 'พร้อมด้วย', 'แห่งใด', 'น้อยๆ', 'ส่วนใด', 'นับจากนั้น', 'เห็น', 'กลับ', 'หลังจาก', 'ของ', 'ทรง', 'ขณะ', 'แต่ไร', 'เป็นต้น', 'นิดหน่อย', 'ยังคง', 'ยาวนาน', 'หากแม้น', 'นั่นแหละ', 'บางกว่า', 'เป็นด้วย', 'ภายภาค', 'วันใด', 'ที่จริง', 'บ่อย', 'ก็แล้วแต่', 'ง่าย', 'ใหญ่', 'เห็นจะ', 'ถึงอย่างไร', 'มั้ยล่ะ', 'ไหน', 'ที่ละ', 'จู่ๆ', 'ลง', 'ตน', 'พร้อม', 'บางครั้ง', 'ผิด', 'พวกเธอ', 'เนี่ยเอง', 'เมื่อคราวที่', 'ไป่', 'เท่าไหร่', 'รับรอง', 'ซึ่งก็คือ', 'นั่น', 'ได้แก่', 'คำ', 'เต็มไปด้วย', 'วัน', 'ครั้งละ', 'อย่างยิ่ง', 'ขั้น', 'เสร็จสมบูรณ์', 'โดย', 'ไร', 'ครัน', 'คราวนี้', 'ในเมื่อ', 'ส่วนน้อย', 'ภายภาคหน้า', 'จริงๆจังๆ', 'ดังกับ', 'บอก', 'ทั้งสิ้น', 'เกี่ยวเนื่อง', 'ดังกล่าว', 'มั๊ย', 'เราๆ', 'หรือไง', 'แล้วกัน', 'อื่นๆ', 'บัดนั้น', 'รวมทั้ง', 'เช่นที่', 'รับ', 'จึงจะ', 'อันละ', 'เผื่อว่า', 'พวกนี้', 'เมื่อเช้า', 'นั่นไง', 'จนเมื่อ', 'ช่วงนี้', 'เคยๆ', 'ทุกเมื่อ', 'บาง', 'ทัน', 'เต็มๆ', 'ตลอดทั่วถึง', 'ไป', 'เชื่อ', 'ต่างก็', 'ทุกวันนี้', 'ครานี้', 'ยังแต่', 'เมื่อนี้', 'ตลอดทั่ว', 'เมื่อก่อน', 'มองว่า', 'ถ้าจะ', 'รึว่า', 'อันๆ', 'อนึ่ง', 'เกี่ยวๆ', 'ค่อย', 'ทว่า', 'นี้เอง', 'ถึงแม้', 'ออก', 'แก่', 'อาจจะ', 'สมัยนี้', 'ตามด้วย', 'ช่วงถัดไป', 'ไม่เป็นไร', 'บ้าง', 'ไง', 'พอจะ', 'เสียจน', 'ย่อม', 'อย่างนี้', 'เป็นอันมาก', 'ได้รับ', 'ยก', 'บัดนี้', 'พวกโน้น', 'ๆ', 'ให้ไป', 'ระหว่าง', 'พวกท่าน', 'สืบเนื่อง', 'นู้น', 'จัดการ', 'ตนเอง', 'แห่งนั้น', 'ถึงแม้ว่า', 'พอที', 'ทุกวัน', 'ขณะนั้น', 'เพิ่มเติม', 'พร้อมเพียง', 'คราวหลัง', 'เนี่ย', 'ดั่งกับว่า', 'ยิ่งขึ้น', 'แม้นว่า', 'แล้วแต่', 'ไม่ค่อยจะ', 'จนทั่ว', 'ขณะหนึ่ง', 'เพื่อที่', 'ควร', 'จวนเจียน', 'น่า', 'ผ่านๆ', 'ให้ดี', 'กำหนด', 'พา', 'กลุ่มๆ', 'แสดง', 'เกี่ยวกับ', 'อาจ', 'มัก', 'ขณะที่', 'เป็นที่สุด', 'พวกนั้น', 'ทั้งๆ', 'ระยะๆ', 'เรื่อยๆ', 'พูด', 'มันๆ', 'ที่ซึ่ง', 'เป็นเพราะว่า', 'สบาย', 'เหล่านี้', 'พวกมึง', 'พวกกัน', 'คราวไหน', 'ส่วนด้อย', 'ทุกชิ้น', 'เพราะ', 'อันที่', 'ช่วงหน้า', 'แต่ต้อง', 'ต่อ', 'เกี่ยวข้อง', 'ครั้งกระนั้น', 'ทันที', 'หมดกัน', 'สูงส่ง', 'ส่วนนั้น', 'ดังเก่า', 'เข้า', 'จากนี้', 'เก็บ', 'ที่ได้', 'จนกว่า', 'นี่เอง', 'เพียงไหน', 'ระยะ', 'สิ้น', 'แก้ไข', 'เมื่อครั้ง', 'ยอมรับ', 'ด้าน', 'กว้างขวาง', 'สมัยนั้น', 'ต้อง', 'ใหญ่โต', 'เห็นแก่', 'เธอ', 'เรื่อย', 'ด้วย', 'ร่วม', 'หาใช่', 'แต่เดิม', 'อย่างไรเสีย', 'ดั่งกับ', 'เท่านั้น', 'เผื่อที่', 'ครั้งไหน', 'จนตลอด', 'บ่อยกว่า', 'สู่', 'ทําให้', 'อย่างไหน', 'เอา', 'นี่แน่ะ', 'นำพา', 'บางๆ', 'แต่นั้น', 'เปลี่ยน', 'พบว่า', 'จัดแจง', 'มาก', 'ถ้าหาก', 'บางแห่ง', 'จริงๆ', 'พอเพียง', 'เมื่อใด', 'มั้ยนั่น', 'เท่ากับ', 'เช่นดังที่', 'มิฉะนั้น', 'เสมือนว่า', 'ดัง', 'กันดีกว่า', 'มี', 'ใหม่', 'ทีละ', 'หาความ', 'อย่างใด', 'เดียว', 'สิ่งนี้', 'เหลือเกิน', 'จัดหา', 'สิ่งใด', 'ถึงเมื่อใด', 'สามารถ', 'เพียงเพราะ', 'ใคร่', 'ทุกทาง', 'แต่', 'ด้วยเช่นกัน', 'เมื่อไร', 'เป็นอันว่า', 'คงอยู่', 'ซึ่งก็', 'การ', 'แม้กระทั่ง', 'ตามที่', 'บางที', 'แค่นี้', 'ผ่าน', 'เฉกเช่น', 'บัดดล', 'เยอะแยะ', 'ยิ่งขึ้นไป', 'แห่งนี้', 'เรียบ', 'กระนั้น', 'ภายหลัง', 'เมื่อเย็น', 'จ๋า', 'นางสาว', 'ก็', 'พยายาม', 'เช่นใด', 'ผิดๆ', 'อันที่จริง', 'ราย', 'กลุ่ม', 'เช่นดังว่า', 'เป็นต้นมา', 'เสียจนกระทั่ง', 'ก็ดี', 'ยาก', 'ฯ', 'ดั่ง', 'ล้วนแต่', 'ส่ง', 'รวมด้วย', 'กว้าง', 'แต่ทว่า', 'ซะจน', 'ถึงแม้จะ', 'ครั้งใด', 'จะ', 'ทั่ว', 'นอกจากว่า', 'จะได้', 'รวม', 'มั้ย', 'ข้างล่าง', 'ยิ่งแล้ว', 'แต่ที่', 'เมื่อคราว', 'ซึ่งกันและกัน', 'แค่', 'ทุก', 'พวกที่', 'ครั้งนั้น', 'แยะๆ', 'แค่ว่า', 'กันนะ', 'บน', 'ตลอดศก', 'มากมาย', 'ด้วยเหตุนั้น', 'สิ่งนั้น', 'จนถึง', 'เท่ากัน', 'สูงสุด', 'ส่วนเกิน', 'นับตั้งแต่', 'ในช่วง', 'จรด', 'ปิด', 'หมดสิ้น', 'คิดว่า', 'ต่างหาก', 'ข้าง', 'พวกเขา', 'ทั้งที่', 'วันไหน', 'เพิ่งจะ', 'ตลอดเวลา', 'แต่เพียง', 'คราวหน้า', 'พวกนู้น', 'ทุกอัน', 'จริงจัง', 'เชื่อว่า', 'ยิ่งเมื่อ', 'ช่วง', 'ผู้', 'ก็ตาม', 'จัด', 'นับแต่', 'นาน', 'กันและกัน', 'จง', 'ยืนยง', 'ใน', 'แต่เมื่อ', 'พอกัน', 'ใคร', 'เพิ่ง', 'ก็ตามที', 'ครับ', 'สําหรับ', 'อย่างไร', 'จด', 'ผล', 'แบบ', 'คราใด', 'จนกระทั่ง', 'เช่น', 'กล่าว', 'เพียง', 'เช่นเคย', 'เพราะว่า', 'เกี่ยวกัน', 'มากกว่า', 'เกิน', 'กันไหม', 'ข้างต้น', 'เกือบจะ', 'แค่นั้น', 'สุด', 'ปรากฏ', 'ดังเคย', 'กันเอง', 'ไม่ใช่', 'เป็นเพื่อ', 'จวน', 'คล้าย', 'ครานั้น', 'นอกจาก', 'ก็ได้', 'ที่แท้', 'ดั่งเคย', 'ฯล', 'มิ', 'ได้ที่', 'ตลอดถึง', 'ทุกครา', 'เสียด้วย', 'เถิด', 'ค่อน', 'ที่แห่งนั้น', 'เหตุไร', 'ภายหน้า', 'เมื่อวันวาน', 'หลาย', 'ด้วยเพราะ', 'เกิด', 'เสียจนถึง', 'ทุกอย่าง', 'ตนฯ', 'อย่างที่', 'อยาก', 'หมด', 'มักจะ', 'แต่ก็', 'ทั้งนั้น', 'พอสมควร', 'นับแต่ที่', 'สิ้นกาลนาน', 'แม้ว่า', 'บางขณะ', 'จนขณะนี้', 'ด้วยกัน', 'นั้นไว', 'พวกแก', 'นั่นเป็น', 'ล่าสุด', 'ก็ตามแต่', 'หากแม้นว่า', 'บัดเดี๋ยวนี้', 'จ๊ะ', 'ส่วนที่', 'ทีใด', 'อย่างไรก็', 'ยัง', 'เช่นดัง', 'ค่อนมาทาง', 'ทั้ง', 'บางคราว', 'นี้แหล่', 'หลัง', 'กันดีไหม', 'ครา', 'อันจะ', 'เขา', 'ข้างบน', 'ทำให้', 'ขณะเดียวกัน', 'จ้ะ', 'ซึ่งๆ', 'ตลอดจน', 'ทีไร', 'ภาย', 'ที่ใด', 'คง', 'พอๆ', 'ทั้งมวล', 'ส่วนใหญ่', 'แค่เพียง', 'คราวละ', 'บอกแล้ว', 'อย่างน้อย', 'แม้', 'มุ่งเน้น', 'โตๆ', 'ยิ่งกว่า', 'เชื่อมั่น', 'นิด', 'ถึงบัดนั้น', 'ช้านาน', 'กันเถอะ', 'มา', 'วันนี้', 'นอกนั้น', 'เสมือนกับ', 'เพื่อที่จะ', 'เล็กๆ', 'เท่านี้', 'ให้', 'ทีๆ', 'หนึ่ง', 'แต่ว่า', 'เป็นอัน', 'ครั้งคราว', 'ด้วยที่', 'ทุกที่', 'นับแต่นั้น', 'คราวๆ', 'ด้วยเหตุนี้', 'เกินๆ', 'ตั้ง', 'จวบ', 'ทั้งหลาย', 'อีก', 'เสร็จ', 'แหละ', 'ที่แท้จริง', 'เพียงแต่', 'อย่าง', 'ทุกสิ่ง', 'ค่อยๆ', 'นอกจากที่', 'ที่ๆ', 'แต่จะ', 'แสดงว่า', 'ทาง', 'เหล่า', 'เพื่อ', 'ไหนๆ', 'อย่างเดียว', 'ที่ว่า', 'อย่างมาก', 'ทั้งนั้นด้วย', 'ซึ่งได้แก่', 'ซะก่อน', 'เช่นนั้น', 'พวกฉัน', 'เป็นเพียง', 'จ้า', 'เมื่อ', 'แรก', 'คล้ายว่า', 'ใช้', 'ที่นี้', 'เหตุผล', 'เป็นอาทิ', 'เผื่อ', 'ล้วน', 'ร่วมกัน', 'เอง', 'เพียงแค่', 'ภายนอก', 'เมื่อไหร่', 'ที่แล้ว', 'ทุกแห่ง', 'ทั้งปวง', 'ไม่', 'ย่อย', 'ฉะนั้น', 'จวนจะ', 'นับแต่นี้', 'สมัยโน้น', 'คราวที่', 'เน้น', 'ดั่งเก่า', 'เมื่อครั้งก่อน', 'ต่าง', 'สูงๆ', 'คราหนึ่ง', 'ถึงจะ', 'ตลอดไป', 'จัง', 'ทุกตัว', 'รวด', 'คราว', 'ขวางๆ', 'พวกกู', 'ยืนยาว', 'คล้ายกับ', 'เท่า', 'ครั้งหนึ่ง', 'เห็นว่า', 'พอตัว', 'กู', 'นาง', 'ครบถ้วน', 'ข้า', 'เกือบๆ', 'นี่แหละ', 'มีแต่', 'ใหม่ๆ', 'ค่อนข้างจะ', 'ทั้งหมด', 'เชื่อถือ', 'สูงกว่า', 'จังๆ', 'อย่างดี', 'ขณะใด', 'คือ', 'หากแม้', 'ที', 'เช่นดังเก่า', 'เหลือ', 'หรือไม่', 'ก็ต่อเมื่อ', 'จาก', 'คราวโน้น', 'เผื่อจะ', 'เมื่อคราวก่อน', 'อยู่', 'นำมา', 'เป็นอันๆ', 'ให้มา', 'เป็นเพราะ', 'หารือ', 'จน', 'อย่างนั้น', 'ฯลฯ', 'ใช่', 'ได้', 'ขึ้น', 'ยิ่งจะ', 'ตาม', 'แต่ถ้า', 'ช้า', 'เพียงพอ', 'ทั้งที', 'แท้จริง', 'อย่างหนึ่ง', 'จนแม้น', 'จำ', 'หนอ', 'เสร็จกัน', 'แก', 'ถึงเมื่อไร', 'เหตุนั้น', 'เปลี่ยนแปลง', 'อันไหน', 'ทุกคราว', 'รวดเร็ว', 'พวก', 'มัน', 'ปรับ', 'ถูกต้อง', 'ตามแต่', 'นัก', 'หน', 'แท้', 'เท่าไร', 'ตลอดทั่วทั้ง', 'มึง', 'คล้ายกับว่า', 'นาย', 'อดีต', 'จรดกับ', 'ประการใด', 'มอง', 'ซะจนกระทั่ง', 'เพียงไร', 'เร็วๆ', 'ประการ', 'ด้วยว่า', 'เช่นไร', 'เห็นควร', 'ภาค', 'ค่อนข้าง', 'เล่าว่า', 'ร่วมมือ', 'แต่อย่างใด', 'เพื่อให้', 'นอกจากนี้', 'นั่นเอง', 'เขียน', 'ครั้งที่', 'ด้วยประการฉะนี้', 'แค่จะ', 'ทันทีทันใด', 'ทั้งคน', 'เช่นเดียวกัน', 'เสียนี่', 'นํา', 'ทันใดนั้น', 'พอดี', 'ให้แด่', 'มุ่ง', 'ตั้งแต่', 'กล่าวคือ', 'เสร็จสิ้น', 'อื่น', 'ค่ะ', 'เหตุ', 'เช่นก่อน', 'น่ะ', 'ถูกๆ', 'ตลอดทั้ง', 'เยอะ', 'ณ', 'ประการฉะนี้', 'เล็ก', 'ขณะใดๆ', 'พอเหมาะ', 'ไม่ค่อย', 'อย่างโน้น', 'นอก', 'ให้แก่', 'ทีเถอะ', 'ได้แต่', 'พร้อมกัน', 'ปัจจุบัน', 'ภาคฯ', 'ถือว่า', 'ตลอดระยะเวลา', 'เนื่องจาก', 'คงจะ', 'จัดตั้ง', 'นอกเหนือ', 'ต่างๆ', 'ปฏิบัติ', 'มั้ยนะ', 'แม้แต่', 'ช่วงแรก', 'ยังงั้น', 'พื้นๆ', 'สั้นๆ', 'อะไร', 'คล้ายกันกับ', 'เช่นที่ว่า', 'ถ้า', 'พึง', 'เป็นต้นไป', 'ตรง', 'จวบจน', 'หน่อย', 'ประมาณ', 'ด้วยเหตุว่า', 'ที่ไหน', 'เข้าใจ', 'มิได้', 'ก่อนหน้า', 'เท่าใด', 'ถึงแก่', 'ช่วงๆ', 'ทีเดียว', 'เปิด', 'คะ', 'ก่อนๆ', 'เพียงใด', 'จัดทำ', 'ช่วงหลัง', 'เสียนี่กระไร', 'อันเนื่องมาจาก', 'โต', 'ถึงบัดนี้', 'ยังจะ', 'แห่งโน้น', 'นับ', 'ข้างๆ', 'แต่ละ', 'ว่า', 'ด้วยเหมือนกัน', 'อย่างละ', 'ข้าพเจ้า', 'เคย', 'จำพวก', 'ในที่', 'ยังงี้', 'นำ', 'คุณ', 'ครั้งก่อน', 'ฝ่าย', 'อาจเป็นด้วย', 'ตรงๆ', 'เป็น', 'คราวก่อน', 'เป็นๆ', 'ขวาง', 'เสียนั่น', 'ในระหว่าง', 'อันที่จะ', 'คราวใด', 'นี่นา', 'ไกลๆ', 'ที่สุด', 'ช่วงระหว่าง', 'รึ', 'แล้ว', 'ยืนนาน', 'คราวหนึ่ง', 'จากนั้น', 'ภายใน', 'ยกให้', 'เพื่อว่า', 'หาก', 'ยิ่ง', 'ง่ายๆ', 'แล้วเสร็จ', 'ถึง', 'คราไหน', 'ไฉน', 'ทั้งเป็น', 'ช่วงที่', 'ผู้ใด', 'พร้อมกับ', 'อย่างๆ', 'กระทั่ง', 'เสียยิ่ง', 'เป็นที', 'ช่วย', 'ครั้งๆ', 'เสร็จแล้ว', 'ได้มา', 'ละ', 'ครบ', 'ช่วงต่อไป', 'เสีย', 'เช่นกัน', 'อย่างเช่น', 'ใต้', 'รวมๆ', 'ทั้งนั้นเพราะ', 'คราที่', 'เกือบ', 'ซึ่งกัน', 'หรือเปล่า', 'ยังไง', 'คราวนั้น', 'สูง', 'บอกว่า', 'ยืนยัน', 'เพราะฉะนั้น', 'กระทำ', 'เยอะๆ', 'ขอ', 'ยังโง้น', 'ส่วนดี', 'ทุกๆ', 'ซะจนถึง', 'ทั้งตัว', 'หรือยัง', 'เล็กน้อย', 'เป็นเพียงว่า', 'เรา', 'ภายใต้', 'เถอะ', 'ค่อยไปทาง', 'ฉะนี้', 'ตลอด', 'ใกล้ๆ', 'ดังกับว่า', 'ด้วยเหตุที่', 'ชาว', 'ยิ่งใหญ่', 'ช่วงท้าย', 'กระผม', 'นั้นๆ', 'จำเป็น', 'ปรากฏว่า', 'ต่อกัน', 'หรือไร', 'เช่นนี้', 'ด้วยเหตุเพราะ', 'นานๆ', 'ซะ']\n",
            "['ตะกี้', 'ไป', 'ชำระ', 'ค่าบริการ', 'ไป', 'แล้ว', ' ', 'แต่', 'ยัง', 'ใช้งาน', 'ไม่', 'ได้']\n"
          ]
        }
      ],
      "source": [
        "import pythainlp\n",
        "from pythainlp import word_tokenize\n",
        "token_time = 0\n",
        "t_start = time.time()\n",
        "\n",
        "thai_stopwords = pythainlp.corpus.thai_stopwords()\n",
        "thai_stopwords = list(thai_stopwords)\n",
        "tokenizer = pythainlp.tokenize.word_tokenize\n",
        "\n",
        "t_end = time.time()\n",
        "token_time += t_end - t_start\n",
        "print(thai_stopwords)\n",
        "\n",
        "print(tokenizer('ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx6gllzrnVVU"
      },
      "source": [
        "# Model 1 TF-IDF\n",
        "\n",
        "Build a model to train a tf-idf text classifier. Use a simple logistic regression model for the classifier.\n",
        "\n",
        "For this part, you may find this [tutorial](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py) helpful.\n",
        "\n",
        "Below are some design choices you need to consider to accomplish this task. Be sure to answer them when you submit your model.\n",
        "\n",
        "What tokenizer will you use? Why?\n",
        "\n",
        "**Ans: I use pythainlp because dataset is thai language, this tokenizer might suit tokenize task for thai sentence.**\n",
        "\n",
        "Will you ignore some stop words (a, an, the, to, etc. for English) in your tf-idf? Is it important?\n",
        "PythaiNLP provides a list of stopwords if you want to use (https://pythainlp.org/docs/2.0/api/corpus.html#pythainlp.corpus.common.thai_stopwords)\n",
        "\n",
        "**Ans: From the experiments, Used stop words yield acuuracy on test set at 0.69, but without stop words reach 0.74. Each sentence in dataset might too short so stop words cut out many context of each sentence.**\n",
        "\n",
        "The dictionary of TF-IDF is usually based on the training data. How many words in the test set are OOVs?\n",
        "\n",
        "**Ans: Occuring 8193 times and there're uniquely 473 words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9vOqTqmfufsT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/andre/anaconda3/envs/ML10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/home/andre/anaconda3/envs/ML10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['กระไร', 'กาลนาน', 'ชิ้น', 'ดังที่', 'ดี', 'ดีกว่า', 'ด้อย', 'ตัว', 'ต่อไป', 'ถัดไป', 'ทั่วถึง', 'ทำ', 'ที่จะ', 'ท่าน', 'ท้าย', 'นา', 'บอ', 'บัด', 'ระยะเวลา', 'ล่ะ', 'วันวาน', 'สม', 'สมบูรณ์', 'สํา', 'หน้า', 'หรับ', 'หา', 'อย', 'เกี่ยว', 'เก่า', 'เดี๋ยวนี้', 'เย็น', 'เล่า', 'เสมือน', 'เหมือนกัน', 'แด่', 'แม้น', 'แหล่', 'โง้น', 'โน้น', 'ใด', 'ไว', 'ไหม', '\\ufeff'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10693, 3286) (1337, 3286) (1337, 3286)\n",
            "(10693,) (1337,) (1337,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# vectorizer = TfidfVectorizer(tokenizer=tokenizer, stop_words=None)\n",
        "t_start = time.time()\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenizer, stop_words=thai_stopwords)\n",
        "\n",
        "X_train = vectorizer.fit_transform(train_raw[:, 0])\n",
        "X_val = vectorizer.transform(val_raw[:, 0])\n",
        "X_test = vectorizer.transform(test_raw[:, 0])\n",
        "\n",
        "y_train = train_raw[:, 1].astype(int)\n",
        "y_val = val_raw[:, 1].astype(int)\n",
        "y_test = test_raw[:, 1].astype(int)\n",
        "\n",
        "t_end = time.time()\n",
        "token_time += t_end - t_start\n",
        "\n",
        "print(X_train.shape, X_val.shape, X_test.shape)\n",
        "print(y_train.shape, y_val.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'คะจะรบกวนสอบถามนิดนึงว่า พอดีใช้โทรศัพท์บีบีใช้ไหม แล้วเล่น ไลน์ ได้ไหม'\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 9 stored elements and shape (1, 3286)>\n",
            "  Coords\tValues\n",
            "  (0, 1)\t0.3001256331419422\n",
            "  (0, 1480)\t0.267998540180016\n",
            "  (0, 1555)\t0.5518381273514346\n",
            "  (0, 1880)\t0.23692016311205774\n",
            "  (0, 2191)\t0.12867156319941367\n",
            "  (0, 2858)\t0.2278119799118207\n",
            "  (0, 3136)\t0.20822971804046236\n",
            "  (0, 3258)\t0.3808233638797442\n",
            "  (0, 3265)\t0.4696851977498628\n",
            "[' ' 'นิดนึง' 'บี' 'รบกวน' 'สอบถาม' 'เล่น' 'โทรศัพท์' 'ไลน์' 'ไหม']\n"
          ]
        }
      ],
      "source": [
        "example = train_raw[4, 0]\n",
        "print(f\"'{example}'\")\n",
        "print(vectorizer.transform([example]))\n",
        "print(vectorizer.get_feature_names_out()[np.where(vectorizer.transform([example]).toarray()[0] > 0)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "train_time = 0\n",
        "t_start = time.time()\n",
        "\n",
        "clf = LogisticRegression(max_iter=5000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "t_end = time.time()\n",
        "train_time += t_end - t_start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on test set: 0.6925953627524308\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy on test set:\", clf.score(X_test, y_test))\n",
        "\n",
        "infer_time = 0\n",
        "t_start = time.time()\n",
        "y_pred = clf.predict(X_test)\n",
        "t_end = time.time()\n",
        "infer_time += t_end - t_start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "oov_list = [] \n",
        "for sen in [tokenizer(sent) for sent in test_raw[:, 0]]:\n",
        "    for word in sen:\n",
        "        if word not in vectorizer.get_feature_names_out():\n",
        "            # print(f'\"{word}\" not in vocab')\n",
        "            oov_list.append(word)\n",
        "print(len(oov_list))\n",
        "oov_list_np = np.array(oov_list)\n",
        "print(np.unique(oov_list_np).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning time: 0.01942 s\n",
            "Tokenization time: 0.61159 s\n",
            "Training time: 8.57086 s\n",
            "Inference time: 0.00037 s\n"
          ]
        }
      ],
      "source": [
        "print(f\"Cleaning time: {clean_time:.5f} s\")\n",
        "print(f\"Tokenization time: {token_time:.5f} s\")\n",
        "print(f\"Training time: {train_time:.5f} s\")\n",
        "print(f\"Inference time: {infer_time:.5f} s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr9_0DnMBcFZ"
      },
      "source": [
        "# Comparison\n",
        "\n",
        "After you have completed the 3 models, compare the accuracy, ease of implementation, and inference speed (from cleaning, tokenization, till model compute) between the three models in mycourseville."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ML10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
